{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import Regularizer\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening Face Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n"
     ]
    }
   ],
   "source": [
    "FILENAME = 'features_points.h5'\n",
    "h5f = h5py.File(FILENAME,'r')\n",
    "\n",
    "for key in h5f.keys():\n",
    "    print(key) #Names of the groups in HDF5 file.\n",
    "x = h5f['points'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 25, 68, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting targets in System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionID</th>\n",
       "      <th>Session</th>\n",
       "      <th>Type</th>\n",
       "      <th>Merged Arousal</th>\n",
       "      <th>Merged Dominance</th>\n",
       "      <th>Merged Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001-001-l000</td>\n",
       "      <td>1</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S001-001-l001</td>\n",
       "      <td>1</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S001-001-l002</td>\n",
       "      <td>1</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S001-003-l000</td>\n",
       "      <td>3</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Schadenfreude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S001-003-l001</td>\n",
       "      <td>3</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Schadenfreude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SessionID  Session      Type  Merged Arousal  Merged Dominance  \\\n",
       "0  S001-001-l000        1  Laughter               1                 1   \n",
       "1  S001-001-l001        1  Laughter               1                 1   \n",
       "2  S001-001-l002        1  Laughter               2                 2   \n",
       "3  S001-003-l000        3  Laughter               1                 1   \n",
       "4  S001-003-l001        3  Laughter               1                 2   \n",
       "\n",
       "  Merged Emotion  \n",
       "0          Happy  \n",
       "1          Happy  \n",
       "2          Happy  \n",
       "3  Schadenfreude  \n",
       "4  Schadenfreude  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('merged_labels.csv')\n",
    "targets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = targets['Merged Arousal']\n",
    "y1 = targets['Merged Dominance']\n",
    "y2 = targets['Merged Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph_arousal_dominance():\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(WINDOW_SIZE,feat_dim)))\n",
    "    model.add(LSTM(num_feat_map, \n",
    "                   return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(num_feat_map, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM Model Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 16\n",
    "feat_dim = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_11 (Batc (None, 25, 136)           544       \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 25, 32)            21632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 25, 32)            128       \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,657\n",
      "Trainable params: 30,321\n",
      "Non-trainable params: 336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.reshape((491,25,-1))\n",
    "x_test = x_test.reshape((123,25,-1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "create_graph_arousal_dominance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 491 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "491/491 [==============================] - 6s - loss: 4.0999 - mean_squared_error: 4.0999 - val_loss: 5.6497 - val_mean_squared_error: 5.6497\n",
      "Epoch 2/100\n",
      "491/491 [==============================] - 1s - loss: 2.3022 - mean_squared_error: 2.3022 - val_loss: 3.2572 - val_mean_squared_error: 3.2572\n",
      "Epoch 3/100\n",
      "491/491 [==============================] - 1s - loss: 1.5412 - mean_squared_error: 1.5412 - val_loss: 2.1329 - val_mean_squared_error: 2.1329\n",
      "Epoch 4/100\n",
      "491/491 [==============================] - 1s - loss: 1.3734 - mean_squared_error: 1.3734 - val_loss: 2.3675 - val_mean_squared_error: 2.3675\n",
      "Epoch 5/100\n",
      "491/491 [==============================] - 1s - loss: 1.2504 - mean_squared_error: 1.2504 - val_loss: 2.1871 - val_mean_squared_error: 2.1871\n",
      "Epoch 6/100\n",
      "491/491 [==============================] - 1s - loss: 1.2306 - mean_squared_error: 1.2306 - val_loss: 2.2680 - val_mean_squared_error: 2.2680\n",
      "Epoch 7/100\n",
      "491/491 [==============================] - 1s - loss: 1.0954 - mean_squared_error: 1.0954 - val_loss: 2.3844 - val_mean_squared_error: 2.3844\n",
      "Epoch 8/100\n",
      "491/491 [==============================] - 1s - loss: 0.9656 - mean_squared_error: 0.9656 - val_loss: 2.3206 - val_mean_squared_error: 2.3206\n",
      "Epoch 9/100\n",
      "491/491 [==============================] - 1s - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 2.3080 - val_mean_squared_error: 2.3080\n",
      "Epoch 10/100\n",
      "491/491 [==============================] - 1s - loss: 0.9987 - mean_squared_error: 0.9987 - val_loss: 2.0542 - val_mean_squared_error: 2.0542\n",
      "Epoch 11/100\n",
      "491/491 [==============================] - 1s - loss: 0.9253 - mean_squared_error: 0.9253 - val_loss: 1.6589 - val_mean_squared_error: 1.6589\n",
      "Epoch 12/100\n",
      "491/491 [==============================] - 1s - loss: 0.8830 - mean_squared_error: 0.8830 - val_loss: 1.5153 - val_mean_squared_error: 1.5153\n",
      "Epoch 13/100\n",
      "491/491 [==============================] - 1s - loss: 0.8208 - mean_squared_error: 0.8208 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
      "Epoch 14/100\n",
      "491/491 [==============================] - 1s - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 1.5322 - val_mean_squared_error: 1.5322\n",
      "Epoch 15/100\n",
      "491/491 [==============================] - 1s - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 1.5110 - val_mean_squared_error: 1.5110\n",
      "Epoch 16/100\n",
      "491/491 [==============================] - 1s - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 1.5547 - val_mean_squared_error: 1.5547\n",
      "Epoch 17/100\n",
      "491/491 [==============================] - 1s - loss: 0.8162 - mean_squared_error: 0.8162 - val_loss: 1.3982 - val_mean_squared_error: 1.3982\n",
      "Epoch 18/100\n",
      "491/491 [==============================] - 1s - loss: 0.7672 - mean_squared_error: 0.7672 - val_loss: 1.3213 - val_mean_squared_error: 1.3213\n",
      "Epoch 19/100\n",
      "491/491 [==============================] - 1s - loss: 0.7125 - mean_squared_error: 0.7125 - val_loss: 1.1371 - val_mean_squared_error: 1.1371\n",
      "Epoch 20/100\n",
      "491/491 [==============================] - 1s - loss: 0.6912 - mean_squared_error: 0.6912 - val_loss: 0.9955 - val_mean_squared_error: 0.9955\n",
      "Epoch 21/100\n",
      "491/491 [==============================] - 1s - loss: 0.7077 - mean_squared_error: 0.7077 - val_loss: 0.9280 - val_mean_squared_error: 0.9280\n",
      "Epoch 22/100\n",
      "491/491 [==============================] - 1s - loss: 0.6490 - mean_squared_error: 0.6490 - val_loss: 0.9150 - val_mean_squared_error: 0.9150\n",
      "Epoch 23/100\n",
      "491/491 [==============================] - 1s - loss: 0.6899 - mean_squared_error: 0.6899 - val_loss: 0.9258 - val_mean_squared_error: 0.9258\n",
      "Epoch 24/100\n",
      "491/491 [==============================] - 1s - loss: 0.7126 - mean_squared_error: 0.7126 - val_loss: 0.9297 - val_mean_squared_error: 0.9297\n",
      "Epoch 25/100\n",
      "491/491 [==============================] - 1s - loss: 0.7276 - mean_squared_error: 0.7276 - val_loss: 0.9462 - val_mean_squared_error: 0.9462\n",
      "Epoch 26/100\n",
      "491/491 [==============================] - 1s - loss: 0.5990 - mean_squared_error: 0.5990 - val_loss: 0.8654 - val_mean_squared_error: 0.8654\n",
      "Epoch 27/100\n",
      "491/491 [==============================] - 1s - loss: 0.7192 - mean_squared_error: 0.7192 - val_loss: 0.8348 - val_mean_squared_error: 0.8348\n",
      "Epoch 28/100\n",
      "491/491 [==============================] - 1s - loss: 0.6131 - mean_squared_error: 0.6131 - val_loss: 0.8417 - val_mean_squared_error: 0.8417\n",
      "Epoch 29/100\n",
      "491/491 [==============================] - 1s - loss: 0.7193 - mean_squared_error: 0.7193 - val_loss: 0.7804 - val_mean_squared_error: 0.7804\n",
      "Epoch 30/100\n",
      "491/491 [==============================] - 1s - loss: 0.6516 - mean_squared_error: 0.6516 - val_loss: 0.7887 - val_mean_squared_error: 0.7887\n",
      "Epoch 31/100\n",
      "491/491 [==============================] - 1s - loss: 0.6311 - mean_squared_error: 0.6311 - val_loss: 0.7701 - val_mean_squared_error: 0.7701\n",
      "Epoch 32/100\n",
      "491/491 [==============================] - 1s - loss: 0.6287 - mean_squared_error: 0.6287 - val_loss: 0.7711 - val_mean_squared_error: 0.7711\n",
      "Epoch 33/100\n",
      "491/491 [==============================] - 1s - loss: 0.5755 - mean_squared_error: 0.5755 - val_loss: 0.8092 - val_mean_squared_error: 0.8092\n",
      "Epoch 34/100\n",
      "491/491 [==============================] - 1s - loss: 0.5874 - mean_squared_error: 0.5874 - val_loss: 0.8086 - val_mean_squared_error: 0.8086\n",
      "Epoch 35/100\n",
      "491/491 [==============================] - 1s - loss: 0.6097 - mean_squared_error: 0.6097 - val_loss: 0.8137 - val_mean_squared_error: 0.8137\n",
      "Epoch 36/100\n",
      "491/491 [==============================] - 1s - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 0.7766 - val_mean_squared_error: 0.7766\n",
      "Epoch 37/100\n",
      "491/491 [==============================] - 1s - loss: 0.5929 - mean_squared_error: 0.5929 - val_loss: 0.8431 - val_mean_squared_error: 0.8431\n",
      "Epoch 38/100\n",
      "491/491 [==============================] - 1s - loss: 0.5775 - mean_squared_error: 0.5775 - val_loss: 0.7414 - val_mean_squared_error: 0.7414\n",
      "Epoch 39/100\n",
      "491/491 [==============================] - 1s - loss: 0.4761 - mean_squared_error: 0.4761 - val_loss: 0.7470 - val_mean_squared_error: 0.7470\n",
      "Epoch 40/100\n",
      "491/491 [==============================] - 1s - loss: 0.5513 - mean_squared_error: 0.5513 - val_loss: 0.7424 - val_mean_squared_error: 0.7424\n",
      "Epoch 41/100\n",
      "491/491 [==============================] - 1s - loss: 0.5892 - mean_squared_error: 0.5892 - val_loss: 0.7930 - val_mean_squared_error: 0.7930\n",
      "Epoch 42/100\n",
      "491/491 [==============================] - 1s - loss: 0.5088 - mean_squared_error: 0.5088 - val_loss: 0.8358 - val_mean_squared_error: 0.8358\n",
      "Epoch 43/100\n",
      "491/491 [==============================] - 1s - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.8431 - val_mean_squared_error: 0.8431\n",
      "Epoch 44/100\n",
      "491/491 [==============================] - 1s - loss: 0.4693 - mean_squared_error: 0.4693 - val_loss: 0.8529 - val_mean_squared_error: 0.8529\n",
      "Epoch 45/100\n",
      "491/491 [==============================] - 1s - loss: 0.6014 - mean_squared_error: 0.6014 - val_loss: 0.8420 - val_mean_squared_error: 0.8420\n",
      "Epoch 46/100\n",
      "491/491 [==============================] - 1s - loss: 0.5219 - mean_squared_error: 0.5219 - val_loss: 0.8416 - val_mean_squared_error: 0.8416\n",
      "Epoch 47/100\n",
      "491/491 [==============================] - 1s - loss: 0.5603 - mean_squared_error: 0.5603 - val_loss: 0.8405 - val_mean_squared_error: 0.8405\n",
      "Epoch 48/100\n",
      "491/491 [==============================] - 1s - loss: 0.5844 - mean_squared_error: 0.5844 - val_loss: 0.8530 - val_mean_squared_error: 0.8530\n",
      "Epoch 49/100\n",
      "491/491 [==============================] - 1s - loss: 0.4766 - mean_squared_error: 0.4766 - val_loss: 0.8725 - val_mean_squared_error: 0.8725\n",
      "Epoch 50/100\n",
      "491/491 [==============================] - 1s - loss: 0.4548 - mean_squared_error: 0.4548 - val_loss: 0.8944 - val_mean_squared_error: 0.8944\n",
      "Epoch 51/100\n",
      "491/491 [==============================] - 1s - loss: 0.5479 - mean_squared_error: 0.5479 - val_loss: 0.9126 - val_mean_squared_error: 0.9126\n",
      "Epoch 52/100\n",
      "491/491 [==============================] - 1s - loss: 0.4851 - mean_squared_error: 0.4851 - val_loss: 0.8895 - val_mean_squared_error: 0.8895\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 1s - loss: 0.5027 - mean_squared_error: 0.5027 - val_loss: 0.8078 - val_mean_squared_error: 0.8078\n",
      "Epoch 54/100\n",
      "491/491 [==============================] - 1s - loss: 0.4701 - mean_squared_error: 0.4701 - val_loss: 0.8023 - val_mean_squared_error: 0.8023\n",
      "Epoch 55/100\n",
      "491/491 [==============================] - 1s - loss: 0.4120 - mean_squared_error: 0.4120 - val_loss: 0.8272 - val_mean_squared_error: 0.8272\n",
      "Epoch 56/100\n",
      "491/491 [==============================] - 1s - loss: 0.4636 - mean_squared_error: 0.4636 - val_loss: 0.8175 - val_mean_squared_error: 0.8175\n",
      "Epoch 57/100\n",
      "491/491 [==============================] - 1s - loss: 0.4592 - mean_squared_error: 0.4592 - val_loss: 0.8403 - val_mean_squared_error: 0.8403\n",
      "Epoch 58/100\n",
      "491/491 [==============================] - 1s - loss: 0.4172 - mean_squared_error: 0.4172 - val_loss: 0.8786 - val_mean_squared_error: 0.8786\n",
      "Epoch 59/100\n",
      "491/491 [==============================] - 1s - loss: 0.4216 - mean_squared_error: 0.4216 - val_loss: 0.9184 - val_mean_squared_error: 0.9184\n",
      "Epoch 60/100\n",
      "491/491 [==============================] - 1s - loss: 0.3743 - mean_squared_error: 0.3743 - val_loss: 0.8443 - val_mean_squared_error: 0.8443\n",
      "Epoch 61/100\n",
      "491/491 [==============================] - 1s - loss: 0.4743 - mean_squared_error: 0.4743 - val_loss: 0.9003 - val_mean_squared_error: 0.9003\n",
      "Epoch 62/100\n",
      "491/491 [==============================] - 1s - loss: 0.4958 - mean_squared_error: 0.4958 - val_loss: 0.8382 - val_mean_squared_error: 0.8382\n",
      "Epoch 63/100\n",
      "491/491 [==============================] - 1s - loss: 0.4362 - mean_squared_error: 0.4362 - val_loss: 0.8646 - val_mean_squared_error: 0.8646\n",
      "Epoch 64/100\n",
      "491/491 [==============================] - 1s - loss: 0.4875 - mean_squared_error: 0.4875 - val_loss: 0.8906 - val_mean_squared_error: 0.8906\n",
      "Epoch 65/100\n",
      "491/491 [==============================] - 1s - loss: 0.4314 - mean_squared_error: 0.4314 - val_loss: 0.9015 - val_mean_squared_error: 0.9015\n",
      "Epoch 66/100\n",
      "491/491 [==============================] - 1s - loss: 0.4437 - mean_squared_error: 0.4437 - val_loss: 0.7853 - val_mean_squared_error: 0.7853\n",
      "Epoch 67/100\n",
      "491/491 [==============================] - 1s - loss: 0.4654 - mean_squared_error: 0.4654 - val_loss: 0.8174 - val_mean_squared_error: 0.8174\n",
      "Epoch 68/100\n",
      "491/491 [==============================] - 1s - loss: 0.4322 - mean_squared_error: 0.4322 - val_loss: 0.7995 - val_mean_squared_error: 0.7995\n",
      "Epoch 69/100\n",
      "491/491 [==============================] - 1s - loss: 0.4099 - mean_squared_error: 0.4099 - val_loss: 0.8057 - val_mean_squared_error: 0.8057\n",
      "Epoch 70/100\n",
      "491/491 [==============================] - 1s - loss: 0.3837 - mean_squared_error: 0.3837 - val_loss: 0.9080 - val_mean_squared_error: 0.9080\n",
      "Epoch 71/100\n",
      "491/491 [==============================] - 1s - loss: 0.4292 - mean_squared_error: 0.4292 - val_loss: 0.8826 - val_mean_squared_error: 0.8826\n",
      "Epoch 72/100\n",
      "491/491 [==============================] - 1s - loss: 0.3976 - mean_squared_error: 0.3976 - val_loss: 0.9330 - val_mean_squared_error: 0.9330\n",
      "Epoch 73/100\n",
      "491/491 [==============================] - 1s - loss: 0.4195 - mean_squared_error: 0.4195 - val_loss: 0.9179 - val_mean_squared_error: 0.9179\n",
      "Epoch 74/100\n",
      "491/491 [==============================] - 1s - loss: 0.4101 - mean_squared_error: 0.4101 - val_loss: 0.8793 - val_mean_squared_error: 0.8793\n",
      "Epoch 75/100\n",
      "491/491 [==============================] - 1s - loss: 0.4572 - mean_squared_error: 0.4572 - val_loss: 0.8784 - val_mean_squared_error: 0.8784\n",
      "Epoch 76/100\n",
      "491/491 [==============================] - 1s - loss: 0.3776 - mean_squared_error: 0.3776 - val_loss: 0.8959 - val_mean_squared_error: 0.8959\n",
      "Epoch 77/100\n",
      "491/491 [==============================] - 1s - loss: 0.3910 - mean_squared_error: 0.3910 - val_loss: 0.8630 - val_mean_squared_error: 0.8630\n",
      "Epoch 78/100\n",
      "491/491 [==============================] - 1s - loss: 0.4079 - mean_squared_error: 0.4079 - val_loss: 0.8682 - val_mean_squared_error: 0.8682\n",
      "Epoch 79/100\n",
      "491/491 [==============================] - 1s - loss: 0.3943 - mean_squared_error: 0.3943 - val_loss: 0.8439 - val_mean_squared_error: 0.8439\n",
      "Epoch 80/100\n",
      "491/491 [==============================] - 1s - loss: 0.3830 - mean_squared_error: 0.3830 - val_loss: 0.8006 - val_mean_squared_error: 0.8006\n",
      "Epoch 81/100\n",
      "491/491 [==============================] - 1s - loss: 0.4436 - mean_squared_error: 0.4436 - val_loss: 0.7996 - val_mean_squared_error: 0.7996\n",
      "Epoch 82/100\n",
      "491/491 [==============================] - 1s - loss: 0.3760 - mean_squared_error: 0.3760 - val_loss: 0.8070 - val_mean_squared_error: 0.8070\n",
      "Epoch 83/100\n",
      "491/491 [==============================] - 1s - loss: 0.3952 - mean_squared_error: 0.3952 - val_loss: 0.8604 - val_mean_squared_error: 0.8604\n",
      "Epoch 84/100\n",
      "491/491 [==============================] - 1s - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.9127 - val_mean_squared_error: 0.9127\n",
      "Epoch 85/100\n",
      "491/491 [==============================] - 1s - loss: 0.3750 - mean_squared_error: 0.3750 - val_loss: 0.8862 - val_mean_squared_error: 0.8862\n",
      "Epoch 86/100\n",
      "491/491 [==============================] - 1s - loss: 0.3169 - mean_squared_error: 0.3169 - val_loss: 0.8552 - val_mean_squared_error: 0.8552\n",
      "Epoch 87/100\n",
      "491/491 [==============================] - 1s - loss: 0.3848 - mean_squared_error: 0.3848 - val_loss: 0.8837 - val_mean_squared_error: 0.8837\n",
      "Epoch 88/100\n",
      "491/491 [==============================] - 1s - loss: 0.3850 - mean_squared_error: 0.3850 - val_loss: 0.8331 - val_mean_squared_error: 0.8331\n",
      "Epoch 89/100\n",
      "491/491 [==============================] - 1s - loss: 0.3425 - mean_squared_error: 0.3425 - val_loss: 0.8436 - val_mean_squared_error: 0.8436\n",
      "Epoch 90/100\n",
      "491/491 [==============================] - 1s - loss: 0.3819 - mean_squared_error: 0.3819 - val_loss: 0.8398 - val_mean_squared_error: 0.8398\n",
      "Epoch 91/100\n",
      "491/491 [==============================] - 1s - loss: 0.3270 - mean_squared_error: 0.3270 - val_loss: 0.8421 - val_mean_squared_error: 0.8421\n",
      "Epoch 92/100\n",
      "491/491 [==============================] - 1s - loss: 0.3853 - mean_squared_error: 0.3853 - val_loss: 0.8792 - val_mean_squared_error: 0.8792\n",
      "Epoch 93/100\n",
      "491/491 [==============================] - 1s - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.9313 - val_mean_squared_error: 0.9313\n",
      "Epoch 94/100\n",
      "491/491 [==============================] - 1s - loss: 0.3743 - mean_squared_error: 0.3743 - val_loss: 0.8461 - val_mean_squared_error: 0.8461\n",
      "Epoch 95/100\n",
      "491/491 [==============================] - 1s - loss: 0.3484 - mean_squared_error: 0.3484 - val_loss: 0.9172 - val_mean_squared_error: 0.9172\n",
      "Epoch 96/100\n",
      "491/491 [==============================] - 1s - loss: 0.3354 - mean_squared_error: 0.3354 - val_loss: 0.8161 - val_mean_squared_error: 0.8161\n",
      "Epoch 97/100\n",
      "491/491 [==============================] - 1s - loss: 0.3863 - mean_squared_error: 0.3863 - val_loss: 0.8561 - val_mean_squared_error: 0.8561\n",
      "Epoch 98/100\n",
      "491/491 [==============================] - 1s - loss: 0.3361 - mean_squared_error: 0.3361 - val_loss: 0.8563 - val_mean_squared_error: 0.8563\n",
      "Epoch 99/100\n",
      "491/491 [==============================] - 1s - loss: 0.3137 - mean_squared_error: 0.3137 - val_loss: 0.8215 - val_mean_squared_error: 0.8215\n",
      "Epoch 100/100\n",
      "491/491 [==============================] - 1s - loss: 0.3568 - mean_squared_error: 0.3568 - val_loss: 0.8843 - val_mean_squared_error: 0.8843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190b5d6d438>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size=32\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "#filepath=\"weights_best_arousal.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "preds = preds.reshape(preds.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of predictions is: 1.65871\n",
      "Variance (benchmark) is: 0.8843350405884116\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(preds)\n",
    "print(\"Mean of predictions is: \"+str(mean_y))\n",
    "\n",
    "mse = np.mean((y_test - preds)**2)\n",
    "print(\"Variance (benchmark) is: \"+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.9370687 ,  2.86960268,  2.60364652,  2.35215139,  1.61213088,\n",
       "        2.36231065,  0.92654347,  1.66885567,  0.30839017,  1.54102659,\n",
       "        0.46046084,  1.17673254,  1.06782055,  1.22596359,  0.93251181,\n",
       "        3.25358295,  2.78149176,  1.67381716,  1.23727059,  0.20302521,\n",
       "        1.60256827,  1.88018322,  0.89881623,  2.36440444,  3.00015736,\n",
       "        1.76857328,  0.66433799,  0.56920522,  1.18367529,  1.61894858,\n",
       "        0.46460742,  0.45883584,  2.66710782,  2.87744474,  1.1294955 ,\n",
       "        1.98543084,  2.34261322,  1.41242576,  1.03656435,  1.24692261,\n",
       "        1.64408052,  1.42260742,  1.10641313,  0.56041658,  2.66080976,\n",
       "        1.11443019,  3.71967149,  1.63532317,  0.92877382,  3.64372087,\n",
       "        0.95324636,  2.77706099,  2.36249447,  3.59891224,  0.69960868,\n",
       "        0.13275936,  1.48006153,  2.31116009,  1.35569644,  1.90782821,\n",
       "        0.72599232,  3.46610141,  1.06118333,  0.64507425,  2.89716125,\n",
       "        2.71648383,  3.11236882,  1.15700746,  1.85945249,  0.98215073,\n",
       "        1.77238822,  2.87758255,  0.77166665,  0.57239968,  0.54500741,\n",
       "        2.07775307,  3.77508116,  0.10122809,  0.81551838,  0.15450883,\n",
       "        1.62980509,  0.80796254,  2.484689  ,  0.70352578,  1.69096065,\n",
       "        2.98514843,  2.52353096,  2.75486279,  1.54585397,  2.61755657,\n",
       "        0.1043134 ,  1.3900547 ,  1.18991411,  1.11309171,  0.50747269,\n",
       "        1.07282519,  1.36479878,  1.108832  , -0.11098364,  1.32908189,\n",
       "        2.10363626,  0.80773741,  1.97672129,  0.93053508,  3.4443934 ,\n",
       "        1.60507059,  3.14949083,  0.46670097,  1.99824095,  1.74791777,\n",
       "        2.83303714,  0.15949561,  2.97736216,  1.8958689 ,  2.17532349,\n",
       "        0.63636899,  0.83891869,  2.38795376,  1.28733134,  1.81414759,\n",
       "        4.01580429,  0.66460901,  2.82351923], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of test set is: 1.886178861788618\n",
      "Variance (benchmark) is: 1.3691585696344772\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "print(\"Mean of test set is: \"+str(mean_y))\n",
    "\n",
    "variance = np.mean((y_test - np.mean(y_test))**2)\n",
    "print(\"Variance (benchmark) is: \"+str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simple LSTM Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_13 (Batc (None, 25, 136)           544       \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 25, 16)            9792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 25, 16)            64        \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 12,529\n",
      "Trainable params: 12,225\n",
      "Non-trainable params: 304\n",
      "_________________________________________________________________\n",
      "Train on 491 samples, validate on 123 samples\n",
      "Epoch 1/60\n",
      "491/491 [==============================] - 3s - loss: 6.5544 - mean_squared_error: 6.5544 - val_loss: 5.7928 - val_mean_squared_error: 5.7928\n",
      "Epoch 2/60\n",
      "491/491 [==============================] - 1s - loss: 5.0622 - mean_squared_error: 5.0622 - val_loss: 4.6106 - val_mean_squared_error: 4.6106\n",
      "Epoch 3/60\n",
      "491/491 [==============================] - 1s - loss: 3.9435 - mean_squared_error: 3.9435 - val_loss: 4.0620 - val_mean_squared_error: 4.0620\n",
      "Epoch 4/60\n",
      "491/491 [==============================] - 1s - loss: 3.1559 - mean_squared_error: 3.1559 - val_loss: 3.4149 - val_mean_squared_error: 3.4149\n",
      "Epoch 5/60\n",
      "491/491 [==============================] - 1s - loss: 2.5262 - mean_squared_error: 2.5262 - val_loss: 2.6313 - val_mean_squared_error: 2.6313\n",
      "Epoch 6/60\n",
      "491/491 [==============================] - 1s - loss: 1.8307 - mean_squared_error: 1.8307 - val_loss: 1.6042 - val_mean_squared_error: 1.6042\n",
      "Epoch 7/60\n",
      "491/491 [==============================] - 1s - loss: 1.4019 - mean_squared_error: 1.4019 - val_loss: 1.2408 - val_mean_squared_error: 1.2408\n",
      "Epoch 8/60\n",
      "491/491 [==============================] - 1s - loss: 1.1687 - mean_squared_error: 1.1687 - val_loss: 1.1291 - val_mean_squared_error: 1.1291\n",
      "Epoch 9/60\n",
      "491/491 [==============================] - 1s - loss: 1.3444 - mean_squared_error: 1.3444 - val_loss: 1.2862 - val_mean_squared_error: 1.2862\n",
      "Epoch 10/60\n",
      "491/491 [==============================] - 1s - loss: 1.2136 - mean_squared_error: 1.2136 - val_loss: 1.2886 - val_mean_squared_error: 1.2886\n",
      "Epoch 11/60\n",
      "491/491 [==============================] - 1s - loss: 1.1394 - mean_squared_error: 1.1394 - val_loss: 1.1547 - val_mean_squared_error: 1.1547\n",
      "Epoch 12/60\n",
      "491/491 [==============================] - 1s - loss: 1.1036 - mean_squared_error: 1.1036 - val_loss: 1.1742 - val_mean_squared_error: 1.1742\n",
      "Epoch 13/60\n",
      "491/491 [==============================] - 1s - loss: 1.0286 - mean_squared_error: 1.0286 - val_loss: 1.1450 - val_mean_squared_error: 1.1450\n",
      "Epoch 14/60\n",
      "491/491 [==============================] - 1s - loss: 1.0073 - mean_squared_error: 1.0073 - val_loss: 1.0281 - val_mean_squared_error: 1.0281\n",
      "Epoch 15/60\n",
      "491/491 [==============================] - 1s - loss: 0.9403 - mean_squared_error: 0.9403 - val_loss: 1.0032 - val_mean_squared_error: 1.0032\n",
      "Epoch 16/60\n",
      "491/491 [==============================] - 1s - loss: 1.0303 - mean_squared_error: 1.0303 - val_loss: 1.0083 - val_mean_squared_error: 1.0083\n",
      "Epoch 17/60\n",
      "491/491 [==============================] - 1s - loss: 1.0276 - mean_squared_error: 1.0276 - val_loss: 1.0309 - val_mean_squared_error: 1.0309\n",
      "Epoch 18/60\n",
      "491/491 [==============================] - 1s - loss: 1.1219 - mean_squared_error: 1.1219 - val_loss: 0.9592 - val_mean_squared_error: 0.9592\n",
      "Epoch 19/60\n",
      "491/491 [==============================] - 1s - loss: 0.9239 - mean_squared_error: 0.9239 - val_loss: 1.0976 - val_mean_squared_error: 1.0976\n",
      "Epoch 20/60\n",
      "491/491 [==============================] - 1s - loss: 0.9585 - mean_squared_error: 0.9585 - val_loss: 1.2065 - val_mean_squared_error: 1.2065\n",
      "Epoch 21/60\n",
      "491/491 [==============================] - 1s - loss: 0.9412 - mean_squared_error: 0.9412 - val_loss: 0.8985 - val_mean_squared_error: 0.8985\n",
      "Epoch 22/60\n",
      "491/491 [==============================] - 1s - loss: 0.8592 - mean_squared_error: 0.8592 - val_loss: 0.8458 - val_mean_squared_error: 0.8458\n",
      "Epoch 23/60\n",
      "491/491 [==============================] - 1s - loss: 0.9204 - mean_squared_error: 0.9204 - val_loss: 0.7822 - val_mean_squared_error: 0.7822\n",
      "Epoch 24/60\n",
      "491/491 [==============================] - 1s - loss: 0.8898 - mean_squared_error: 0.8898 - val_loss: 0.7279 - val_mean_squared_error: 0.7279\n",
      "Epoch 25/60\n",
      "491/491 [==============================] - 1s - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 0.7207 - val_mean_squared_error: 0.7207\n",
      "Epoch 26/60\n",
      "491/491 [==============================] - 1s - loss: 0.9264 - mean_squared_error: 0.9264 - val_loss: 0.7767 - val_mean_squared_error: 0.7767\n",
      "Epoch 27/60\n",
      "491/491 [==============================] - 1s - loss: 0.8847 - mean_squared_error: 0.8847 - val_loss: 0.6945 - val_mean_squared_error: 0.6945\n",
      "Epoch 28/60\n",
      "491/491 [==============================] - 1s - loss: 0.7914 - mean_squared_error: 0.7914 - val_loss: 0.6692 - val_mean_squared_error: 0.6692\n",
      "Epoch 29/60\n",
      "491/491 [==============================] - 1s - loss: 0.9385 - mean_squared_error: 0.9385 - val_loss: 0.7059 - val_mean_squared_error: 0.7059\n",
      "Epoch 30/60\n",
      "491/491 [==============================] - 1s - loss: 0.8158 - mean_squared_error: 0.8158 - val_loss: 0.6822 - val_mean_squared_error: 0.6822\n",
      "Epoch 31/60\n",
      "491/491 [==============================] - 1s - loss: 0.8663 - mean_squared_error: 0.8663 - val_loss: 0.6313 - val_mean_squared_error: 0.6313\n",
      "Epoch 32/60\n",
      "491/491 [==============================] - 1s - loss: 0.8142 - mean_squared_error: 0.8142 - val_loss: 0.6296 - val_mean_squared_error: 0.6296\n",
      "Epoch 33/60\n",
      "491/491 [==============================] - 1s - loss: 0.7824 - mean_squared_error: 0.7824 - val_loss: 0.6782 - val_mean_squared_error: 0.6782\n",
      "Epoch 34/60\n",
      "491/491 [==============================] - 1s - loss: 0.7344 - mean_squared_error: 0.7344 - val_loss: 0.7079 - val_mean_squared_error: 0.7079\n",
      "Epoch 35/60\n",
      "491/491 [==============================] - 1s - loss: 0.7740 - mean_squared_error: 0.7740 - val_loss: 0.6584 - val_mean_squared_error: 0.6584\n",
      "Epoch 36/60\n",
      "491/491 [==============================] - 1s - loss: 0.8049 - mean_squared_error: 0.8049 - val_loss: 0.6459 - val_mean_squared_error: 0.6459\n",
      "Epoch 37/60\n",
      "491/491 [==============================] - 1s - loss: 0.8495 - mean_squared_error: 0.8495 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 38/60\n",
      "491/491 [==============================] - 1s - loss: 0.8213 - mean_squared_error: 0.8213 - val_loss: 0.6795 - val_mean_squared_error: 0.6795\n",
      "Epoch 39/60\n",
      "491/491 [==============================] - 1s - loss: 0.7513 - mean_squared_error: 0.7513 - val_loss: 0.6739 - val_mean_squared_error: 0.6739\n",
      "Epoch 40/60\n",
      "491/491 [==============================] - 1s - loss: 0.7230 - mean_squared_error: 0.7230 - val_loss: 0.6821 - val_mean_squared_error: 0.6821\n",
      "Epoch 41/60\n",
      "491/491 [==============================] - 1s - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 0.7736 - val_mean_squared_error: 0.7736\n",
      "Epoch 42/60\n",
      "491/491 [==============================] - 1s - loss: 0.7072 - mean_squared_error: 0.7072 - val_loss: 0.6829 - val_mean_squared_error: 0.6829\n",
      "Epoch 43/60\n",
      "491/491 [==============================] - 1s - loss: 0.7423 - mean_squared_error: 0.7423 - val_loss: 0.6801 - val_mean_squared_error: 0.6801\n",
      "Epoch 44/60\n",
      "491/491 [==============================] - 1s - loss: 0.7504 - mean_squared_error: 0.7504 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 45/60\n",
      "491/491 [==============================] - 1s - loss: 0.7454 - mean_squared_error: 0.7454 - val_loss: 0.6434 - val_mean_squared_error: 0.6434\n",
      "Epoch 46/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 1s - loss: 0.6613 - mean_squared_error: 0.6613 - val_loss: 0.6359 - val_mean_squared_error: 0.6359\n",
      "Epoch 47/60\n",
      "491/491 [==============================] - 1s - loss: 0.7964 - mean_squared_error: 0.7964 - val_loss: 0.7720 - val_mean_squared_error: 0.7720\n",
      "Epoch 48/60\n",
      "491/491 [==============================] - 1s - loss: 0.7058 - mean_squared_error: 0.7058 - val_loss: 0.6729 - val_mean_squared_error: 0.6729\n",
      "Epoch 49/60\n",
      "491/491 [==============================] - 1s - loss: 0.6759 - mean_squared_error: 0.6759 - val_loss: 0.6822 - val_mean_squared_error: 0.6822\n",
      "Epoch 50/60\n",
      "491/491 [==============================] - 1s - loss: 0.7038 - mean_squared_error: 0.7038 - val_loss: 0.6121 - val_mean_squared_error: 0.6121\n",
      "Epoch 51/60\n",
      "491/491 [==============================] - 1s - loss: 0.7673 - mean_squared_error: 0.7673 - val_loss: 0.6363 - val_mean_squared_error: 0.6363\n",
      "Epoch 52/60\n",
      "491/491 [==============================] - 1s - loss: 0.6650 - mean_squared_error: 0.6650 - val_loss: 0.6440 - val_mean_squared_error: 0.6440\n",
      "Epoch 53/60\n",
      "491/491 [==============================] - 1s - loss: 0.7613 - mean_squared_error: 0.7613 - val_loss: 0.6590 - val_mean_squared_error: 0.6590\n",
      "Epoch 54/60\n",
      "491/491 [==============================] - 1s - loss: 0.7343 - mean_squared_error: 0.7343 - val_loss: 0.6686 - val_mean_squared_error: 0.6686\n",
      "Epoch 55/60\n",
      "491/491 [==============================] - 1s - loss: 0.7279 - mean_squared_error: 0.7279 - val_loss: 0.6833 - val_mean_squared_error: 0.6833\n",
      "Epoch 56/60\n",
      "491/491 [==============================] - 1s - loss: 0.7248 - mean_squared_error: 0.7248 - val_loss: 0.6639 - val_mean_squared_error: 0.6639\n",
      "Epoch 57/60\n",
      "491/491 [==============================] - 1s - loss: 0.6650 - mean_squared_error: 0.6650 - val_loss: 0.6415 - val_mean_squared_error: 0.6415\n",
      "Epoch 58/60\n",
      "491/491 [==============================] - 1s - loss: 0.7295 - mean_squared_error: 0.7295 - val_loss: 0.6493 - val_mean_squared_error: 0.6493\n",
      "Epoch 59/60\n",
      "491/491 [==============================] - 1s - loss: 0.6199 - mean_squared_error: 0.6199 - val_loss: 0.6484 - val_mean_squared_error: 0.6484\n",
      "Epoch 60/60\n",
      "491/491 [==============================] - 1s - loss: 0.6783 - mean_squared_error: 0.6783 - val_loss: 0.6669 - val_mean_squared_error: 0.6669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190b7105208>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 16\n",
    "feat_dim = 136\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.reshape((491,25,-1))\n",
    "x_test = x_test.reshape((123,25,-1))\n",
    "\n",
    "create_graph_arousal_dominance()\n",
    "\n",
    "epochs = 60\n",
    "batch_size=32\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "sgd = keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "filepath=\"weights_best_arousal.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#adam1 = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of predictions is: 2.20046\n",
      "Variance (benchmark) is: 0.6669065394550879\n",
      "\n",
      "Mean of test set is: 2.3983739837398375\n",
      "Variance (benchmark) is: 0.7437371934694955\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test)\n",
    "preds = preds.reshape(preds.shape[0])\n",
    "\n",
    "mean_y = np.mean(preds)\n",
    "print(\"Mean of predictions is: \"+str(mean_y))\n",
    "\n",
    "mse = np.mean((y_test - preds)**2)\n",
    "print(\"Variance (benchmark) is: \"+str(mse))\n",
    "\n",
    "print()\n",
    "\n",
    "mean_y = np.mean(y_test)\n",
    "print(\"Mean of test set is: \"+str(mean_y))\n",
    "\n",
    "variance = np.mean((y_test - np.mean(y_test))**2)\n",
    "print(\"Variance (benchmark) is: \"+str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.93438828,  2.85008526,  2.94865203,  1.59903073,  1.92163181,\n",
       "        1.71720862,  2.96405625,  1.63232052,  1.90440106,  2.65493441,\n",
       "        2.2405498 ,  0.28412151,  2.09852314,  2.85752106,  1.92819703,\n",
       "        1.77414715,  2.78698397,  2.83201766,  2.32855535,  2.18887043,\n",
       "        1.96209598,  2.00622106,  1.44849026,  2.62282562,  1.9600687 ,\n",
       "        2.45689845,  1.20643401,  2.1491313 ,  2.74603939,  2.12131214,\n",
       "        1.12179673,  2.08844233,  2.61769485,  2.35749626,  1.85754907,\n",
       "        2.52867079,  1.97575128,  1.71261132,  1.4023807 ,  2.16627312,\n",
       "        2.86901736,  1.35656846,  2.98326397,  1.76708508,  2.38357306,\n",
       "        2.33252597,  3.09941125,  2.37027478,  2.36272693,  2.99411798,\n",
       "        1.79808366,  2.76097298,  1.57374728,  2.54396415,  2.11508846,\n",
       "        1.89099157,  2.40068293,  2.33851147,  2.17876196,  2.42810583,\n",
       "        2.21405435,  2.91950178,  1.36441004,  2.4013412 ,  2.83751059,\n",
       "        2.36942887,  2.07267213,  2.10179186,  1.95358908,  1.43690908,\n",
       "        2.22592688,  3.02775073,  2.31121731,  1.5054332 ,  2.45929956,\n",
       "        2.62180638,  2.9407649 ,  2.4710393 ,  2.65700078,  2.31515145,\n",
       "        1.21182501,  1.81091523,  1.93084574,  1.72521877,  2.82873917,\n",
       "        2.46338654,  2.36373544,  2.13263798,  1.40193486,  2.8408556 ,\n",
       "        1.49387825,  2.23836517,  2.55799556,  1.90275013,  1.8241266 ,\n",
       "        1.52997351,  1.89893091,  2.06231451,  1.46357048,  2.13891482,\n",
       "        2.9369247 ,  1.1190263 ,  2.48391843,  1.36972988,  2.82340598,\n",
       "        2.09440041,  2.71929216,  2.81834126,  0.51730448,  2.81576824,\n",
       "        3.2705524 ,  2.32364607,  2.87254143,  2.95348573,  1.98257864,\n",
       "        2.81047797,  2.44626784,  2.49244809,  2.85071707,  1.86305869,\n",
       "        2.69262958,  1.09150052,  2.77923179], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM Model Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph_emotions():\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(WINDOW_SIZE,feat_dim)))\n",
    "    model.add(LSTM(num_feat_map, \n",
    "                   return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(num_feat_map, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4,activation='softmax'))\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_21 (Batc (None, 25, 136)           544       \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 25, 16)            9792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 25, 16)            64        \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 12,580\n",
      "Trainable params: 12,276\n",
      "Non-trainable params: 304\n",
      "_________________________________________________________________\n",
      "Train on 491 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "491/491 [==============================] - 4s - loss: 0.1908 - acc: 0.2749 - val_loss: 0.1804 - val_acc: 0.5041\n",
      "Epoch 2/100\n",
      "491/491 [==============================] - 1s - loss: 0.1805 - acc: 0.3544 - val_loss: 0.1770 - val_acc: 0.4959\n",
      "Epoch 3/100\n",
      "491/491 [==============================] - 1s - loss: 0.1758 - acc: 0.4012 - val_loss: 0.1754 - val_acc: 0.4959\n",
      "Epoch 4/100\n",
      "491/491 [==============================] - 1s - loss: 0.1689 - acc: 0.4358 - val_loss: 0.1735 - val_acc: 0.4959\n",
      "Epoch 5/100\n",
      "491/491 [==============================] - 1s - loss: 0.1644 - acc: 0.4644 - val_loss: 0.1705 - val_acc: 0.4959\n",
      "Epoch 6/100\n",
      "491/491 [==============================] - 1s - loss: 0.1609 - acc: 0.4847 - val_loss: 0.1699 - val_acc: 0.4959\n",
      "Epoch 7/100\n",
      "491/491 [==============================] - 1s - loss: 0.1569 - acc: 0.5255 - val_loss: 0.1671 - val_acc: 0.4878\n",
      "Epoch 8/100\n",
      "491/491 [==============================] - 1s - loss: 0.1544 - acc: 0.5316 - val_loss: 0.1656 - val_acc: 0.4959\n",
      "Epoch 9/100\n",
      "491/491 [==============================] - 1s - loss: 0.1528 - acc: 0.5316 - val_loss: 0.1651 - val_acc: 0.4959\n",
      "Epoch 10/100\n",
      "491/491 [==============================] - 1s - loss: 0.1505 - acc: 0.5418 - val_loss: 0.1654 - val_acc: 0.4959\n",
      "Epoch 11/100\n",
      "491/491 [==============================] - 1s - loss: 0.1448 - acc: 0.5580 - val_loss: 0.1710 - val_acc: 0.4959\n",
      "Epoch 12/100\n",
      "491/491 [==============================] - 1s - loss: 0.1448 - acc: 0.5784 - val_loss: 0.1738 - val_acc: 0.4959\n",
      "Epoch 13/100\n",
      "491/491 [==============================] - 1s - loss: 0.1404 - acc: 0.5682 - val_loss: 0.1730 - val_acc: 0.4959\n",
      "Epoch 14/100\n",
      "491/491 [==============================] - 1s - loss: 0.1419 - acc: 0.5825 - val_loss: 0.1743 - val_acc: 0.4959\n",
      "Epoch 15/100\n",
      "491/491 [==============================] - 1s - loss: 0.1417 - acc: 0.5866 - val_loss: 0.1725 - val_acc: 0.4959\n",
      "Epoch 16/100\n",
      "491/491 [==============================] - 1s - loss: 0.1375 - acc: 0.6008 - val_loss: 0.1790 - val_acc: 0.5041\n",
      "Epoch 17/100\n",
      "491/491 [==============================] - 1s - loss: 0.1349 - acc: 0.6090 - val_loss: 0.1765 - val_acc: 0.4959\n",
      "Epoch 18/100\n",
      "491/491 [==============================] - 1s - loss: 0.1315 - acc: 0.6212 - val_loss: 0.1785 - val_acc: 0.5041\n",
      "Epoch 19/100\n",
      "491/491 [==============================] - 1s - loss: 0.1334 - acc: 0.6151 - val_loss: 0.1766 - val_acc: 0.5041\n",
      "Epoch 20/100\n",
      "491/491 [==============================] - 1s - loss: 0.1334 - acc: 0.6293 - val_loss: 0.1772 - val_acc: 0.5041\n",
      "Epoch 21/100\n",
      "491/491 [==============================] - 1s - loss: 0.1293 - acc: 0.6232 - val_loss: 0.1723 - val_acc: 0.4959\n",
      "Epoch 22/100\n",
      "491/491 [==============================] - 1s - loss: 0.1293 - acc: 0.6110 - val_loss: 0.1685 - val_acc: 0.4797\n",
      "Epoch 23/100\n",
      "491/491 [==============================] - 1s - loss: 0.1324 - acc: 0.6110 - val_loss: 0.1638 - val_acc: 0.4634\n",
      "Epoch 24/100\n",
      "491/491 [==============================] - 1s - loss: 0.1265 - acc: 0.6660 - val_loss: 0.1654 - val_acc: 0.4553\n",
      "Epoch 25/100\n",
      "491/491 [==============================] - 1s - loss: 0.1234 - acc: 0.6599 - val_loss: 0.1623 - val_acc: 0.4634\n",
      "Epoch 26/100\n",
      "491/491 [==============================] - 1s - loss: 0.1270 - acc: 0.6212 - val_loss: 0.1601 - val_acc: 0.4959\n",
      "Epoch 27/100\n",
      "491/491 [==============================] - 1s - loss: 0.1241 - acc: 0.6578 - val_loss: 0.1640 - val_acc: 0.4878\n",
      "Epoch 28/100\n",
      "491/491 [==============================] - 1s - loss: 0.1236 - acc: 0.6517 - val_loss: 0.1552 - val_acc: 0.5122\n",
      "Epoch 29/100\n",
      "491/491 [==============================] - 1s - loss: 0.1183 - acc: 0.6640 - val_loss: 0.1545 - val_acc: 0.5447\n",
      "Epoch 30/100\n",
      "491/491 [==============================] - 1s - loss: 0.1249 - acc: 0.6619 - val_loss: 0.1562 - val_acc: 0.5366\n",
      "Epoch 31/100\n",
      "491/491 [==============================] - 1s - loss: 0.1223 - acc: 0.6782 - val_loss: 0.1556 - val_acc: 0.5528\n",
      "Epoch 32/100\n",
      "491/491 [==============================] - 1s - loss: 0.1197 - acc: 0.6538 - val_loss: 0.1543 - val_acc: 0.5447\n",
      "Epoch 33/100\n",
      "491/491 [==============================] - 1s - loss: 0.1161 - acc: 0.6823 - val_loss: 0.1531 - val_acc: 0.5122\n",
      "Epoch 34/100\n",
      "491/491 [==============================] - 1s - loss: 0.1126 - acc: 0.6741 - val_loss: 0.1493 - val_acc: 0.5691\n",
      "Epoch 35/100\n",
      "491/491 [==============================] - 1s - loss: 0.1126 - acc: 0.7006 - val_loss: 0.1498 - val_acc: 0.5772\n",
      "Epoch 36/100\n",
      "491/491 [==============================] - 1s - loss: 0.1117 - acc: 0.6986 - val_loss: 0.1537 - val_acc: 0.5854\n",
      "Epoch 37/100\n",
      "491/491 [==============================] - 1s - loss: 0.1081 - acc: 0.6884 - val_loss: 0.1595 - val_acc: 0.5203\n",
      "Epoch 38/100\n",
      "491/491 [==============================] - 1s - loss: 0.1096 - acc: 0.6843 - val_loss: 0.1458 - val_acc: 0.5854\n",
      "Epoch 39/100\n",
      "491/491 [==============================] - 1s - loss: 0.1086 - acc: 0.7006 - val_loss: 0.1546 - val_acc: 0.5285\n",
      "Epoch 40/100\n",
      "491/491 [==============================] - 1s - loss: 0.1063 - acc: 0.7006 - val_loss: 0.1559 - val_acc: 0.5122\n",
      "Epoch 41/100\n",
      "491/491 [==============================] - 1s - loss: 0.1115 - acc: 0.7067 - val_loss: 0.1627 - val_acc: 0.5041\n",
      "Epoch 42/100\n",
      "491/491 [==============================] - 1s - loss: 0.1168 - acc: 0.6823 - val_loss: 0.1630 - val_acc: 0.4959\n",
      "Epoch 43/100\n",
      "491/491 [==============================] - 1s - loss: 0.1080 - acc: 0.7169 - val_loss: 0.1619 - val_acc: 0.4959\n",
      "Epoch 44/100\n",
      "491/491 [==============================] - 1s - loss: 0.1065 - acc: 0.7108 - val_loss: 0.1584 - val_acc: 0.5203\n",
      "Epoch 45/100\n",
      "491/491 [==============================] - 1s - loss: 0.1064 - acc: 0.6925 - val_loss: 0.1567 - val_acc: 0.5366\n",
      "Epoch 46/100\n",
      "491/491 [==============================] - 1s - loss: 0.1037 - acc: 0.7251 - val_loss: 0.1568 - val_acc: 0.5447\n",
      "Epoch 47/100\n",
      "491/491 [==============================] - 1s - loss: 0.1032 - acc: 0.7251 - val_loss: 0.1580 - val_acc: 0.5528\n",
      "Epoch 48/100\n",
      "491/491 [==============================] - 1s - loss: 0.0995 - acc: 0.7251 - val_loss: 0.1561 - val_acc: 0.5447\n",
      "Epoch 49/100\n",
      "491/491 [==============================] - 1s - loss: 0.1084 - acc: 0.7047 - val_loss: 0.1582 - val_acc: 0.5366\n",
      "Epoch 50/100\n",
      "491/491 [==============================] - 1s - loss: 0.1030 - acc: 0.7026 - val_loss: 0.1663 - val_acc: 0.5041\n",
      "Epoch 51/100\n",
      "491/491 [==============================] - 1s - loss: 0.1058 - acc: 0.7088 - val_loss: 0.1665 - val_acc: 0.5122\n",
      "Epoch 52/100\n",
      "491/491 [==============================] - 1s - loss: 0.0998 - acc: 0.7475 - val_loss: 0.1592 - val_acc: 0.5122\n",
      "Epoch 53/100\n",
      "491/491 [==============================] - 1s - loss: 0.0988 - acc: 0.7495 - val_loss: 0.1652 - val_acc: 0.4959\n",
      "Epoch 54/100\n",
      "491/491 [==============================] - 1s - loss: 0.1027 - acc: 0.7149 - val_loss: 0.1659 - val_acc: 0.5122\n",
      "Epoch 55/100\n",
      "491/491 [==============================] - 1s - loss: 0.0987 - acc: 0.7413 - val_loss: 0.1674 - val_acc: 0.4959\n",
      "Epoch 56/100\n",
      "491/491 [==============================] - 1s - loss: 0.0948 - acc: 0.7678 - val_loss: 0.1638 - val_acc: 0.5041\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 1s - loss: 0.0956 - acc: 0.7373 - val_loss: 0.1590 - val_acc: 0.5203\n",
      "Epoch 58/100\n",
      "491/491 [==============================] - 1s - loss: 0.0939 - acc: 0.7637 - val_loss: 0.1579 - val_acc: 0.5122\n",
      "Epoch 59/100\n",
      "491/491 [==============================] - 1s - loss: 0.0974 - acc: 0.7169 - val_loss: 0.1662 - val_acc: 0.5122\n",
      "Epoch 60/100\n",
      "491/491 [==============================] - 1s - loss: 0.0934 - acc: 0.7556 - val_loss: 0.1673 - val_acc: 0.5041\n",
      "Epoch 61/100\n",
      "491/491 [==============================] - 1s - loss: 0.0972 - acc: 0.7251 - val_loss: 0.1669 - val_acc: 0.5447\n",
      "Epoch 62/100\n",
      "491/491 [==============================] - 1s - loss: 0.0952 - acc: 0.7413 - val_loss: 0.1623 - val_acc: 0.5447\n",
      "Epoch 63/100\n",
      "491/491 [==============================] - 1s - loss: 0.0861 - acc: 0.7760 - val_loss: 0.1655 - val_acc: 0.5203\n",
      "Epoch 64/100\n",
      "491/491 [==============================] - 1s - loss: 0.0885 - acc: 0.7699 - val_loss: 0.1656 - val_acc: 0.5366\n",
      "Epoch 65/100\n",
      "491/491 [==============================] - 1s - loss: 0.0835 - acc: 0.7882 - val_loss: 0.1662 - val_acc: 0.5285\n",
      "Epoch 66/100\n",
      "491/491 [==============================] - 1s - loss: 0.0854 - acc: 0.7699 - val_loss: 0.1654 - val_acc: 0.5041\n",
      "Epoch 67/100\n",
      "491/491 [==============================] - 1s - loss: 0.0804 - acc: 0.7902 - val_loss: 0.1673 - val_acc: 0.5203\n",
      "Epoch 68/100\n",
      "491/491 [==============================] - 1s - loss: 0.0917 - acc: 0.7699 - val_loss: 0.1667 - val_acc: 0.5203\n",
      "Epoch 69/100\n",
      "491/491 [==============================] - 1s - loss: 0.0850 - acc: 0.7658 - val_loss: 0.1614 - val_acc: 0.5610\n",
      "Epoch 70/100\n",
      "491/491 [==============================] - 1s - loss: 0.0861 - acc: 0.7617 - val_loss: 0.1731 - val_acc: 0.4959\n",
      "Epoch 71/100\n",
      "491/491 [==============================] - 1s - loss: 0.0862 - acc: 0.7821 - val_loss: 0.1641 - val_acc: 0.5203\n",
      "Epoch 72/100\n",
      "491/491 [==============================] - 1s - loss: 0.0895 - acc: 0.7536 - val_loss: 0.1735 - val_acc: 0.4878\n",
      "Epoch 73/100\n",
      "491/491 [==============================] - 1s - loss: 0.0828 - acc: 0.7841 - val_loss: 0.1730 - val_acc: 0.4878\n",
      "Epoch 74/100\n",
      "491/491 [==============================] - 1s - loss: 0.0799 - acc: 0.7963 - val_loss: 0.1739 - val_acc: 0.5041\n",
      "Epoch 75/100\n",
      "491/491 [==============================] - 1s - loss: 0.0939 - acc: 0.7515 - val_loss: 0.1853 - val_acc: 0.5041\n",
      "Epoch 76/100\n",
      "491/491 [==============================] - 1s - loss: 0.0892 - acc: 0.7719 - val_loss: 0.1837 - val_acc: 0.4878\n",
      "Epoch 77/100\n",
      "491/491 [==============================] - 1s - loss: 0.0826 - acc: 0.8106 - val_loss: 0.1896 - val_acc: 0.4553\n",
      "Epoch 78/100\n",
      "491/491 [==============================] - 1s - loss: 0.0931 - acc: 0.7576 - val_loss: 0.1815 - val_acc: 0.5122\n",
      "Epoch 79/100\n",
      "491/491 [==============================] - 1s - loss: 0.0938 - acc: 0.7495 - val_loss: 0.1806 - val_acc: 0.5041\n",
      "Epoch 80/100\n",
      "491/491 [==============================] - 1s - loss: 0.0813 - acc: 0.7780 - val_loss: 0.1811 - val_acc: 0.4878\n",
      "Epoch 81/100\n",
      "491/491 [==============================] - 1s - loss: 0.0855 - acc: 0.7760 - val_loss: 0.1781 - val_acc: 0.5122\n",
      "Epoch 82/100\n",
      "491/491 [==============================] - 1s - loss: 0.0812 - acc: 0.7862 - val_loss: 0.1744 - val_acc: 0.5122\n",
      "Epoch 83/100\n",
      "491/491 [==============================] - 1s - loss: 0.0772 - acc: 0.8147 - val_loss: 0.1696 - val_acc: 0.5203\n",
      "Epoch 84/100\n",
      "491/491 [==============================] - 1s - loss: 0.0896 - acc: 0.7739 - val_loss: 0.1720 - val_acc: 0.5447\n",
      "Epoch 85/100\n",
      "491/491 [==============================] - 1s - loss: 0.0721 - acc: 0.8147 - val_loss: 0.1728 - val_acc: 0.5447\n",
      "Epoch 86/100\n",
      "491/491 [==============================] - 1s - loss: 0.0724 - acc: 0.8187 - val_loss: 0.1705 - val_acc: 0.5447\n",
      "Epoch 87/100\n",
      "491/491 [==============================] - 1s - loss: 0.0778 - acc: 0.7882 - val_loss: 0.1710 - val_acc: 0.5366\n",
      "Epoch 88/100\n",
      "491/491 [==============================] - 1s - loss: 0.0821 - acc: 0.7800 - val_loss: 0.1699 - val_acc: 0.5528\n",
      "Epoch 89/100\n",
      "491/491 [==============================] - 1s - loss: 0.0721 - acc: 0.8167 - val_loss: 0.1785 - val_acc: 0.5285\n",
      "Epoch 90/100\n",
      "491/491 [==============================] - 1s - loss: 0.0789 - acc: 0.7984 - val_loss: 0.1758 - val_acc: 0.5285\n",
      "Epoch 91/100\n",
      "491/491 [==============================] - 1s - loss: 0.0816 - acc: 0.7780 - val_loss: 0.1807 - val_acc: 0.5122\n",
      "Epoch 92/100\n",
      "491/491 [==============================] - 1s - loss: 0.0910 - acc: 0.7719 - val_loss: 0.1847 - val_acc: 0.4878\n",
      "Epoch 93/100\n",
      "491/491 [==============================] - 1s - loss: 0.0931 - acc: 0.7515 - val_loss: 0.1775 - val_acc: 0.5447\n",
      "Epoch 94/100\n",
      "491/491 [==============================] - 1s - loss: 0.0905 - acc: 0.7678 - val_loss: 0.1821 - val_acc: 0.4959\n",
      "Epoch 95/100\n",
      "491/491 [==============================] - 1s - loss: 0.0861 - acc: 0.7678 - val_loss: 0.1870 - val_acc: 0.4878\n",
      "Epoch 96/100\n",
      "491/491 [==============================] - 1s - loss: 0.0833 - acc: 0.7841 - val_loss: 0.1919 - val_acc: 0.4878\n",
      "Epoch 97/100\n",
      "491/491 [==============================] - 1s - loss: 0.0771 - acc: 0.8045 - val_loss: 0.1814 - val_acc: 0.5285\n",
      "Epoch 98/100\n",
      "491/491 [==============================] - 1s - loss: 0.0827 - acc: 0.7699 - val_loss: 0.1959 - val_acc: 0.4797\n",
      "Epoch 99/100\n",
      "491/491 [==============================] - 1s - loss: 0.0804 - acc: 0.7943 - val_loss: 0.1862 - val_acc: 0.5041\n",
      "Epoch 100/100\n",
      "491/491 [==============================] - 1s - loss: 0.0870 - acc: 0.7658 - val_loss: 0.1784 - val_acc: 0.5366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190c9218c88>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 16\n",
    "feat_dim = 136\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y2 = label_encoder.fit_transform(y2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = x_train.reshape((491,25,-1))\n",
    "x_test = x_test.reshape((123,25,-1))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "create_graph_emotions()\n",
    "\n",
    "epochs = 100\n",
    "batch_size=32\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "#filepath=\"weights_best_arousal.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 2, 2, 0, 3, 0, 3, 0, 2, 0, 0, 2, 3, 2, 2, 0, 0, 2, 2, 1,\n",
       "       3, 2, 0, 2, 2, 3, 2, 3, 1, 3, 2, 1, 2, 2, 2, 3, 2, 0, 2, 2, 2, 1, 2,\n",
       "       3, 2, 2, 2, 2, 2, 1, 2, 0, 0, 3, 2, 2, 2, 1, 3, 2, 0, 3, 2, 3, 2, 2,\n",
       "       0, 1, 3, 0, 0, 3, 3, 2, 1, 0, 1, 3, 2, 2, 2, 3, 3, 2, 2, 3, 1, 1, 3,\n",
       "       2, 2, 2, 2, 2, 2, 0, 0, 3, 0, 2, 0, 2, 2, 2, 1, 2, 2, 3, 1, 2, 3, 2,\n",
       "       3, 1, 0, 0, 3, 3, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 2, 1, 2, 2, 2, 0, 2, 0, 2, 0, 0, 1, 3, 2, 2, 0, 3, 2, 0, 0,\n",
       "       3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 0, 2, 1, 0, 3, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 3, 1, 0, 2, 3, 3, 1, 2, 3, 2,\n",
       "       0, 1, 3, 0, 2, 2, 2, 2, 0, 2, 0, 3, 0, 0, 2, 2, 3, 2, 2, 3, 1, 0, 3,\n",
       "       0, 0, 0, 1, 1, 0, 2, 0, 3, 2, 2, 0, 2, 2, 3, 0, 2, 0, 2, 1, 2, 2, 1,\n",
       "       2, 1, 2, 2, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.argmax(y_test,axis=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  7, 10,  2],\n",
       "       [ 0,  6,  8,  1],\n",
       "       [ 9,  1, 37, 14],\n",
       "       [ 2,  0,  3, 12]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.37      0.42        30\n",
      "          1       0.43      0.40      0.41        15\n",
      "          2       0.64      0.61      0.62        61\n",
      "          3       0.41      0.71      0.52        17\n",
      "\n",
      "avg / total       0.55      0.54      0.53       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
