{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, BatchNormalization,concatenate,Input\n",
    "from keras.layers.core import Permute, Reshape\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import Regularizer\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening Face Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n"
     ]
    }
   ],
   "source": [
    "FILENAME = 'features_points.h5'\n",
    "h5f = h5py.File(FILENAME,'r')\n",
    "\n",
    "for key in h5f.keys():\n",
    "    print(key) #Names of the groups in HDF5 file.\n",
    "x = h5f['points'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting AUX input into system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_inputs = pd.read_csv('aux_inputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionID</th>\n",
       "      <th>Female (y=1)</th>\n",
       "      <th>Length(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001-001-l000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S001-001-l001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S001-001-l002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S001-003-l000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S001-003-l001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SessionID  Female (y=1)  Length(s)\n",
       "0  S001-001-l000             0      0.822\n",
       "1  S001-001-l001             0      0.912\n",
       "2  S001-001-l002             0      0.834\n",
       "3  S001-003-l000             0      0.552\n",
       "4  S001-003-l001             0      0.576"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Common Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "\n",
    "with open(\"train_indx.p\", 'rb') as pickleFile:\n",
    "    train_idx = pickle.load(pickleFile)\n",
    "with open(\"test_indx.p\", 'rb') as pickleFile:\n",
    "    test_idx = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx))\n",
    "print(len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting targets in System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionID</th>\n",
       "      <th>Session</th>\n",
       "      <th>Type</th>\n",
       "      <th>Merged Arousal</th>\n",
       "      <th>Merged Dominance</th>\n",
       "      <th>Merged Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>S019-001-l005</td>\n",
       "      <td>130</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Schadenfreude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>S002-005-l005</td>\n",
       "      <td>13</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>S007-007-l003</td>\n",
       "      <td>62</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>S011-001-l002</td>\n",
       "      <td>76</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Schadenfreude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S001-003-l005</td>\n",
       "      <td>3</td>\n",
       "      <td>Laughter</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Schadenfreude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionID  Session      Type  Merged Arousal  Merged Dominance  \\\n",
       "406  S019-001-l005      130  Laughter               4                 4   \n",
       "32   S002-005-l005       13  Laughter               3                 2   \n",
       "196  S007-007-l003       62  Laughter               2                 2   \n",
       "265  S011-001-l002       76  Laughter               2                 3   \n",
       "8    S001-003-l005        3  Laughter               3                 2   \n",
       "\n",
       "    Merged Emotion  \n",
       "406  Schadenfreude  \n",
       "32           Happy  \n",
       "196          Happy  \n",
       "265  Schadenfreude  \n",
       "8    Schadenfreude  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('merged_labels.csv')\n",
    "\n",
    "target_train = targets.iloc[train_idx]\n",
    "target_test = targets.iloc[test_idx]\n",
    "\n",
    "target_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 136, 25, 1)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(target_train['Merged Arousal'])\n",
    "y_test = np.array(target_test['Merged Arousal'])\n",
    "y1_train = target_train['Merged Dominance']\n",
    "y1_test = target_test['Merged Dominance']\n",
    "y2_train = target_train['Merged Emotion']\n",
    "y2_test = target_test['Merged Emotion']\n",
    "\n",
    "aux_inputs_train = np.array(aux_inputs[['Female (y=1)','Length(s)']].iloc[train_idx])\n",
    "aux_inputs_test = np.array(aux_inputs[['Female (y=1)','Length(s)']].iloc[test_idx])\n",
    "\n",
    "x_train = x[train_idx,:,:,:]\n",
    "x_test = x[test_idx,:,:,:]\n",
    "\n",
    "x_train = x_train.reshape((len(train_idx),25,-1))\n",
    "x_test = x_test.reshape((len(test_idx),25,-1))\n",
    "\n",
    "x_train = np.swapaxes(x_train,1,2)\n",
    "x_train = np.reshape(x_train, (-1, feat_dim, WINDOW_SIZE, 1))\n",
    "x_test = np.swapaxes(x_test,1,2)\n",
    "x_test = np.reshape(x_test, (-1, feat_dim, WINDOW_SIZE, 1))\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph_arousal_dominance():\n",
    "    \n",
    "    aux_input = Input(shape=(2,),dtype='float32', name='aux_input')\n",
    "    \n",
    "    inputs = Input(shape=(feat_dim,WINDOW_SIZE,1), name='main_input')\n",
    "    \n",
    "    x = BatchNormalization()(inputs)\n",
    "    \n",
    "    x = Conv2D(num_feat_map, kernel_size=(1, 5),\n",
    "                 activation='relu',\n",
    "                 padding='same')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(num_feat_map, kernel_size=(1, 5), activation='relu',padding='same')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Permute((2, 1, 3))(x) # for swap-dimension\n",
    "    x = Reshape((-1,num_feat_map*feat_dim))(x)\n",
    "\n",
    "    x = LSTM(num_feat_map, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x,aux_input])\n",
    "    main_output = Dense(1,activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs, aux_input], outputs=main_output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv LSTM Model Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 32\n",
    "feat_dim = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 136, 25)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 136, 25, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 136, 25, 1)    4           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 136, 25, 32)   192         batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 136, 25, 32)   0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 136, 25, 32)   128         dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 136, 25, 32)   5152        batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 136, 25, 32)   0           conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "permute_5 (Permute)              (None, 25, 136, 32)   0           dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 25, 4352)      0           permute_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 32)            561280      reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 32)            0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 34)            0           dropout_15[0][0]                 \n",
      "                                                                   aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             35          concatenate_5[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 566,791\n",
      "Trainable params: 566,725\n",
      "Non-trainable params: 66\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_graph_arousal_dominance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 405 samples, validate on 209 samples\n",
      "Epoch 1/150\n",
      "405/405 [==============================] - 8s - loss: 1.7127 - mean_squared_error: 1.7127 - val_loss: 1.0030 - val_mean_squared_error: 1.0030\n",
      "Epoch 2/150\n",
      "405/405 [==============================] - 2s - loss: 1.3646 - mean_squared_error: 1.3646 - val_loss: 1.0236 - val_mean_squared_error: 1.0236\n",
      "Epoch 3/150\n",
      "405/405 [==============================] - 2s - loss: 1.4056 - mean_squared_error: 1.4056 - val_loss: 0.9978 - val_mean_squared_error: 0.9978\n",
      "Epoch 4/150\n",
      "405/405 [==============================] - 2s - loss: 1.3731 - mean_squared_error: 1.3731 - val_loss: 1.0188 - val_mean_squared_error: 1.0188\n",
      "Epoch 5/150\n",
      "405/405 [==============================] - 2s - loss: 1.3698 - mean_squared_error: 1.3698 - val_loss: 1.0621 - val_mean_squared_error: 1.0621\n",
      "Epoch 6/150\n",
      "405/405 [==============================] - 2s - loss: 1.3605 - mean_squared_error: 1.3605 - val_loss: 1.0027 - val_mean_squared_error: 1.0027\n",
      "Epoch 7/150\n",
      "405/405 [==============================] - 2s - loss: 1.3094 - mean_squared_error: 1.3094 - val_loss: 1.0073 - val_mean_squared_error: 1.0073\n",
      "Epoch 8/150\n",
      "405/405 [==============================] - 2s - loss: 1.3172 - mean_squared_error: 1.3172 - val_loss: 0.9812 - val_mean_squared_error: 0.9812\n",
      "Epoch 9/150\n",
      "405/405 [==============================] - 2s - loss: 1.3664 - mean_squared_error: 1.3664 - val_loss: 0.9485 - val_mean_squared_error: 0.9485\n",
      "Epoch 10/150\n",
      "405/405 [==============================] - 2s - loss: 1.4451 - mean_squared_error: 1.4451 - val_loss: 0.9538 - val_mean_squared_error: 0.9538\n",
      "Epoch 11/150\n",
      "405/405 [==============================] - 2s - loss: 1.5314 - mean_squared_error: 1.5314 - val_loss: 0.9525 - val_mean_squared_error: 0.9525\n",
      "Epoch 12/150\n",
      "405/405 [==============================] - 2s - loss: 1.3305 - mean_squared_error: 1.3305 - val_loss: 0.9778 - val_mean_squared_error: 0.9778\n",
      "Epoch 13/150\n",
      "405/405 [==============================] - 2s - loss: 1.3881 - mean_squared_error: 1.3881 - val_loss: 1.0061 - val_mean_squared_error: 1.0061\n",
      "Epoch 14/150\n",
      "405/405 [==============================] - 2s - loss: 1.3646 - mean_squared_error: 1.3646 - val_loss: 0.9569 - val_mean_squared_error: 0.9569\n",
      "Epoch 15/150\n",
      "405/405 [==============================] - 2s - loss: 1.3022 - mean_squared_error: 1.3022 - val_loss: 0.9461 - val_mean_squared_error: 0.9461\n",
      "Epoch 16/150\n",
      "405/405 [==============================] - 2s - loss: 1.3265 - mean_squared_error: 1.3265 - val_loss: 0.8962 - val_mean_squared_error: 0.8962\n",
      "Epoch 17/150\n",
      "405/405 [==============================] - 2s - loss: 1.2316 - mean_squared_error: 1.2316 - val_loss: 0.8884 - val_mean_squared_error: 0.8884\n",
      "Epoch 18/150\n",
      "405/405 [==============================] - 2s - loss: 1.3115 - mean_squared_error: 1.3115 - val_loss: 0.9077 - val_mean_squared_error: 0.9077\n",
      "Epoch 19/150\n",
      "405/405 [==============================] - 2s - loss: 1.3090 - mean_squared_error: 1.3090 - val_loss: 0.8596 - val_mean_squared_error: 0.8596\n",
      "Epoch 20/150\n",
      "405/405 [==============================] - 2s - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 0.8312 - val_mean_squared_error: 0.8312\n",
      "Epoch 21/150\n",
      "405/405 [==============================] - 2s - loss: 1.2623 - mean_squared_error: 1.2623 - val_loss: 0.8163 - val_mean_squared_error: 0.8163\n",
      "Epoch 22/150\n",
      "405/405 [==============================] - 2s - loss: 1.1530 - mean_squared_error: 1.1530 - val_loss: 0.8042 - val_mean_squared_error: 0.8042\n",
      "Epoch 23/150\n",
      "405/405 [==============================] - 2s - loss: 1.2042 - mean_squared_error: 1.2042 - val_loss: 0.8327 - val_mean_squared_error: 0.8327\n",
      "Epoch 24/150\n",
      "405/405 [==============================] - 2s - loss: 1.1150 - mean_squared_error: 1.1150 - val_loss: 0.9927 - val_mean_squared_error: 0.9927\n",
      "Epoch 25/150\n",
      "405/405 [==============================] - 2s - loss: 1.2554 - mean_squared_error: 1.2554 - val_loss: 0.8435 - val_mean_squared_error: 0.8435\n",
      "Epoch 26/150\n",
      "405/405 [==============================] - 2s - loss: 1.1186 - mean_squared_error: 1.1186 - val_loss: 0.8023 - val_mean_squared_error: 0.8023\n",
      "Epoch 27/150\n",
      "405/405 [==============================] - 2s - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 0.7670 - val_mean_squared_error: 0.7670\n",
      "Epoch 28/150\n",
      "405/405 [==============================] - 2s - loss: 1.0787 - mean_squared_error: 1.0787 - val_loss: 0.7667 - val_mean_squared_error: 0.7667\n",
      "Epoch 29/150\n",
      "405/405 [==============================] - 2s - loss: 1.2176 - mean_squared_error: 1.2176 - val_loss: 0.7526 - val_mean_squared_error: 0.7526\n",
      "Epoch 30/150\n",
      "405/405 [==============================] - 2s - loss: 1.1598 - mean_squared_error: 1.1598 - val_loss: 0.7402 - val_mean_squared_error: 0.7402\n",
      "Epoch 31/150\n",
      "405/405 [==============================] - 2s - loss: 1.2608 - mean_squared_error: 1.2608 - val_loss: 0.7589 - val_mean_squared_error: 0.7589\n",
      "Epoch 32/150\n",
      "405/405 [==============================] - 2s - loss: 1.2254 - mean_squared_error: 1.2254 - val_loss: 0.7462 - val_mean_squared_error: 0.7462\n",
      "Epoch 33/150\n",
      "405/405 [==============================] - 2s - loss: 1.1609 - mean_squared_error: 1.1609 - val_loss: 0.7267 - val_mean_squared_error: 0.7267\n",
      "Epoch 34/150\n",
      "405/405 [==============================] - 2s - loss: 1.0333 - mean_squared_error: 1.0333 - val_loss: 0.7284 - val_mean_squared_error: 0.7284\n",
      "Epoch 35/150\n",
      "405/405 [==============================] - 2s - loss: 1.1188 - mean_squared_error: 1.1188 - val_loss: 0.7308 - val_mean_squared_error: 0.7308\n",
      "Epoch 36/150\n",
      "405/405 [==============================] - 2s - loss: 1.0421 - mean_squared_error: 1.0421 - val_loss: 0.7336 - val_mean_squared_error: 0.7336\n",
      "Epoch 37/150\n",
      "405/405 [==============================] - 2s - loss: 1.0433 - mean_squared_error: 1.0433 - val_loss: 0.7346 - val_mean_squared_error: 0.7346\n",
      "Epoch 38/150\n",
      "405/405 [==============================] - 2s - loss: 1.1320 - mean_squared_error: 1.1320 - val_loss: 0.7241 - val_mean_squared_error: 0.7241\n",
      "Epoch 39/150\n",
      "405/405 [==============================] - 2s - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 0.7325 - val_mean_squared_error: 0.7325\n",
      "Epoch 40/150\n",
      "405/405 [==============================] - 2s - loss: 1.0118 - mean_squared_error: 1.0118 - val_loss: 0.7406 - val_mean_squared_error: 0.7406\n",
      "Epoch 41/150\n",
      "405/405 [==============================] - 2s - loss: 1.0948 - mean_squared_error: 1.0948 - val_loss: 0.7532 - val_mean_squared_error: 0.7532\n",
      "Epoch 42/150\n",
      "405/405 [==============================] - 2s - loss: 1.1721 - mean_squared_error: 1.1721 - val_loss: 0.7663 - val_mean_squared_error: 0.7663\n",
      "Epoch 43/150\n",
      "405/405 [==============================] - 2s - loss: 1.0791 - mean_squared_error: 1.0791 - val_loss: 0.7124 - val_mean_squared_error: 0.7124\n",
      "Epoch 44/150\n",
      "405/405 [==============================] - 2s - loss: 1.0516 - mean_squared_error: 1.0516 - val_loss: 0.7279 - val_mean_squared_error: 0.7279\n",
      "Epoch 45/150\n",
      "405/405 [==============================] - 2s - loss: 1.0394 - mean_squared_error: 1.0394 - val_loss: 0.7177 - val_mean_squared_error: 0.7177\n",
      "Epoch 46/150\n",
      "405/405 [==============================] - 2s - loss: 1.0498 - mean_squared_error: 1.0498 - val_loss: 0.7174 - val_mean_squared_error: 0.7174\n",
      "Epoch 47/150\n",
      "405/405 [==============================] - 2s - loss: 1.0800 - mean_squared_error: 1.0800 - val_loss: 0.7644 - val_mean_squared_error: 0.7644\n",
      "Epoch 48/150\n",
      "405/405 [==============================] - 2s - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 0.7061 - val_mean_squared_error: 0.7061\n",
      "Epoch 49/150\n",
      "405/405 [==============================] - 2s - loss: 1.0816 - mean_squared_error: 1.0816 - val_loss: 0.7716 - val_mean_squared_error: 0.7716\n",
      "Epoch 50/150\n",
      "405/405 [==============================] - 2s - loss: 1.0263 - mean_squared_error: 1.0263 - val_loss: 0.7701 - val_mean_squared_error: 0.7701\n",
      "Epoch 51/150\n",
      "405/405 [==============================] - 2s - loss: 1.1076 - mean_squared_error: 1.1076 - val_loss: 0.7506 - val_mean_squared_error: 0.7506\n",
      "Epoch 52/150\n",
      "405/405 [==============================] - 2s - loss: 1.1105 - mean_squared_error: 1.1105 - val_loss: 0.7130 - val_mean_squared_error: 0.7130\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 2s - loss: 0.9362 - mean_squared_error: 0.9362 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 54/150\n",
      "405/405 [==============================] - 2s - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 0.7744 - val_mean_squared_error: 0.7744\n",
      "Epoch 55/150\n",
      "405/405 [==============================] - 2s - loss: 1.0378 - mean_squared_error: 1.0378 - val_loss: 0.7129 - val_mean_squared_error: 0.7129\n",
      "Epoch 56/150\n",
      "405/405 [==============================] - 2s - loss: 1.0447 - mean_squared_error: 1.0447 - val_loss: 0.8068 - val_mean_squared_error: 0.8068\n",
      "Epoch 57/150\n",
      "405/405 [==============================] - 2s - loss: 1.0599 - mean_squared_error: 1.0599 - val_loss: 0.7592 - val_mean_squared_error: 0.7592\n",
      "Epoch 58/150\n",
      "405/405 [==============================] - 2s - loss: 1.1131 - mean_squared_error: 1.1131 - val_loss: 0.7146 - val_mean_squared_error: 0.7146\n",
      "Epoch 59/150\n",
      "405/405 [==============================] - 2s - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 0.7076 - val_mean_squared_error: 0.7076\n",
      "Epoch 60/150\n",
      "405/405 [==============================] - 2s - loss: 1.0821 - mean_squared_error: 1.0821 - val_loss: 0.7121 - val_mean_squared_error: 0.7121\n",
      "Epoch 61/150\n",
      "405/405 [==============================] - 2s - loss: 1.0553 - mean_squared_error: 1.0553 - val_loss: 0.7337 - val_mean_squared_error: 0.7337\n",
      "Epoch 62/150\n",
      "405/405 [==============================] - 2s - loss: 1.0633 - mean_squared_error: 1.0633 - val_loss: 0.7088 - val_mean_squared_error: 0.7088\n",
      "Epoch 63/150\n",
      "405/405 [==============================] - 2s - loss: 1.0640 - mean_squared_error: 1.0640 - val_loss: 0.6995 - val_mean_squared_error: 0.6995\n",
      "Epoch 64/150\n",
      "405/405 [==============================] - 2s - loss: 1.0063 - mean_squared_error: 1.0063 - val_loss: 0.7196 - val_mean_squared_error: 0.7196\n",
      "Epoch 65/150\n",
      "405/405 [==============================] - 2s - loss: 1.0327 - mean_squared_error: 1.0327 - val_loss: 0.7009 - val_mean_squared_error: 0.7009\n",
      "Epoch 66/150\n",
      "405/405 [==============================] - 2s - loss: 1.0507 - mean_squared_error: 1.0507 - val_loss: 0.7853 - val_mean_squared_error: 0.7853\n",
      "Epoch 67/150\n",
      "405/405 [==============================] - 2s - loss: 0.9929 - mean_squared_error: 0.9929 - val_loss: 0.7181 - val_mean_squared_error: 0.7181\n",
      "Epoch 68/150\n",
      "405/405 [==============================] - 2s - loss: 0.9878 - mean_squared_error: 0.9878 - val_loss: 0.7091 - val_mean_squared_error: 0.7091\n",
      "Epoch 69/150\n",
      "405/405 [==============================] - 2s - loss: 0.9771 - mean_squared_error: 0.9771 - val_loss: 0.7624 - val_mean_squared_error: 0.7624\n",
      "Epoch 70/150\n",
      "405/405 [==============================] - 2s - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 0.7105 - val_mean_squared_error: 0.7105\n",
      "Epoch 71/150\n",
      "405/405 [==============================] - 2s - loss: 0.9617 - mean_squared_error: 0.9617 - val_loss: 0.7209 - val_mean_squared_error: 0.7209\n",
      "Epoch 72/150\n",
      "405/405 [==============================] - 2s - loss: 1.0383 - mean_squared_error: 1.0383 - val_loss: 0.7496 - val_mean_squared_error: 0.7496\n",
      "Epoch 73/150\n",
      "405/405 [==============================] - 2s - loss: 1.0524 - mean_squared_error: 1.0524 - val_loss: 0.7035 - val_mean_squared_error: 0.7035\n",
      "Epoch 74/150\n",
      "405/405 [==============================] - 2s - loss: 1.0307 - mean_squared_error: 1.0307 - val_loss: 0.7124 - val_mean_squared_error: 0.7124\n",
      "Epoch 75/150\n",
      "405/405 [==============================] - 2s - loss: 1.0699 - mean_squared_error: 1.0699 - val_loss: 0.7722 - val_mean_squared_error: 0.7722\n",
      "Epoch 76/150\n",
      "405/405 [==============================] - 2s - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 0.7190 - val_mean_squared_error: 0.7190\n",
      "Epoch 77/150\n",
      "405/405 [==============================] - 2s - loss: 1.0056 - mean_squared_error: 1.0056 - val_loss: 0.7032 - val_mean_squared_error: 0.7032\n",
      "Epoch 78/150\n",
      "405/405 [==============================] - 2s - loss: 1.0363 - mean_squared_error: 1.0363 - val_loss: 0.7074 - val_mean_squared_error: 0.7074\n",
      "Epoch 79/150\n",
      "405/405 [==============================] - 2s - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 0.7799 - val_mean_squared_error: 0.7799\n",
      "Epoch 80/150\n",
      "405/405 [==============================] - 2s - loss: 0.9730 - mean_squared_error: 0.9730 - val_loss: 0.7120 - val_mean_squared_error: 0.7120\n",
      "Epoch 81/150\n",
      "405/405 [==============================] - 2s - loss: 0.9525 - mean_squared_error: 0.9525 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 82/150\n",
      "405/405 [==============================] - 2s - loss: 1.0325 - mean_squared_error: 1.0325 - val_loss: 0.6918 - val_mean_squared_error: 0.6918\n",
      "Epoch 83/150\n",
      "405/405 [==============================] - 2s - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 0.7932 - val_mean_squared_error: 0.7932\n",
      "Epoch 84/150\n",
      "405/405 [==============================] - 2s - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 0.7759 - val_mean_squared_error: 0.7759\n",
      "Epoch 85/150\n",
      "405/405 [==============================] - 2s - loss: 0.9823 - mean_squared_error: 0.9823 - val_loss: 0.7471 - val_mean_squared_error: 0.7471\n",
      "Epoch 86/150\n",
      "405/405 [==============================] - 2s - loss: 0.9871 - mean_squared_error: 0.9871 - val_loss: 0.7670 - val_mean_squared_error: 0.7670\n",
      "Epoch 87/150\n",
      "405/405 [==============================] - 2s - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 0.7020 - val_mean_squared_error: 0.7020\n",
      "Epoch 88/150\n",
      "405/405 [==============================] - 2s - loss: 1.0002 - mean_squared_error: 1.0002 - val_loss: 0.7075 - val_mean_squared_error: 0.7075\n",
      "Epoch 89/150\n",
      "405/405 [==============================] - 2s - loss: 0.9386 - mean_squared_error: 0.9386 - val_loss: 0.7012 - val_mean_squared_error: 0.7012\n",
      "Epoch 90/150\n",
      "405/405 [==============================] - 2s - loss: 1.0321 - mean_squared_error: 1.0321 - val_loss: 0.7029 - val_mean_squared_error: 0.7029\n",
      "Epoch 91/150\n",
      "405/405 [==============================] - 2s - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 0.7063 - val_mean_squared_error: 0.7063\n",
      "Epoch 92/150\n",
      "405/405 [==============================] - 2s - loss: 0.9935 - mean_squared_error: 0.9935 - val_loss: 0.8263 - val_mean_squared_error: 0.8263\n",
      "Epoch 93/150\n",
      "405/405 [==============================] - 2s - loss: 0.9534 - mean_squared_error: 0.9534 - val_loss: 0.7976 - val_mean_squared_error: 0.7976\n",
      "Epoch 94/150\n",
      "405/405 [==============================] - 2s - loss: 0.9455 - mean_squared_error: 0.9455 - val_loss: 0.6918 - val_mean_squared_error: 0.6918\n",
      "Epoch 95/150\n",
      "405/405 [==============================] - 2s - loss: 0.9693 - mean_squared_error: 0.9693 - val_loss: 0.7695 - val_mean_squared_error: 0.7695\n",
      "Epoch 96/150\n",
      "405/405 [==============================] - 2s - loss: 0.9200 - mean_squared_error: 0.9200 - val_loss: 0.8238 - val_mean_squared_error: 0.8238\n",
      "Epoch 97/150\n",
      "405/405 [==============================] - 2s - loss: 1.1414 - mean_squared_error: 1.1414 - val_loss: 0.7368 - val_mean_squared_error: 0.7368\n",
      "Epoch 98/150\n",
      "405/405 [==============================] - 2s - loss: 0.9549 - mean_squared_error: 0.9549 - val_loss: 0.7151 - val_mean_squared_error: 0.7151\n",
      "Epoch 99/150\n",
      "405/405 [==============================] - 2s - loss: 0.9974 - mean_squared_error: 0.9974 - val_loss: 0.7346 - val_mean_squared_error: 0.7346\n",
      "Epoch 100/150\n",
      "405/405 [==============================] - 2s - loss: 0.9456 - mean_squared_error: 0.9456 - val_loss: 0.6953 - val_mean_squared_error: 0.6953\n",
      "Epoch 101/150\n",
      "405/405 [==============================] - 2s - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 0.7152 - val_mean_squared_error: 0.7152\n",
      "Epoch 102/150\n",
      "405/405 [==============================] - 2s - loss: 0.9464 - mean_squared_error: 0.9464 - val_loss: 0.7590 - val_mean_squared_error: 0.7590\n",
      "Epoch 103/150\n",
      "405/405 [==============================] - 2s - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 0.7870 - val_mean_squared_error: 0.7870\n",
      "Epoch 104/150\n",
      "405/405 [==============================] - 2s - loss: 0.9761 - mean_squared_error: 0.9761 - val_loss: 0.7032 - val_mean_squared_error: 0.7032\n",
      "Epoch 105/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 2s - loss: 0.9086 - mean_squared_error: 0.9086 - val_loss: 0.8197 - val_mean_squared_error: 0.8197\n",
      "Epoch 106/150\n",
      "405/405 [==============================] - 2s - loss: 1.0230 - mean_squared_error: 1.0230 - val_loss: 0.8171 - val_mean_squared_error: 0.8171\n",
      "Epoch 107/150\n",
      "405/405 [==============================] - 2s - loss: 1.0555 - mean_squared_error: 1.0555 - val_loss: 0.8026 - val_mean_squared_error: 0.8026\n",
      "Epoch 108/150\n",
      "405/405 [==============================] - 2s - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 0.7276 - val_mean_squared_error: 0.7276\n",
      "Epoch 109/150\n",
      "405/405 [==============================] - 2s - loss: 0.9121 - mean_squared_error: 0.9121 - val_loss: 0.6964 - val_mean_squared_error: 0.6964\n",
      "Epoch 110/150\n",
      "405/405 [==============================] - 2s - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 0.7184 - val_mean_squared_error: 0.7184\n",
      "Epoch 111/150\n",
      "405/405 [==============================] - 2s - loss: 0.9772 - mean_squared_error: 0.9772 - val_loss: 0.7084 - val_mean_squared_error: 0.7084\n",
      "Epoch 112/150\n",
      "405/405 [==============================] - 2s - loss: 0.9402 - mean_squared_error: 0.9402 - val_loss: 0.7145 - val_mean_squared_error: 0.7145\n",
      "Epoch 113/150\n",
      "405/405 [==============================] - 2s - loss: 0.8919 - mean_squared_error: 0.8919 - val_loss: 0.7218 - val_mean_squared_error: 0.7218\n",
      "Epoch 114/150\n",
      "405/405 [==============================] - 2s - loss: 0.9038 - mean_squared_error: 0.9038 - val_loss: 0.7122 - val_mean_squared_error: 0.7122\n",
      "Epoch 115/150\n",
      "405/405 [==============================] - 2s - loss: 1.0407 - mean_squared_error: 1.0407 - val_loss: 0.7023 - val_mean_squared_error: 0.7023\n",
      "Epoch 116/150\n",
      "405/405 [==============================] - 2s - loss: 0.9512 - mean_squared_error: 0.9512 - val_loss: 0.7041 - val_mean_squared_error: 0.7041\n",
      "Epoch 117/150\n",
      "405/405 [==============================] - 2s - loss: 0.8803 - mean_squared_error: 0.8803 - val_loss: 0.7170 - val_mean_squared_error: 0.7170\n",
      "Epoch 118/150\n",
      "405/405 [==============================] - 2s - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 0.7240 - val_mean_squared_error: 0.7240\n",
      "Epoch 119/150\n",
      "405/405 [==============================] - 2s - loss: 0.9879 - mean_squared_error: 0.9879 - val_loss: 0.6974 - val_mean_squared_error: 0.6974\n",
      "Epoch 120/150\n",
      "405/405 [==============================] - 2s - loss: 1.0480 - mean_squared_error: 1.0480 - val_loss: 0.7116 - val_mean_squared_error: 0.7116\n",
      "Epoch 121/150\n",
      "405/405 [==============================] - 2s - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 0.7262 - val_mean_squared_error: 0.7262\n",
      "Epoch 122/150\n",
      "405/405 [==============================] - 2s - loss: 0.9064 - mean_squared_error: 0.9064 - val_loss: 0.7650 - val_mean_squared_error: 0.7650\n",
      "Epoch 123/150\n",
      "405/405 [==============================] - 2s - loss: 0.8861 - mean_squared_error: 0.8861 - val_loss: 0.7036 - val_mean_squared_error: 0.7036\n",
      "Epoch 124/150\n",
      "405/405 [==============================] - 2s - loss: 0.9357 - mean_squared_error: 0.9357 - val_loss: 0.7072 - val_mean_squared_error: 0.7072\n",
      "Epoch 125/150\n",
      "405/405 [==============================] - 2s - loss: 0.9714 - mean_squared_error: 0.9714 - val_loss: 0.7123 - val_mean_squared_error: 0.7123\n",
      "Epoch 126/150\n",
      "405/405 [==============================] - 2s - loss: 0.9142 - mean_squared_error: 0.9142 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 127/150\n",
      "405/405 [==============================] - 2s - loss: 0.8746 - mean_squared_error: 0.8746 - val_loss: 0.6912 - val_mean_squared_error: 0.6912\n",
      "Epoch 128/150\n",
      "405/405 [==============================] - 2s - loss: 0.9444 - mean_squared_error: 0.9444 - val_loss: 0.6951 - val_mean_squared_error: 0.6951\n",
      "Epoch 129/150\n",
      "405/405 [==============================] - 2s - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 0.7111 - val_mean_squared_error: 0.7111\n",
      "Epoch 130/150\n",
      "405/405 [==============================] - 2s - loss: 0.8716 - mean_squared_error: 0.8716 - val_loss: 0.6974 - val_mean_squared_error: 0.6974\n",
      "Epoch 131/150\n",
      "405/405 [==============================] - 2s - loss: 0.9212 - mean_squared_error: 0.9212 - val_loss: 0.6916 - val_mean_squared_error: 0.6916\n",
      "Epoch 132/150\n",
      "405/405 [==============================] - 2s - loss: 0.8124 - mean_squared_error: 0.8124 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 133/150\n",
      "405/405 [==============================] - 2s - loss: 0.8907 - mean_squared_error: 0.8907 - val_loss: 0.7093 - val_mean_squared_error: 0.7093\n",
      "Epoch 134/150\n",
      "405/405 [==============================] - 2s - loss: 0.9177 - mean_squared_error: 0.9177 - val_loss: 0.6948 - val_mean_squared_error: 0.6948\n",
      "Epoch 135/150\n",
      "405/405 [==============================] - 2s - loss: 0.8725 - mean_squared_error: 0.8725 - val_loss: 0.6964 - val_mean_squared_error: 0.6964\n",
      "Epoch 136/150\n",
      "405/405 [==============================] - 2s - loss: 0.9141 - mean_squared_error: 0.9141 - val_loss: 0.6960 - val_mean_squared_error: 0.6960\n",
      "Epoch 137/150\n",
      "405/405 [==============================] - 2s - loss: 0.9740 - mean_squared_error: 0.9740 - val_loss: 0.7618 - val_mean_squared_error: 0.7618\n",
      "Epoch 138/150\n",
      "405/405 [==============================] - 2s - loss: 0.9176 - mean_squared_error: 0.9176 - val_loss: 0.7217 - val_mean_squared_error: 0.7217\n",
      "Epoch 139/150\n",
      "405/405 [==============================] - 2s - loss: 0.9728 - mean_squared_error: 0.9728 - val_loss: 0.7094 - val_mean_squared_error: 0.7094\n",
      "Epoch 140/150\n",
      "405/405 [==============================] - 2s - loss: 0.9222 - mean_squared_error: 0.9222 - val_loss: 0.7274 - val_mean_squared_error: 0.7274\n",
      "Epoch 141/150\n",
      "405/405 [==============================] - 2s - loss: 0.9075 - mean_squared_error: 0.9075 - val_loss: 0.7130 - val_mean_squared_error: 0.7130\n",
      "Epoch 142/150\n",
      "405/405 [==============================] - 2s - loss: 0.9506 - mean_squared_error: 0.9506 - val_loss: 0.7712 - val_mean_squared_error: 0.7712\n",
      "Epoch 143/150\n",
      "405/405 [==============================] - 2s - loss: 0.9142 - mean_squared_error: 0.9142 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 144/150\n",
      "405/405 [==============================] - 2s - loss: 0.9197 - mean_squared_error: 0.9197 - val_loss: 0.6861 - val_mean_squared_error: 0.6861\n",
      "Epoch 145/150\n",
      "405/405 [==============================] - 2s - loss: 0.9362 - mean_squared_error: 0.9362 - val_loss: 0.6906 - val_mean_squared_error: 0.6906\n",
      "Epoch 146/150\n",
      "405/405 [==============================] - 2s - loss: 0.8697 - mean_squared_error: 0.8697 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 147/150\n",
      "405/405 [==============================] - 2s - loss: 0.8878 - mean_squared_error: 0.8878 - val_loss: 0.7150 - val_mean_squared_error: 0.7150\n",
      "Epoch 148/150\n",
      "405/405 [==============================] - 2s - loss: 0.9118 - mean_squared_error: 0.9118 - val_loss: 0.6857 - val_mean_squared_error: 0.6857\n",
      "Epoch 149/150\n",
      "405/405 [==============================] - 2s - loss: 0.8841 - mean_squared_error: 0.8841 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 150/150\n",
      "405/405 [==============================] - 2s - loss: 0.8859 - mean_squared_error: 0.8859 - val_loss: 0.7015 - val_mean_squared_error: 0.7015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198114e6ef0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size=16\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "#filepath=\"weights_best_arousal.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "\n",
    "model.fit([x_train,aux_inputs_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_test,aux_inputs_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([x_test,aux_inputs_test])\n",
    "preds = preds.reshape(preds.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of predictions is: 1.88531\n",
      "Variance (benchmark) is: 0.701500721508\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(preds)\n",
    "print(\"Mean of predictions is: \"+str(mean_y))\n",
    "\n",
    "mse = np.mean((y_test - preds)**2)\n",
    "print(\"Variance (benchmark) is: \"+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.17263126,  1.04617345,  1.64479566,  1.23388839,  2.86100817,\n",
       "        3.71547222,  2.80520201,  3.64250803,  3.14486122,  2.18505716,\n",
       "        2.1777451 ,  0.96880531,  2.20417738,  0.96121305,  1.0687139 ,\n",
       "        1.14163768,  0.99956346,  0.95929384,  0.97007936,  1.22787547,\n",
       "        1.15225303,  1.22155011,  1.13291335,  1.02378726,  1.53078473,\n",
       "        1.90811276,  1.2129823 ,  2.43585014,  0.99306613,  0.98058033,\n",
       "        1.00851059,  2.90824413,  2.03819227,  1.45416629,  1.81926012,\n",
       "        1.81660342,  1.612082  ,  1.82738423,  1.70068717,  1.47721899,\n",
       "        2.08559966,  1.57050943,  1.21196914,  1.3934505 ,  2.43544984,\n",
       "        2.39549494,  1.94811702,  1.42889047,  1.57413816,  1.11355615,\n",
       "        1.34874034,  1.05631065,  1.59055412,  1.06175244,  1.05425107,\n",
       "        1.40130591,  1.06867611,  1.80071044,  1.94403291,  2.43722987,\n",
       "        0.98168635,  1.04482853,  0.97133076,  2.22818112,  0.97025025,\n",
       "        0.99537861,  0.98309267,  1.02088857,  1.07566464,  0.96239704,\n",
       "        1.13414335,  1.68996811,  1.47400916,  2.04149103,  2.58548975,\n",
       "        2.29390264,  1.94941759,  2.29524589,  1.44755292,  1.88225532,\n",
       "        2.11499381,  1.48564863,  1.91613483,  1.27909851,  1.9594748 ,\n",
       "        2.27773666,  2.80588961,  3.13209176,  3.8866539 ,  2.84492183,\n",
       "        2.04941559,  2.39109015,  2.62730122,  2.06141281,  1.66292334,\n",
       "        2.86689425,  2.69936848,  2.74133444,  3.36651134,  3.38542366,\n",
       "        2.78477883,  2.58474445,  2.90701723,  2.90927362,  2.99743199,\n",
       "        1.2581352 ,  2.28982329,  2.10197926,  1.58470631,  1.39681339,\n",
       "        2.23533201,  2.83233213,  2.37420774,  1.26580405,  1.25324202,\n",
       "        1.68083525,  2.11840129,  2.23895049,  1.2762959 ,  1.95328903,\n",
       "        2.2186861 ,  1.82388496,  1.39676952,  1.28140664,  1.81302762,\n",
       "        2.12019944,  1.03610682,  2.59428334,  1.2159189 ,  1.5750078 ,\n",
       "        4.41750336,  2.14015102,  2.52386427,  1.15876102,  2.62738752,\n",
       "        2.49645472,  2.73942637,  1.75698495,  2.86340523,  2.38431501,\n",
       "        2.34461427,  1.27944398,  1.28402352,  1.3355062 ,  2.29946709,\n",
       "        1.92695355,  1.59799099,  2.02227426,  1.32226968,  1.01989627,\n",
       "        2.23348737,  1.65320396,  1.26276755,  1.46530938,  2.01556301,\n",
       "        1.98947263,  1.74131131,  1.72013307,  2.24933863,  2.04265261,\n",
       "        1.91934657,  2.85740471,  1.8262713 ,  1.91556692,  1.50734389,\n",
       "        1.5118773 ,  2.30911255,  1.35371232,  1.33856034,  2.124403  ,\n",
       "        2.5141573 ,  1.29001749,  1.76865625,  2.17193532,  3.1638689 ,\n",
       "        3.09022331,  1.99671531,  3.23837423,  1.53956342,  1.58166611,\n",
       "        1.49343324,  2.23416972,  1.29270267,  1.74104714,  1.84954906,\n",
       "        1.31717956,  1.79063296,  1.29562736,  1.96738362,  1.61250222,\n",
       "        1.33321929,  1.41624308,  1.6104815 ,  2.08023572,  2.64972496,\n",
       "        2.60989237,  2.03923059,  1.51227534,  1.43734121,  2.32699728,\n",
       "        2.1562686 ,  2.56856418,  2.19847703,  2.03018975,  2.19203568,\n",
       "        1.81163597,  1.92881608,  1.65886688,  1.93712735], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of test set is: 1.86124401914\n",
      "Variance (benchmark) is: 1.28696687347\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "print(\"Mean of test set is: \"+str(mean_y))\n",
    "\n",
    "variance = np.mean((y_test - np.mean(y_test))**2)\n",
    "print(\"Variance (benchmark) is: \"+str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conv LSTM Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 136, 25, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 136, 25, 1)    4           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 136, 25, 32)   192         batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 136, 25, 32)   0           conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 136, 25, 32)   128         dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 136, 25, 32)   5152        batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 136, 25, 32)   0           conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "permute_9 (Permute)              (None, 25, 136, 32)   0           dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 25, 4352)      0           permute_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                    (None, 32)            561280      reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 32)            0           lstm_9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 34)            0           dropout_27[0][0]                 \n",
      "                                                                   aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             35          concatenate_9[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 566,791\n",
      "Trainable params: 566,725\n",
      "Non-trainable params: 66\n",
      "____________________________________________________________________________________________________\n",
      "Train on 405 samples, validate on 209 samples\n",
      "Epoch 1/50\n",
      "405/405 [==============================] - 4s - loss: 1.5671 - mean_squared_error: 1.5671 - val_loss: 0.6941 - val_mean_squared_error: 0.6941\n",
      "Epoch 2/50\n",
      "405/405 [==============================] - 2s - loss: 1.2063 - mean_squared_error: 1.2063 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 3/50\n",
      "405/405 [==============================] - 2s - loss: 1.1294 - mean_squared_error: 1.1294 - val_loss: 0.7068 - val_mean_squared_error: 0.7068\n",
      "Epoch 4/50\n",
      "405/405 [==============================] - 2s - loss: 1.0509 - mean_squared_error: 1.0509 - val_loss: 0.7030 - val_mean_squared_error: 0.7030\n",
      "Epoch 5/50\n",
      "405/405 [==============================] - 2s - loss: 1.1253 - mean_squared_error: 1.1253 - val_loss: 0.6916 - val_mean_squared_error: 0.6916\n",
      "Epoch 6/50\n",
      "405/405 [==============================] - 2s - loss: 1.1409 - mean_squared_error: 1.1409 - val_loss: 0.7168 - val_mean_squared_error: 0.7168\n",
      "Epoch 7/50\n",
      "405/405 [==============================] - 2s - loss: 1.0293 - mean_squared_error: 1.0293 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 8/50\n",
      "405/405 [==============================] - 2s - loss: 0.8688 - mean_squared_error: 0.8688 - val_loss: 0.6855 - val_mean_squared_error: 0.6855\n",
      "Epoch 9/50\n",
      "405/405 [==============================] - 2s - loss: 0.9795 - mean_squared_error: 0.9795 - val_loss: 0.6842 - val_mean_squared_error: 0.6842\n",
      "Epoch 10/50\n",
      "405/405 [==============================] - 2s - loss: 0.8682 - mean_squared_error: 0.8682 - val_loss: 0.6927 - val_mean_squared_error: 0.6927\n",
      "Epoch 11/50\n",
      "405/405 [==============================] - 2s - loss: 0.9127 - mean_squared_error: 0.9127 - val_loss: 0.6808 - val_mean_squared_error: 0.6808\n",
      "Epoch 12/50\n",
      "405/405 [==============================] - 2s - loss: 0.9103 - mean_squared_error: 0.9103 - val_loss: 0.6776 - val_mean_squared_error: 0.6776\n",
      "Epoch 13/50\n",
      "405/405 [==============================] - 2s - loss: 0.9575 - mean_squared_error: 0.9575 - val_loss: 0.6689 - val_mean_squared_error: 0.6689\n",
      "Epoch 14/50\n",
      "405/405 [==============================] - 2s - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 0.6949 - val_mean_squared_error: 0.6949\n",
      "Epoch 15/50\n",
      "405/405 [==============================] - 2s - loss: 0.9057 - mean_squared_error: 0.9057 - val_loss: 0.6680 - val_mean_squared_error: 0.6680\n",
      "Epoch 16/50\n",
      "405/405 [==============================] - 2s - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 0.6652 - val_mean_squared_error: 0.6652\n",
      "Epoch 17/50\n",
      "405/405 [==============================] - 2s - loss: 0.8938 - mean_squared_error: 0.8938 - val_loss: 0.6674 - val_mean_squared_error: 0.6674\n",
      "Epoch 18/50\n",
      "405/405 [==============================] - 2s - loss: 0.8481 - mean_squared_error: 0.8481 - val_loss: 0.6725 - val_mean_squared_error: 0.6725\n",
      "Epoch 19/50\n",
      "405/405 [==============================] - 2s - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.6556 - val_mean_squared_error: 0.6556\n",
      "Epoch 20/50\n",
      "405/405 [==============================] - 2s - loss: 0.8563 - mean_squared_error: 0.8563 - val_loss: 0.6818 - val_mean_squared_error: 0.6818\n",
      "Epoch 21/50\n",
      "405/405 [==============================] - 2s - loss: 0.8906 - mean_squared_error: 0.8906 - val_loss: 0.6551 - val_mean_squared_error: 0.6551\n",
      "Epoch 22/50\n",
      "405/405 [==============================] - 2s - loss: 0.8321 - mean_squared_error: 0.8321 - val_loss: 0.6711 - val_mean_squared_error: 0.6711\n",
      "Epoch 23/50\n",
      "405/405 [==============================] - 2s - loss: 0.8731 - mean_squared_error: 0.8731 - val_loss: 0.6358 - val_mean_squared_error: 0.6358\n",
      "Epoch 24/50\n",
      "405/405 [==============================] - 2s - loss: 0.8727 - mean_squared_error: 0.8727 - val_loss: 0.6533 - val_mean_squared_error: 0.6533\n",
      "Epoch 25/50\n",
      "405/405 [==============================] - 2s - loss: 0.8154 - mean_squared_error: 0.8154 - val_loss: 0.6389 - val_mean_squared_error: 0.6389\n",
      "Epoch 26/50\n",
      "405/405 [==============================] - 2s - loss: 0.8130 - mean_squared_error: 0.8130 - val_loss: 0.6323 - val_mean_squared_error: 0.6323\n",
      "Epoch 27/50\n",
      "405/405 [==============================] - 2s - loss: 0.8757 - mean_squared_error: 0.8757 - val_loss: 0.6647 - val_mean_squared_error: 0.6647\n",
      "Epoch 28/50\n",
      "405/405 [==============================] - 2s - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 0.6455 - val_mean_squared_error: 0.6455\n",
      "Epoch 29/50\n",
      "405/405 [==============================] - 2s - loss: 0.7980 - mean_squared_error: 0.7980 - val_loss: 0.6515 - val_mean_squared_error: 0.6515\n",
      "Epoch 30/50\n",
      "405/405 [==============================] - 2s - loss: 0.8599 - mean_squared_error: 0.8599 - val_loss: 0.6343 - val_mean_squared_error: 0.6343\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 2s - loss: 0.8091 - mean_squared_error: 0.8091 - val_loss: 0.6256 - val_mean_squared_error: 0.6256\n",
      "Epoch 32/50\n",
      "405/405 [==============================] - 2s - loss: 0.8096 - mean_squared_error: 0.8096 - val_loss: 0.6222 - val_mean_squared_error: 0.6222\n",
      "Epoch 33/50\n",
      "405/405 [==============================] - 2s - loss: 0.8315 - mean_squared_error: 0.8315 - val_loss: 0.6322 - val_mean_squared_error: 0.6322\n",
      "Epoch 34/50\n",
      "405/405 [==============================] - 2s - loss: 0.9056 - mean_squared_error: 0.9056 - val_loss: 0.6251 - val_mean_squared_error: 0.6251\n",
      "Epoch 35/50\n",
      "405/405 [==============================] - 2s - loss: 0.7768 - mean_squared_error: 0.7768 - val_loss: 0.6572 - val_mean_squared_error: 0.6572\n",
      "Epoch 36/50\n",
      "405/405 [==============================] - 2s - loss: 0.8196 - mean_squared_error: 0.8196 - val_loss: 0.6478 - val_mean_squared_error: 0.6478\n",
      "Epoch 37/50\n",
      "405/405 [==============================] - 2s - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 0.6465 - val_mean_squared_error: 0.6465\n",
      "Epoch 38/50\n",
      "405/405 [==============================] - 2s - loss: 0.8465 - mean_squared_error: 0.8465 - val_loss: 0.6222 - val_mean_squared_error: 0.6222\n",
      "Epoch 39/50\n",
      "405/405 [==============================] - 2s - loss: 0.8322 - mean_squared_error: 0.8322 - val_loss: 0.6341 - val_mean_squared_error: 0.6341\n",
      "Epoch 40/50\n",
      "405/405 [==============================] - 2s - loss: 0.8683 - mean_squared_error: 0.8683 - val_loss: 0.6681 - val_mean_squared_error: 0.6681\n",
      "Epoch 41/50\n",
      "405/405 [==============================] - 2s - loss: 0.7970 - mean_squared_error: 0.7970 - val_loss: 0.6477 - val_mean_squared_error: 0.6477\n",
      "Epoch 42/50\n",
      "405/405 [==============================] - 2s - loss: 0.7835 - mean_squared_error: 0.7835 - val_loss: 0.6202 - val_mean_squared_error: 0.6202\n",
      "Epoch 43/50\n",
      "405/405 [==============================] - 2s - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.6255 - val_mean_squared_error: 0.6255\n",
      "Epoch 44/50\n",
      "405/405 [==============================] - 2s - loss: 0.8311 - mean_squared_error: 0.8311 - val_loss: 0.6232 - val_mean_squared_error: 0.6232\n",
      "Epoch 45/50\n",
      "405/405 [==============================] - 2s - loss: 0.7973 - mean_squared_error: 0.7973 - val_loss: 0.6519 - val_mean_squared_error: 0.6519\n",
      "Epoch 46/50\n",
      "405/405 [==============================] - 2s - loss: 0.7906 - mean_squared_error: 0.7906 - val_loss: 0.6242 - val_mean_squared_error: 0.6242\n",
      "Epoch 47/50\n",
      "405/405 [==============================] - 2s - loss: 0.7541 - mean_squared_error: 0.7541 - val_loss: 0.6690 - val_mean_squared_error: 0.6690\n",
      "Epoch 48/50\n",
      "405/405 [==============================] - 2s - loss: 0.8166 - mean_squared_error: 0.8166 - val_loss: 0.6551 - val_mean_squared_error: 0.6551\n",
      "Epoch 49/50\n",
      "405/405 [==============================] - 2s - loss: 0.8338 - mean_squared_error: 0.8338 - val_loss: 0.6190 - val_mean_squared_error: 0.6190\n",
      "Epoch 50/50\n",
      "405/405 [==============================] - 2s - loss: 0.7571 - mean_squared_error: 0.7571 - val_loss: 0.6222 - val_mean_squared_error: 0.6222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198c08f4a90>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 32\n",
    "feat_dim = 136\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "#x_train = x_train.reshape((491,25,-1))\n",
    "#x_test = x_test.reshape((123,25,-1))\n",
    "\n",
    "#x_train = np.swapaxes(x_train,1,2)\n",
    "#x_train = np.reshape(x_train, (-1, feat_dim, WINDOW_SIZE, 1))\n",
    "#x_test = np.swapaxes(x_test,1,2)\n",
    "#x_test = np.reshape(x_test, (-1, feat_dim, WINDOW_SIZE, 1))\n",
    "\n",
    "model = create_graph_arousal_dominance()\n",
    "\n",
    "epochs = 50\n",
    "batch_size=16\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "sgd = keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "filepath=\"weights_best_arousal.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#adam1 = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "\n",
    "model.fit([x_train,aux_inputs_train], y1_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_test,aux_inputs_test], y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of predictions is: 2.43652\n",
      "Variance (benchmark) is: 0.6222497387116338\n",
      "\n",
      "Mean of test set is: 2.311004784688995\n",
      "Variance (benchmark) is: 0.9046038323298448\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([x_test,aux_inputs_test])\n",
    "preds = preds.reshape(preds.shape[0])\n",
    "\n",
    "mean_y = np.mean(preds)\n",
    "print(\"Mean of predictions is: \"+str(mean_y))\n",
    "\n",
    "mse = np.mean((y1_test - preds)**2)\n",
    "print(\"Variance (benchmark) is: \"+str(mse))\n",
    "\n",
    "print()\n",
    "\n",
    "mean_y = np.mean(y1_test)\n",
    "print(\"Mean of test set is: \"+str(mean_y))\n",
    "\n",
    "variance = np.mean((y1_test - np.mean(y_test))**2)\n",
    "print(\"Variance (benchmark) is: \"+str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.4474628 ,  2.27879333,  2.20608926,  1.71106207,  2.82110453,\n",
       "        2.98488808,  2.26071262,  2.65786552,  2.75414515,  2.37763596,\n",
       "        2.39434695,  2.04531622,  2.4380939 ,  2.2634933 ,  2.43008089,\n",
       "        2.14567041,  2.44361067,  2.41811013,  2.39364195,  2.28936744,\n",
       "        2.3412509 ,  2.22638297,  2.39908981,  2.40327334,  2.35173202,\n",
       "        2.59816599,  2.44988489,  2.79702067,  2.14157867,  2.10602689,\n",
       "        2.13534474,  2.99174976,  2.3427465 ,  2.20174956,  2.47160602,\n",
       "        2.31461287,  2.30588388,  2.66174126,  2.35055614,  2.26085448,\n",
       "        2.30857873,  2.34453249,  2.20147896,  2.23436904,  2.65773845,\n",
       "        2.59819198,  2.50496197,  2.28457284,  2.32651067,  2.21581936,\n",
       "        2.06962872,  1.95111072,  2.2014792 ,  2.06576657,  2.00407839,\n",
       "        2.23171091,  2.08660746,  2.32423878,  2.26668525,  2.32133341,\n",
       "        1.90671766,  2.13174272,  1.81924582,  2.53938198,  2.03945065,\n",
       "        2.01759434,  2.06604242,  2.04603148,  2.37366796,  2.08463955,\n",
       "        2.47490597,  2.02212882,  2.61577225,  2.69765854,  2.66417837,\n",
       "        2.69692397,  2.73510861,  2.88174748,  2.62408304,  2.56087732,\n",
       "        2.45667791,  2.20920753,  2.26343608,  2.16272879,  2.48775935,\n",
       "        2.67743516,  2.77483892,  2.79720354,  3.62016702,  2.70254326,\n",
       "        2.35422421,  2.46801996,  2.58891678,  3.02036476,  3.1384964 ,\n",
       "        2.9496932 ,  2.75699711,  2.76562047,  3.05742431,  3.56128311,\n",
       "        2.57028437,  2.99423862,  2.65879059,  2.79937696,  2.7831409 ,\n",
       "        2.31373453,  2.16555452,  2.14007545,  2.69018102,  2.58895183,\n",
       "        2.92209697,  3.15407419,  2.8409636 ,  2.53318691,  2.45763803,\n",
       "        2.91695595,  2.85123158,  2.89019084,  2.29007864,  2.98643374,\n",
       "        2.31258488,  2.23891187,  2.30301571,  2.18114543,  2.19066358,\n",
       "        2.23430181,  2.12261844,  2.67663884,  2.34150124,  2.57414913,\n",
       "        4.57297277,  2.40898895,  2.85685754,  2.27219248,  2.17603946,\n",
       "        2.37643623,  2.43610716,  2.30840373,  2.38031983,  2.05890322,\n",
       "        2.23216701,  2.57448387,  2.29004645,  2.76287889,  3.05116487,\n",
       "        2.65518236,  2.5839839 ,  2.37663221,  2.31317163,  2.04636335,\n",
       "        2.27612662,  2.0221827 ,  2.18657041,  2.26594353,  2.59562731,\n",
       "        2.3525598 ,  2.21059394,  2.19849062,  2.54000354,  2.51854038,\n",
       "        2.34933305,  2.60317278,  2.30052924,  2.23808956,  2.20970011,\n",
       "        2.22431207,  2.39115262,  2.24696684,  2.25778055,  2.51160097,\n",
       "        2.59156156,  2.18499851,  2.45378494,  2.53720307,  2.52317619,\n",
       "        2.47288442,  2.3456955 ,  2.54406643,  2.2921617 ,  2.37424541,\n",
       "        2.17928457,  2.26233816,  2.07867885,  2.2487731 ,  2.20005012,\n",
       "        2.10689259,  2.10053825,  1.98047304,  2.34620428,  2.2627399 ,\n",
       "        2.15107775,  2.20890713,  2.19930959,  2.81293941,  3.4174366 ,\n",
       "        2.38993287,  3.35651755,  2.42684126,  2.10895777,  2.54295397,\n",
       "        2.3152802 ,  2.54019737,  2.71242261,  2.23261619,  2.30070734,\n",
       "        2.23434711,  2.22103024,  2.15385318,  2.24232459], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv LSTM Model Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph_emotions():\n",
    "    \n",
    "    aux_input = Input(shape=(2,),dtype='float32', name='aux_input')\n",
    "    \n",
    "    inputs = Input(shape=(feat_dim,WINDOW_SIZE,1), name='main_input')\n",
    "    \n",
    "    x = BatchNormalization()(inputs)\n",
    "    \n",
    "    x = Conv2D(num_feat_map, kernel_size=(1, 5),\n",
    "                 activation='relu',\n",
    "                 padding='same')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(num_feat_map, kernel_size=(1, 5), activation='relu',padding='same')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Permute((2, 1, 3))(x) # for swap-dimension\n",
    "    x = Reshape((-1,num_feat_map*feat_dim))(x)\n",
    "\n",
    "    x = LSTM(num_feat_map, return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = concatenate([x,aux_input])\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    main_output = Dense(4,activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs, aux_input], outputs=main_output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405, 4)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 136, 25, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNor (None, 136, 25, 1)    4           main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)               (None, 136, 25, 32)   192         batch_normalization_44[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)             (None, 136, 25, 32)   0           conv2d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNor (None, 136, 25, 32)   128         dropout_58[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)               (None, 136, 25, 32)   5152        batch_normalization_45[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 136, 25, 32)   0           conv2d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "permute_20 (Permute)             (None, 25, 136, 32)   0           dropout_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)             (None, 25, 4352)      0           permute_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 32)            561280      reshape_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 32)            0           lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 34)            0           dropout_60[0][0]                 \n",
      "                                                                   aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNor (None, 34)            136         concatenate_20[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 4)             140         batch_normalization_46[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 567,032\n",
      "Trainable params: 566,898\n",
      "Non-trainable params: 134\n",
      "____________________________________________________________________________________________________\n",
      "Train on 405 samples, validate on 209 samples\n",
      "Epoch 1/400\n",
      "405/405 [==============================] - 4s - loss: 2.0599 - acc: 0.2543 - val_loss: 1.4688 - val_acc: 0.2727\n",
      "Epoch 2/400\n",
      "405/405 [==============================] - 1s - loss: 1.9767 - acc: 0.2642 - val_loss: 1.5271 - val_acc: 0.1483\n",
      "Epoch 3/400\n",
      "405/405 [==============================] - 1s - loss: 1.8015 - acc: 0.2889 - val_loss: 1.4208 - val_acc: 0.4689\n",
      "Epoch 4/400\n",
      "405/405 [==============================] - 1s - loss: 1.7070 - acc: 0.3136 - val_loss: 1.3737 - val_acc: 0.4593\n",
      "Epoch 5/400\n",
      "405/405 [==============================] - 1s - loss: 1.8035 - acc: 0.2938 - val_loss: 1.3417 - val_acc: 0.4593\n",
      "Epoch 6/400\n",
      "405/405 [==============================] - 1s - loss: 1.7423 - acc: 0.3284 - val_loss: 1.3254 - val_acc: 0.4593\n",
      "Epoch 7/400\n",
      "405/405 [==============================] - 1s - loss: 1.5767 - acc: 0.3654 - val_loss: 1.3264 - val_acc: 0.4593\n",
      "Epoch 8/400\n",
      "405/405 [==============================] - 1s - loss: 1.6320 - acc: 0.3062 - val_loss: 1.3186 - val_acc: 0.4593\n",
      "Epoch 9/400\n",
      "405/405 [==============================] - 1s - loss: 1.6573 - acc: 0.3086 - val_loss: 1.3118 - val_acc: 0.4593\n",
      "Epoch 10/400\n",
      "405/405 [==============================] - 2s - loss: 1.6015 - acc: 0.3432 - val_loss: 1.2975 - val_acc: 0.4593\n",
      "Epoch 11/400\n",
      "405/405 [==============================] - 1s - loss: 1.5965 - acc: 0.3556 - val_loss: 1.2740 - val_acc: 0.4593\n",
      "Epoch 12/400\n",
      "405/405 [==============================] - 1s - loss: 1.6226 - acc: 0.2938 - val_loss: 1.2661 - val_acc: 0.4593\n",
      "Epoch 13/400\n",
      "405/405 [==============================] - 1s - loss: 1.5380 - acc: 0.3679 - val_loss: 1.2532 - val_acc: 0.4593\n",
      "Epoch 14/400\n",
      "405/405 [==============================] - 1s - loss: 1.4524 - acc: 0.3901 - val_loss: 1.2544 - val_acc: 0.4593\n",
      "Epoch 15/400\n",
      "405/405 [==============================] - 1s - loss: 1.4629 - acc: 0.3679 - val_loss: 1.2479 - val_acc: 0.4593\n",
      "Epoch 16/400\n",
      "405/405 [==============================] - 1s - loss: 1.4941 - acc: 0.3580 - val_loss: 1.2447 - val_acc: 0.4593\n",
      "Epoch 17/400\n",
      "405/405 [==============================] - 1s - loss: 1.4511 - acc: 0.3654 - val_loss: 1.2371 - val_acc: 0.4593\n",
      "Epoch 18/400\n",
      "405/405 [==============================] - 1s - loss: 1.3644 - acc: 0.3975 - val_loss: 1.2324 - val_acc: 0.4593\n",
      "Epoch 19/400\n",
      "405/405 [==============================] - 1s - loss: 1.4332 - acc: 0.4148 - val_loss: 1.2312 - val_acc: 0.4641\n",
      "Epoch 20/400\n",
      "405/405 [==============================] - 1s - loss: 1.4591 - acc: 0.3654 - val_loss: 1.2372 - val_acc: 0.4737\n",
      "Epoch 21/400\n",
      "405/405 [==============================] - 2s - loss: 1.4194 - acc: 0.3605 - val_loss: 1.2425 - val_acc: 0.4737\n",
      "Epoch 22/400\n",
      "405/405 [==============================] - 1s - loss: 1.3286 - acc: 0.4321 - val_loss: 1.2356 - val_acc: 0.4880\n",
      "Epoch 23/400\n",
      "405/405 [==============================] - 1s - loss: 1.3455 - acc: 0.4296 - val_loss: 1.2287 - val_acc: 0.4880\n",
      "Epoch 24/400\n",
      "405/405 [==============================] - 1s - loss: 1.3505 - acc: 0.4049 - val_loss: 1.2268 - val_acc: 0.4689\n",
      "Epoch 25/400\n",
      "405/405 [==============================] - 1s - loss: 1.3517 - acc: 0.4148 - val_loss: 1.2206 - val_acc: 0.4833\n",
      "Epoch 26/400\n",
      "405/405 [==============================] - 1s - loss: 1.3370 - acc: 0.4272 - val_loss: 1.2225 - val_acc: 0.4785\n",
      "Epoch 27/400\n",
      "405/405 [==============================] - 1s - loss: 1.3440 - acc: 0.4099 - val_loss: 1.2147 - val_acc: 0.4737\n",
      "Epoch 28/400\n",
      "405/405 [==============================] - 1s - loss: 1.2652 - acc: 0.4543 - val_loss: 1.2184 - val_acc: 0.4593\n",
      "Epoch 29/400\n",
      "405/405 [==============================] - 1s - loss: 1.2814 - acc: 0.4395 - val_loss: 1.2377 - val_acc: 0.4593\n",
      "Epoch 30/400\n",
      "405/405 [==============================] - 1s - loss: 1.2834 - acc: 0.4494 - val_loss: 1.2155 - val_acc: 0.4593\n",
      "Epoch 31/400\n",
      "405/405 [==============================] - 1s - loss: 1.2797 - acc: 0.4272 - val_loss: 1.2167 - val_acc: 0.4593\n",
      "Epoch 32/400\n",
      "405/405 [==============================] - 1s - loss: 1.2769 - acc: 0.4469 - val_loss: 1.2047 - val_acc: 0.4593\n",
      "Epoch 33/400\n",
      "405/405 [==============================] - 1s - loss: 1.2789 - acc: 0.4247 - val_loss: 1.1990 - val_acc: 0.4593\n",
      "Epoch 34/400\n",
      "405/405 [==============================] - 1s - loss: 1.2675 - acc: 0.4395 - val_loss: 1.2193 - val_acc: 0.4593\n",
      "Epoch 35/400\n",
      "405/405 [==============================] - 1s - loss: 1.2509 - acc: 0.4198 - val_loss: 1.2253 - val_acc: 0.4593\n",
      "Epoch 36/400\n",
      "405/405 [==============================] - 1s - loss: 1.2549 - acc: 0.4667 - val_loss: 1.2167 - val_acc: 0.4593\n",
      "Epoch 37/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.2564 - acc: 0.4444 - val_loss: 1.2360 - val_acc: 0.4593\n",
      "Epoch 38/400\n",
      "405/405 [==============================] - 1s - loss: 1.2676 - acc: 0.4593 - val_loss: 1.2311 - val_acc: 0.4593\n",
      "Epoch 39/400\n",
      "405/405 [==============================] - 1s - loss: 1.2481 - acc: 0.4173 - val_loss: 1.2511 - val_acc: 0.4593\n",
      "Epoch 40/400\n",
      "405/405 [==============================] - 2s - loss: 1.2694 - acc: 0.4247 - val_loss: 1.2480 - val_acc: 0.4593\n",
      "Epoch 41/400\n",
      "405/405 [==============================] - 2s - loss: 1.2393 - acc: 0.4543 - val_loss: 1.2095 - val_acc: 0.4641\n",
      "Epoch 42/400\n",
      "405/405 [==============================] - 1s - loss: 1.2494 - acc: 0.4691 - val_loss: 1.2067 - val_acc: 0.4593\n",
      "Epoch 43/400\n",
      "405/405 [==============================] - 2s - loss: 1.2561 - acc: 0.4444 - val_loss: 1.2029 - val_acc: 0.4785\n",
      "Epoch 44/400\n",
      "405/405 [==============================] - 1s - loss: 1.2530 - acc: 0.4593 - val_loss: 1.1970 - val_acc: 0.4833\n",
      "Epoch 45/400\n",
      "405/405 [==============================] - 1s - loss: 1.2348 - acc: 0.4519 - val_loss: 1.1971 - val_acc: 0.4833\n",
      "Epoch 46/400\n",
      "405/405 [==============================] - 1s - loss: 1.2264 - acc: 0.4543 - val_loss: 1.2129 - val_acc: 0.4641\n",
      "Epoch 47/400\n",
      "405/405 [==============================] - 1s - loss: 1.2174 - acc: 0.4370 - val_loss: 1.2277 - val_acc: 0.4641\n",
      "Epoch 48/400\n",
      "405/405 [==============================] - 1s - loss: 1.2359 - acc: 0.4815 - val_loss: 1.2096 - val_acc: 0.4641\n",
      "Epoch 49/400\n",
      "405/405 [==============================] - 2s - loss: 1.2363 - acc: 0.4568 - val_loss: 1.1921 - val_acc: 0.4785\n",
      "Epoch 50/400\n",
      "405/405 [==============================] - 1s - loss: 1.2459 - acc: 0.4593 - val_loss: 1.1977 - val_acc: 0.4833\n",
      "Epoch 51/400\n",
      "405/405 [==============================] - 1s - loss: 1.2301 - acc: 0.4765 - val_loss: 1.1989 - val_acc: 0.4880\n",
      "Epoch 52/400\n",
      "405/405 [==============================] - 1s - loss: 1.2091 - acc: 0.4741 - val_loss: 1.1943 - val_acc: 0.4880\n",
      "Epoch 53/400\n",
      "405/405 [==============================] - 1s - loss: 1.2129 - acc: 0.4790 - val_loss: 1.1906 - val_acc: 0.4785\n",
      "Epoch 54/400\n",
      "405/405 [==============================] - 1s - loss: 1.2264 - acc: 0.4617 - val_loss: 1.1977 - val_acc: 0.4880\n",
      "Epoch 55/400\n",
      "405/405 [==============================] - 1s - loss: 1.2130 - acc: 0.4617 - val_loss: 1.2183 - val_acc: 0.4641\n",
      "Epoch 56/400\n",
      "405/405 [==============================] - 1s - loss: 1.2133 - acc: 0.4741 - val_loss: 1.1989 - val_acc: 0.4833\n",
      "Epoch 57/400\n",
      "405/405 [==============================] - 1s - loss: 1.1996 - acc: 0.4889 - val_loss: 1.2052 - val_acc: 0.4737\n",
      "Epoch 58/400\n",
      "405/405 [==============================] - 1s - loss: 1.2180 - acc: 0.4691 - val_loss: 1.1987 - val_acc: 0.4833\n",
      "Epoch 59/400\n",
      "405/405 [==============================] - 1s - loss: 1.2106 - acc: 0.4617 - val_loss: 1.2033 - val_acc: 0.4833\n",
      "Epoch 60/400\n",
      "405/405 [==============================] - 1s - loss: 1.2204 - acc: 0.4938 - val_loss: 1.1984 - val_acc: 0.4833\n",
      "Epoch 61/400\n",
      "405/405 [==============================] - 1s - loss: 1.2161 - acc: 0.4765 - val_loss: 1.1986 - val_acc: 0.4785\n",
      "Epoch 62/400\n",
      "405/405 [==============================] - 1s - loss: 1.2073 - acc: 0.4667 - val_loss: 1.2094 - val_acc: 0.4689\n",
      "Epoch 63/400\n",
      "405/405 [==============================] - 1s - loss: 1.2171 - acc: 0.4765 - val_loss: 1.2139 - val_acc: 0.4785\n",
      "Epoch 64/400\n",
      "405/405 [==============================] - 1s - loss: 1.1916 - acc: 0.4840 - val_loss: 1.1994 - val_acc: 0.4880\n",
      "Epoch 65/400\n",
      "405/405 [==============================] - 1s - loss: 1.2213 - acc: 0.4765 - val_loss: 1.2018 - val_acc: 0.4737\n",
      "Epoch 66/400\n",
      "405/405 [==============================] - 1s - loss: 1.2040 - acc: 0.4938 - val_loss: 1.1969 - val_acc: 0.4833\n",
      "Epoch 67/400\n",
      "405/405 [==============================] - 1s - loss: 1.2021 - acc: 0.4988 - val_loss: 1.2025 - val_acc: 0.4641\n",
      "Epoch 68/400\n",
      "405/405 [==============================] - 1s - loss: 1.1920 - acc: 0.4914 - val_loss: 1.1993 - val_acc: 0.4785\n",
      "Epoch 69/400\n",
      "405/405 [==============================] - 2s - loss: 1.2077 - acc: 0.4840 - val_loss: 1.2080 - val_acc: 0.4737\n",
      "Epoch 70/400\n",
      "405/405 [==============================] - 1s - loss: 1.1972 - acc: 0.4815 - val_loss: 1.2181 - val_acc: 0.4641\n",
      "Epoch 71/400\n",
      "405/405 [==============================] - 1s - loss: 1.1973 - acc: 0.4914 - val_loss: 1.1841 - val_acc: 0.4880\n",
      "Epoch 72/400\n",
      "405/405 [==============================] - 1s - loss: 1.1832 - acc: 0.5062 - val_loss: 1.1812 - val_acc: 0.4976\n",
      "Epoch 73/400\n",
      "405/405 [==============================] - 1s - loss: 1.1690 - acc: 0.5111 - val_loss: 1.1879 - val_acc: 0.4593\n",
      "Epoch 74/400\n",
      "405/405 [==============================] - 1s - loss: 1.2003 - acc: 0.4716 - val_loss: 1.2076 - val_acc: 0.4593\n",
      "Epoch 75/400\n",
      "405/405 [==============================] - 1s - loss: 1.2254 - acc: 0.4691 - val_loss: 1.1999 - val_acc: 0.4641\n",
      "Epoch 76/400\n",
      "405/405 [==============================] - 2s - loss: 1.1884 - acc: 0.4864 - val_loss: 1.1992 - val_acc: 0.4833\n",
      "Epoch 77/400\n",
      "405/405 [==============================] - 1s - loss: 1.1850 - acc: 0.5086 - val_loss: 1.1864 - val_acc: 0.4833\n",
      "Epoch 78/400\n",
      "405/405 [==============================] - 1s - loss: 1.1706 - acc: 0.4963 - val_loss: 1.1946 - val_acc: 0.4833\n",
      "Epoch 79/400\n",
      "405/405 [==============================] - 1s - loss: 1.2085 - acc: 0.4815 - val_loss: 1.1872 - val_acc: 0.4785\n",
      "Epoch 80/400\n",
      "405/405 [==============================] - 1s - loss: 1.1813 - acc: 0.5037 - val_loss: 1.2380 - val_acc: 0.4641\n",
      "Epoch 81/400\n",
      "405/405 [==============================] - 1s - loss: 1.1923 - acc: 0.4914 - val_loss: 1.1940 - val_acc: 0.4833\n",
      "Epoch 82/400\n",
      "405/405 [==============================] - 1s - loss: 1.1971 - acc: 0.4889 - val_loss: 1.1920 - val_acc: 0.4689\n",
      "Epoch 83/400\n",
      "405/405 [==============================] - 1s - loss: 1.1774 - acc: 0.5358 - val_loss: 1.1985 - val_acc: 0.4833\n",
      "Epoch 84/400\n",
      "405/405 [==============================] - 1s - loss: 1.1965 - acc: 0.4889 - val_loss: 1.1793 - val_acc: 0.4880\n",
      "Epoch 85/400\n",
      "405/405 [==============================] - 1s - loss: 1.1875 - acc: 0.4914 - val_loss: 1.1848 - val_acc: 0.4833\n",
      "Epoch 86/400\n",
      "405/405 [==============================] - 1s - loss: 1.1927 - acc: 0.4889 - val_loss: 1.1870 - val_acc: 0.4833\n",
      "Epoch 87/400\n",
      "405/405 [==============================] - 1s - loss: 1.1871 - acc: 0.5086 - val_loss: 1.1907 - val_acc: 0.4641\n",
      "Epoch 88/400\n",
      "405/405 [==============================] - 1s - loss: 1.1973 - acc: 0.4815 - val_loss: 1.1858 - val_acc: 0.4833\n",
      "Epoch 89/400\n",
      "405/405 [==============================] - 1s - loss: 1.1672 - acc: 0.4914 - val_loss: 1.1894 - val_acc: 0.4545\n",
      "Epoch 90/400\n",
      "405/405 [==============================] - 1s - loss: 1.1916 - acc: 0.4914 - val_loss: 1.1823 - val_acc: 0.4785\n",
      "Epoch 91/400\n",
      "405/405 [==============================] - 1s - loss: 1.1847 - acc: 0.4864 - val_loss: 1.1782 - val_acc: 0.4785\n",
      "Epoch 92/400\n",
      "405/405 [==============================] - 1s - loss: 1.1951 - acc: 0.4864 - val_loss: 1.1944 - val_acc: 0.4641\n",
      "Epoch 93/400\n",
      "405/405 [==============================] - 1s - loss: 1.1902 - acc: 0.4840 - val_loss: 1.1890 - val_acc: 0.4737\n",
      "Epoch 94/400\n",
      "405/405 [==============================] - 1s - loss: 1.1913 - acc: 0.4815 - val_loss: 1.1902 - val_acc: 0.4593\n",
      "Epoch 95/400\n",
      "405/405 [==============================] - 1s - loss: 1.2004 - acc: 0.4815 - val_loss: 1.1857 - val_acc: 0.4880\n",
      "Epoch 96/400\n",
      "405/405 [==============================] - 1s - loss: 1.1834 - acc: 0.4815 - val_loss: 1.1957 - val_acc: 0.4689\n",
      "Epoch 97/400\n",
      "405/405 [==============================] - 1s - loss: 1.1675 - acc: 0.4938 - val_loss: 1.2131 - val_acc: 0.4593\n",
      "Epoch 98/400\n",
      "405/405 [==============================] - 1s - loss: 1.1836 - acc: 0.5086 - val_loss: 1.1769 - val_acc: 0.4976\n",
      "Epoch 99/400\n",
      "405/405 [==============================] - 2s - loss: 1.1850 - acc: 0.4667 - val_loss: 1.1920 - val_acc: 0.5072\n",
      "Epoch 100/400\n",
      "405/405 [==============================] - 2s - loss: 1.1976 - acc: 0.5012 - val_loss: 1.1802 - val_acc: 0.5024\n",
      "Epoch 101/400\n",
      "405/405 [==============================] - 1s - loss: 1.2067 - acc: 0.4593 - val_loss: 1.1846 - val_acc: 0.4785\n",
      "Epoch 102/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.1951 - acc: 0.4963 - val_loss: 1.1820 - val_acc: 0.4928\n",
      "Epoch 103/400\n",
      "405/405 [==============================] - 1s - loss: 1.2045 - acc: 0.4889 - val_loss: 1.1914 - val_acc: 0.4928\n",
      "Epoch 104/400\n",
      "405/405 [==============================] - 1s - loss: 1.1879 - acc: 0.4938 - val_loss: 1.1956 - val_acc: 0.4880\n",
      "Epoch 105/400\n",
      "405/405 [==============================] - 1s - loss: 1.1906 - acc: 0.4988 - val_loss: 1.1911 - val_acc: 0.4928\n",
      "Epoch 106/400\n",
      "405/405 [==============================] - 1s - loss: 1.1871 - acc: 0.4716 - val_loss: 1.1900 - val_acc: 0.4785\n",
      "Epoch 107/400\n",
      "405/405 [==============================] - 1s - loss: 1.1773 - acc: 0.5012 - val_loss: 1.1766 - val_acc: 0.4833\n",
      "Epoch 108/400\n",
      "405/405 [==============================] - 1s - loss: 1.1823 - acc: 0.4765 - val_loss: 1.1993 - val_acc: 0.4928\n",
      "Epoch 109/400\n",
      "405/405 [==============================] - 1s - loss: 1.1731 - acc: 0.5111 - val_loss: 1.1896 - val_acc: 0.4976\n",
      "Epoch 110/400\n",
      "405/405 [==============================] - 1s - loss: 1.1857 - acc: 0.4840 - val_loss: 1.2073 - val_acc: 0.4450\n",
      "Epoch 111/400\n",
      "405/405 [==============================] - 1s - loss: 1.1784 - acc: 0.5111 - val_loss: 1.1946 - val_acc: 0.4498\n",
      "Epoch 112/400\n",
      "405/405 [==============================] - 1s - loss: 1.1800 - acc: 0.5012 - val_loss: 1.1812 - val_acc: 0.5072\n",
      "Epoch 113/400\n",
      "405/405 [==============================] - 1s - loss: 1.1896 - acc: 0.4938 - val_loss: 1.1973 - val_acc: 0.4689\n",
      "Epoch 114/400\n",
      "405/405 [==============================] - 1s - loss: 1.1960 - acc: 0.5160 - val_loss: 1.1861 - val_acc: 0.4880\n",
      "Epoch 115/400\n",
      "405/405 [==============================] - 1s - loss: 1.1794 - acc: 0.4938 - val_loss: 1.1782 - val_acc: 0.4880\n",
      "Epoch 116/400\n",
      "405/405 [==============================] - 1s - loss: 1.1733 - acc: 0.5037 - val_loss: 1.1882 - val_acc: 0.4689\n",
      "Epoch 117/400\n",
      "405/405 [==============================] - 1s - loss: 1.1680 - acc: 0.4889 - val_loss: 1.1896 - val_acc: 0.4833\n",
      "Epoch 118/400\n",
      "405/405 [==============================] - 1s - loss: 1.1879 - acc: 0.4840 - val_loss: 1.1830 - val_acc: 0.4928\n",
      "Epoch 119/400\n",
      "405/405 [==============================] - 1s - loss: 1.1850 - acc: 0.5136 - val_loss: 1.1786 - val_acc: 0.4880\n",
      "Epoch 120/400\n",
      "405/405 [==============================] - 1s - loss: 1.1953 - acc: 0.5037 - val_loss: 1.1825 - val_acc: 0.4880\n",
      "Epoch 121/400\n",
      "405/405 [==============================] - 1s - loss: 1.1671 - acc: 0.4938 - val_loss: 1.1768 - val_acc: 0.4976\n",
      "Epoch 122/400\n",
      "405/405 [==============================] - 1s - loss: 1.2014 - acc: 0.4840 - val_loss: 1.2025 - val_acc: 0.4593\n",
      "Epoch 123/400\n",
      "405/405 [==============================] - 1s - loss: 1.1666 - acc: 0.5160 - val_loss: 1.1873 - val_acc: 0.4641\n",
      "Epoch 124/400\n",
      "405/405 [==============================] - 1s - loss: 1.1485 - acc: 0.5111 - val_loss: 1.1797 - val_acc: 0.5120\n",
      "Epoch 125/400\n",
      "405/405 [==============================] - 1s - loss: 1.1814 - acc: 0.4914 - val_loss: 1.1871 - val_acc: 0.4976\n",
      "Epoch 126/400\n",
      "405/405 [==============================] - 1s - loss: 1.1981 - acc: 0.4938 - val_loss: 1.1731 - val_acc: 0.4833\n",
      "Epoch 127/400\n",
      "405/405 [==============================] - 1s - loss: 1.2033 - acc: 0.4840 - val_loss: 1.1812 - val_acc: 0.4928\n",
      "Epoch 128/400\n",
      "405/405 [==============================] - 1s - loss: 1.1957 - acc: 0.4963 - val_loss: 1.1830 - val_acc: 0.4833\n",
      "Epoch 129/400\n",
      "405/405 [==============================] - 1s - loss: 1.1799 - acc: 0.4889 - val_loss: 1.1880 - val_acc: 0.4928\n",
      "Epoch 130/400\n",
      "405/405 [==============================] - 1s - loss: 1.1689 - acc: 0.4840 - val_loss: 1.1797 - val_acc: 0.4833\n",
      "Epoch 131/400\n",
      "405/405 [==============================] - 1s - loss: 1.1891 - acc: 0.4938 - val_loss: 1.1763 - val_acc: 0.4641\n",
      "Epoch 132/400\n",
      "405/405 [==============================] - 1s - loss: 1.1880 - acc: 0.4840 - val_loss: 1.1760 - val_acc: 0.4880\n",
      "Epoch 133/400\n",
      "405/405 [==============================] - 1s - loss: 1.1909 - acc: 0.4765 - val_loss: 1.1824 - val_acc: 0.4928\n",
      "Epoch 134/400\n",
      "405/405 [==============================] - 1s - loss: 1.1796 - acc: 0.4889 - val_loss: 1.1792 - val_acc: 0.4976\n",
      "Epoch 135/400\n",
      "405/405 [==============================] - 1s - loss: 1.1554 - acc: 0.5086 - val_loss: 1.2027 - val_acc: 0.4737\n",
      "Epoch 136/400\n",
      "405/405 [==============================] - 1s - loss: 1.1954 - acc: 0.4840 - val_loss: 1.1696 - val_acc: 0.5072\n",
      "Epoch 137/400\n",
      "405/405 [==============================] - 1s - loss: 1.1920 - acc: 0.5062 - val_loss: 1.1798 - val_acc: 0.4785\n",
      "Epoch 138/400\n",
      "405/405 [==============================] - 1s - loss: 1.1691 - acc: 0.5037 - val_loss: 1.1764 - val_acc: 0.4737\n",
      "Epoch 139/400\n",
      "405/405 [==============================] - 1s - loss: 1.1840 - acc: 0.4790 - val_loss: 1.1778 - val_acc: 0.4689\n",
      "Epoch 140/400\n",
      "405/405 [==============================] - 1s - loss: 1.1642 - acc: 0.4889 - val_loss: 1.1729 - val_acc: 0.4833\n",
      "Epoch 141/400\n",
      "405/405 [==============================] - 1s - loss: 1.1684 - acc: 0.5012 - val_loss: 1.1893 - val_acc: 0.4833\n",
      "Epoch 142/400\n",
      "405/405 [==============================] - 1s - loss: 1.1649 - acc: 0.4963 - val_loss: 1.1764 - val_acc: 0.4976\n",
      "Epoch 143/400\n",
      "405/405 [==============================] - 1s - loss: 1.1621 - acc: 0.5309 - val_loss: 1.1929 - val_acc: 0.4833\n",
      "Epoch 144/400\n",
      "405/405 [==============================] - 1s - loss: 1.1587 - acc: 0.5037 - val_loss: 1.1768 - val_acc: 0.4880\n",
      "Epoch 145/400\n",
      "405/405 [==============================] - 1s - loss: 1.1636 - acc: 0.5037 - val_loss: 1.1781 - val_acc: 0.5215\n",
      "Epoch 146/400\n",
      "405/405 [==============================] - 2s - loss: 1.1658 - acc: 0.4963 - val_loss: 1.1807 - val_acc: 0.4928\n",
      "Epoch 147/400\n",
      "405/405 [==============================] - 1s - loss: 1.2017 - acc: 0.4815 - val_loss: 1.1726 - val_acc: 0.4976\n",
      "Epoch 148/400\n",
      "405/405 [==============================] - 1s - loss: 1.1828 - acc: 0.4938 - val_loss: 1.1696 - val_acc: 0.5072\n",
      "Epoch 149/400\n",
      "405/405 [==============================] - 1s - loss: 1.1548 - acc: 0.5160 - val_loss: 1.1691 - val_acc: 0.4928\n",
      "Epoch 150/400\n",
      "405/405 [==============================] - 1s - loss: 1.1700 - acc: 0.5111 - val_loss: 1.1902 - val_acc: 0.4641\n",
      "Epoch 151/400\n",
      "405/405 [==============================] - 1s - loss: 1.1708 - acc: 0.5185 - val_loss: 1.1624 - val_acc: 0.5072\n",
      "Epoch 152/400\n",
      "405/405 [==============================] - 1s - loss: 1.1900 - acc: 0.4914 - val_loss: 1.1648 - val_acc: 0.5024\n",
      "Epoch 153/400\n",
      "405/405 [==============================] - 1s - loss: 1.1628 - acc: 0.4840 - val_loss: 1.1689 - val_acc: 0.4833\n",
      "Epoch 154/400\n",
      "405/405 [==============================] - 1s - loss: 1.1638 - acc: 0.5037 - val_loss: 1.1698 - val_acc: 0.4880\n",
      "Epoch 155/400\n",
      "405/405 [==============================] - 1s - loss: 1.1519 - acc: 0.5037 - val_loss: 1.1980 - val_acc: 0.4737\n",
      "Epoch 156/400\n",
      "405/405 [==============================] - 1s - loss: 1.1642 - acc: 0.4914 - val_loss: 1.1764 - val_acc: 0.4928\n",
      "Epoch 157/400\n",
      "405/405 [==============================] - 1s - loss: 1.1647 - acc: 0.5160 - val_loss: 1.1715 - val_acc: 0.4976\n",
      "Epoch 158/400\n",
      "405/405 [==============================] - 1s - loss: 1.1807 - acc: 0.4963 - val_loss: 1.1667 - val_acc: 0.4928\n",
      "Epoch 159/400\n",
      "405/405 [==============================] - 1s - loss: 1.1789 - acc: 0.4914 - val_loss: 1.1791 - val_acc: 0.4833\n",
      "Epoch 160/400\n",
      "405/405 [==============================] - 1s - loss: 1.1557 - acc: 0.5136 - val_loss: 1.1693 - val_acc: 0.5024\n",
      "Epoch 161/400\n",
      "405/405 [==============================] - 1s - loss: 1.1701 - acc: 0.5185 - val_loss: 1.1882 - val_acc: 0.4833\n",
      "Epoch 162/400\n",
      "405/405 [==============================] - 1s - loss: 1.1660 - acc: 0.4790 - val_loss: 1.1784 - val_acc: 0.4880\n",
      "Epoch 163/400\n",
      "405/405 [==============================] - 1s - loss: 1.1695 - acc: 0.5185 - val_loss: 1.1705 - val_acc: 0.5024\n",
      "Epoch 164/400\n",
      "405/405 [==============================] - 1s - loss: 1.1665 - acc: 0.5062 - val_loss: 1.1793 - val_acc: 0.4928\n",
      "Epoch 165/400\n",
      "405/405 [==============================] - 1s - loss: 1.1516 - acc: 0.5185 - val_loss: 1.1699 - val_acc: 0.4928\n",
      "Epoch 166/400\n",
      "405/405 [==============================] - 1s - loss: 1.1837 - acc: 0.5037 - val_loss: 1.2045 - val_acc: 0.4689\n",
      "Epoch 167/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.1628 - acc: 0.5185 - val_loss: 1.1773 - val_acc: 0.4928\n",
      "Epoch 168/400\n",
      "405/405 [==============================] - 1s - loss: 1.1659 - acc: 0.5037 - val_loss: 1.1777 - val_acc: 0.4880\n",
      "Epoch 169/400\n",
      "405/405 [==============================] - 1s - loss: 1.1644 - acc: 0.4938 - val_loss: 1.1679 - val_acc: 0.4976\n",
      "Epoch 170/400\n",
      "405/405 [==============================] - 1s - loss: 1.1833 - acc: 0.5062 - val_loss: 1.1706 - val_acc: 0.5072\n",
      "Epoch 171/400\n",
      "405/405 [==============================] - 1s - loss: 1.1532 - acc: 0.5284 - val_loss: 1.1605 - val_acc: 0.4976\n",
      "Epoch 172/400\n",
      "405/405 [==============================] - 1s - loss: 1.1631 - acc: 0.5185 - val_loss: 1.1671 - val_acc: 0.5072\n",
      "Epoch 173/400\n",
      "405/405 [==============================] - 1s - loss: 1.1666 - acc: 0.5210 - val_loss: 1.1679 - val_acc: 0.4928\n",
      "Epoch 174/400\n",
      "405/405 [==============================] - 1s - loss: 1.1529 - acc: 0.5136 - val_loss: 1.1752 - val_acc: 0.5024\n",
      "Epoch 175/400\n",
      "405/405 [==============================] - 1s - loss: 1.1836 - acc: 0.4790 - val_loss: 1.1808 - val_acc: 0.4928\n",
      "Epoch 176/400\n",
      "405/405 [==============================] - 1s - loss: 1.1728 - acc: 0.4988 - val_loss: 1.1898 - val_acc: 0.4545\n",
      "Epoch 177/400\n",
      "405/405 [==============================] - 1s - loss: 1.1822 - acc: 0.4938 - val_loss: 1.1861 - val_acc: 0.4641\n",
      "Epoch 178/400\n",
      "405/405 [==============================] - 1s - loss: 1.1794 - acc: 0.5086 - val_loss: 1.1825 - val_acc: 0.4976\n",
      "Epoch 179/400\n",
      "405/405 [==============================] - 1s - loss: 1.1508 - acc: 0.5160 - val_loss: 1.1850 - val_acc: 0.4785\n",
      "Epoch 180/400\n",
      "405/405 [==============================] - 1s - loss: 1.1578 - acc: 0.5160 - val_loss: 1.2237 - val_acc: 0.4737\n",
      "Epoch 181/400\n",
      "405/405 [==============================] - 1s - loss: 1.1614 - acc: 0.5284 - val_loss: 1.1789 - val_acc: 0.4880\n",
      "Epoch 182/400\n",
      "405/405 [==============================] - 1s - loss: 1.1709 - acc: 0.4815 - val_loss: 1.1586 - val_acc: 0.5359\n",
      "Epoch 183/400\n",
      "405/405 [==============================] - 1s - loss: 1.1417 - acc: 0.5086 - val_loss: 1.1798 - val_acc: 0.4880\n",
      "Epoch 184/400\n",
      "405/405 [==============================] - 1s - loss: 1.1649 - acc: 0.5235 - val_loss: 1.1726 - val_acc: 0.5024\n",
      "Epoch 185/400\n",
      "405/405 [==============================] - 1s - loss: 1.1545 - acc: 0.5210 - val_loss: 1.1652 - val_acc: 0.5024\n",
      "Epoch 186/400\n",
      "405/405 [==============================] - 1s - loss: 1.1587 - acc: 0.4938 - val_loss: 1.1877 - val_acc: 0.4928\n",
      "Epoch 187/400\n",
      "405/405 [==============================] - 1s - loss: 1.1519 - acc: 0.5284 - val_loss: 1.1766 - val_acc: 0.4689\n",
      "Epoch 188/400\n",
      "405/405 [==============================] - 1s - loss: 1.1530 - acc: 0.5160 - val_loss: 1.1621 - val_acc: 0.5120\n",
      "Epoch 189/400\n",
      "405/405 [==============================] - 1s - loss: 1.1742 - acc: 0.5037 - val_loss: 1.1726 - val_acc: 0.4928\n",
      "Epoch 190/400\n",
      "405/405 [==============================] - 1s - loss: 1.1515 - acc: 0.5136 - val_loss: 1.1692 - val_acc: 0.4880\n",
      "Epoch 191/400\n",
      "405/405 [==============================] - 1s - loss: 1.1778 - acc: 0.5086 - val_loss: 1.1977 - val_acc: 0.4785\n",
      "Epoch 192/400\n",
      "405/405 [==============================] - 1s - loss: 1.1671 - acc: 0.5111 - val_loss: 1.1649 - val_acc: 0.4976\n",
      "Epoch 193/400\n",
      "405/405 [==============================] - 1s - loss: 1.1574 - acc: 0.4988 - val_loss: 1.1565 - val_acc: 0.4928\n",
      "Epoch 194/400\n",
      "405/405 [==============================] - 1s - loss: 1.1647 - acc: 0.5259 - val_loss: 1.1617 - val_acc: 0.5263\n",
      "Epoch 195/400\n",
      "405/405 [==============================] - 1s - loss: 1.1432 - acc: 0.5235 - val_loss: 1.1911 - val_acc: 0.4928\n",
      "Epoch 196/400\n",
      "405/405 [==============================] - 1s - loss: 1.1631 - acc: 0.5086 - val_loss: 1.1611 - val_acc: 0.4976\n",
      "Epoch 197/400\n",
      "405/405 [==============================] - 1s - loss: 1.1395 - acc: 0.5284 - val_loss: 1.1804 - val_acc: 0.4928\n",
      "Epoch 198/400\n",
      "405/405 [==============================] - 1s - loss: 1.1573 - acc: 0.5235 - val_loss: 1.1932 - val_acc: 0.4976\n",
      "Epoch 199/400\n",
      "405/405 [==============================] - 1s - loss: 1.1468 - acc: 0.5210 - val_loss: 1.2154 - val_acc: 0.4689\n",
      "Epoch 200/400\n",
      "405/405 [==============================] - 1s - loss: 1.1486 - acc: 0.5160 - val_loss: 1.2047 - val_acc: 0.4880\n",
      "Epoch 201/400\n",
      "405/405 [==============================] - 1s - loss: 1.1488 - acc: 0.5333 - val_loss: 1.1943 - val_acc: 0.4785\n",
      "Epoch 202/400\n",
      "405/405 [==============================] - 1s - loss: 1.1419 - acc: 0.5284 - val_loss: 1.1746 - val_acc: 0.5072\n",
      "Epoch 203/400\n",
      "405/405 [==============================] - 1s - loss: 1.1397 - acc: 0.5210 - val_loss: 1.1631 - val_acc: 0.5120\n",
      "Epoch 204/400\n",
      "405/405 [==============================] - 1s - loss: 1.1638 - acc: 0.5086 - val_loss: 1.1756 - val_acc: 0.4976\n",
      "Epoch 205/400\n",
      "405/405 [==============================] - 1s - loss: 1.1604 - acc: 0.5086 - val_loss: 1.1669 - val_acc: 0.5167\n",
      "Epoch 206/400\n",
      "405/405 [==============================] - 1s - loss: 1.1409 - acc: 0.5136 - val_loss: 1.1929 - val_acc: 0.4833\n",
      "Epoch 207/400\n",
      "405/405 [==============================] - 1s - loss: 1.1505 - acc: 0.5136 - val_loss: 1.1510 - val_acc: 0.5024\n",
      "Epoch 208/400\n",
      "405/405 [==============================] - 1s - loss: 1.1570 - acc: 0.5012 - val_loss: 1.1698 - val_acc: 0.4880\n",
      "Epoch 209/400\n",
      "405/405 [==============================] - 1s - loss: 1.1432 - acc: 0.5235 - val_loss: 1.1771 - val_acc: 0.4976\n",
      "Epoch 210/400\n",
      "405/405 [==============================] - 1s - loss: 1.1749 - acc: 0.5333 - val_loss: 1.1880 - val_acc: 0.5024\n",
      "Epoch 211/400\n",
      "405/405 [==============================] - 1s - loss: 1.1612 - acc: 0.5037 - val_loss: 1.2081 - val_acc: 0.4928\n",
      "Epoch 212/400\n",
      "405/405 [==============================] - 1s - loss: 1.1428 - acc: 0.5160 - val_loss: 1.2204 - val_acc: 0.4880\n",
      "Epoch 213/400\n",
      "405/405 [==============================] - 1s - loss: 1.1461 - acc: 0.5160 - val_loss: 1.2048 - val_acc: 0.5024\n",
      "Epoch 214/400\n",
      "405/405 [==============================] - 1s - loss: 1.1553 - acc: 0.5185 - val_loss: 1.1829 - val_acc: 0.4880\n",
      "Epoch 215/400\n",
      "405/405 [==============================] - 1s - loss: 1.1545 - acc: 0.5284 - val_loss: 1.2058 - val_acc: 0.4689\n",
      "Epoch 216/400\n",
      "405/405 [==============================] - 1s - loss: 1.1730 - acc: 0.4988 - val_loss: 1.1673 - val_acc: 0.5072\n",
      "Epoch 217/400\n",
      "405/405 [==============================] - 1s - loss: 1.1361 - acc: 0.4963 - val_loss: 1.1558 - val_acc: 0.5359\n",
      "Epoch 218/400\n",
      "405/405 [==============================] - 1s - loss: 1.1345 - acc: 0.5506 - val_loss: 1.2279 - val_acc: 0.4976\n",
      "Epoch 219/400\n",
      "405/405 [==============================] - 1s - loss: 1.1214 - acc: 0.5358 - val_loss: 1.2410 - val_acc: 0.4785\n",
      "Epoch 220/400\n",
      "405/405 [==============================] - 1s - loss: 1.1477 - acc: 0.5160 - val_loss: 1.1683 - val_acc: 0.5215\n",
      "Epoch 221/400\n",
      "405/405 [==============================] - 1s - loss: 1.1623 - acc: 0.5136 - val_loss: 1.1619 - val_acc: 0.5072\n",
      "Epoch 222/400\n",
      "405/405 [==============================] - 1s - loss: 1.1343 - acc: 0.5309 - val_loss: 1.1563 - val_acc: 0.5263\n",
      "Epoch 223/400\n",
      "405/405 [==============================] - 1s - loss: 1.1400 - acc: 0.4938 - val_loss: 1.1763 - val_acc: 0.4928\n",
      "Epoch 224/400\n",
      "405/405 [==============================] - 1s - loss: 1.1452 - acc: 0.5235 - val_loss: 1.1661 - val_acc: 0.5072\n",
      "Epoch 225/400\n",
      "405/405 [==============================] - 1s - loss: 1.1457 - acc: 0.4963 - val_loss: 1.1587 - val_acc: 0.5120\n",
      "Epoch 226/400\n",
      "405/405 [==============================] - 1s - loss: 1.1378 - acc: 0.5235 - val_loss: 1.1876 - val_acc: 0.4928\n",
      "Epoch 227/400\n",
      "405/405 [==============================] - 1s - loss: 1.1480 - acc: 0.5309 - val_loss: 1.1976 - val_acc: 0.4976\n",
      "Epoch 228/400\n",
      "405/405 [==============================] - 1s - loss: 1.1611 - acc: 0.5037 - val_loss: 1.1723 - val_acc: 0.5407\n",
      "Epoch 229/400\n",
      "405/405 [==============================] - 1s - loss: 1.1628 - acc: 0.5309 - val_loss: 1.1965 - val_acc: 0.4833\n",
      "Epoch 230/400\n",
      "405/405 [==============================] - 1s - loss: 1.1389 - acc: 0.5185 - val_loss: 1.1631 - val_acc: 0.4928\n",
      "Epoch 231/400\n",
      "405/405 [==============================] - 1s - loss: 1.1551 - acc: 0.5160 - val_loss: 1.1945 - val_acc: 0.4641\n",
      "Epoch 232/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.1534 - acc: 0.5160 - val_loss: 1.1949 - val_acc: 0.4880\n",
      "Epoch 233/400\n",
      "405/405 [==============================] - 1s - loss: 1.1316 - acc: 0.5407 - val_loss: 1.2118 - val_acc: 0.4833\n",
      "Epoch 234/400\n",
      "405/405 [==============================] - 1s - loss: 1.1427 - acc: 0.5185 - val_loss: 1.2098 - val_acc: 0.5024\n",
      "Epoch 235/400\n",
      "405/405 [==============================] - 1s - loss: 1.1516 - acc: 0.5210 - val_loss: 1.2922 - val_acc: 0.4737\n",
      "Epoch 236/400\n",
      "405/405 [==============================] - 1s - loss: 1.1360 - acc: 0.5062 - val_loss: 1.2355 - val_acc: 0.4689\n",
      "Epoch 237/400\n",
      "405/405 [==============================] - 1s - loss: 1.1338 - acc: 0.5383 - val_loss: 1.1939 - val_acc: 0.5263\n",
      "Epoch 238/400\n",
      "405/405 [==============================] - 1s - loss: 1.1915 - acc: 0.4988 - val_loss: 1.1850 - val_acc: 0.5072\n",
      "Epoch 239/400\n",
      "405/405 [==============================] - 1s - loss: 1.1462 - acc: 0.5160 - val_loss: 1.2065 - val_acc: 0.4689\n",
      "Epoch 240/400\n",
      "405/405 [==============================] - 1s - loss: 1.1542 - acc: 0.5432 - val_loss: 1.1707 - val_acc: 0.5120\n",
      "Epoch 241/400\n",
      "405/405 [==============================] - 1s - loss: 1.1819 - acc: 0.5136 - val_loss: 1.1855 - val_acc: 0.4880\n",
      "Epoch 242/400\n",
      "405/405 [==============================] - 1s - loss: 1.1658 - acc: 0.5062 - val_loss: 1.1661 - val_acc: 0.5167\n",
      "Epoch 243/400\n",
      "405/405 [==============================] - 1s - loss: 1.1463 - acc: 0.5160 - val_loss: 1.1877 - val_acc: 0.5072\n",
      "Epoch 244/400\n",
      "405/405 [==============================] - 1s - loss: 1.1179 - acc: 0.5457 - val_loss: 1.2407 - val_acc: 0.4976\n",
      "Epoch 245/400\n",
      "405/405 [==============================] - 1s - loss: 1.1312 - acc: 0.5358 - val_loss: 1.1712 - val_acc: 0.5120\n",
      "Epoch 246/400\n",
      "405/405 [==============================] - 1s - loss: 1.1195 - acc: 0.5284 - val_loss: 1.1840 - val_acc: 0.5072\n",
      "Epoch 247/400\n",
      "405/405 [==============================] - 1s - loss: 1.1535 - acc: 0.5136 - val_loss: 1.2114 - val_acc: 0.5120\n",
      "Epoch 248/400\n",
      "405/405 [==============================] - 1s - loss: 1.1484 - acc: 0.5012 - val_loss: 1.1827 - val_acc: 0.5024\n",
      "Epoch 249/400\n",
      "405/405 [==============================] - 1s - loss: 1.1316 - acc: 0.5407 - val_loss: 1.2062 - val_acc: 0.5024\n",
      "Epoch 250/400\n",
      "405/405 [==============================] - 1s - loss: 1.1475 - acc: 0.5210 - val_loss: 1.1796 - val_acc: 0.5072\n",
      "Epoch 251/400\n",
      "405/405 [==============================] - 1s - loss: 1.1538 - acc: 0.5210 - val_loss: 1.1984 - val_acc: 0.5024\n",
      "Epoch 252/400\n",
      "405/405 [==============================] - 1s - loss: 1.1260 - acc: 0.5358 - val_loss: 1.2325 - val_acc: 0.4880\n",
      "Epoch 253/400\n",
      "405/405 [==============================] - 1s - loss: 1.1706 - acc: 0.5358 - val_loss: 1.1574 - val_acc: 0.5120\n",
      "Epoch 254/400\n",
      "405/405 [==============================] - 1s - loss: 1.1528 - acc: 0.5086 - val_loss: 1.1795 - val_acc: 0.5215\n",
      "Epoch 255/400\n",
      "405/405 [==============================] - 1s - loss: 1.1483 - acc: 0.5136 - val_loss: 1.2546 - val_acc: 0.4641\n",
      "Epoch 256/400\n",
      "405/405 [==============================] - 1s - loss: 1.1468 - acc: 0.5210 - val_loss: 1.1578 - val_acc: 0.5263\n",
      "Epoch 257/400\n",
      "405/405 [==============================] - 1s - loss: 1.1464 - acc: 0.5012 - val_loss: 1.2473 - val_acc: 0.4833\n",
      "Epoch 258/400\n",
      "405/405 [==============================] - 1s - loss: 1.1451 - acc: 0.5358 - val_loss: 1.2326 - val_acc: 0.4833\n",
      "Epoch 259/400\n",
      "405/405 [==============================] - 1s - loss: 1.1393 - acc: 0.5481 - val_loss: 1.1959 - val_acc: 0.4928\n",
      "Epoch 260/400\n",
      "405/405 [==============================] - 1s - loss: 1.1508 - acc: 0.5481 - val_loss: 1.1837 - val_acc: 0.5024\n",
      "Epoch 261/400\n",
      "405/405 [==============================] - 1s - loss: 1.1389 - acc: 0.5284 - val_loss: 1.1874 - val_acc: 0.5072\n",
      "Epoch 262/400\n",
      "405/405 [==============================] - 1s - loss: 1.1533 - acc: 0.5012 - val_loss: 1.1861 - val_acc: 0.5024\n",
      "Epoch 263/400\n",
      "405/405 [==============================] - 1s - loss: 1.1363 - acc: 0.5333 - val_loss: 1.2211 - val_acc: 0.4785\n",
      "Epoch 264/400\n",
      "405/405 [==============================] - 1s - loss: 1.1676 - acc: 0.5210 - val_loss: 1.1638 - val_acc: 0.5024\n",
      "Epoch 265/400\n",
      "405/405 [==============================] - 1s - loss: 1.1451 - acc: 0.5210 - val_loss: 1.2389 - val_acc: 0.4737\n",
      "Epoch 266/400\n",
      "405/405 [==============================] - 1s - loss: 1.1625 - acc: 0.5432 - val_loss: 1.1812 - val_acc: 0.5167\n",
      "Epoch 267/400\n",
      "405/405 [==============================] - 1s - loss: 1.1231 - acc: 0.5383 - val_loss: 1.1634 - val_acc: 0.5072\n",
      "Epoch 268/400\n",
      "405/405 [==============================] - 2s - loss: 1.1267 - acc: 0.5383 - val_loss: 1.1727 - val_acc: 0.5359\n",
      "Epoch 269/400\n",
      "405/405 [==============================] - 1s - loss: 1.1450 - acc: 0.5037 - val_loss: 1.1629 - val_acc: 0.4976\n",
      "Epoch 270/400\n",
      "405/405 [==============================] - 1s - loss: 1.1312 - acc: 0.5284 - val_loss: 1.1988 - val_acc: 0.5072\n",
      "Epoch 271/400\n",
      "405/405 [==============================] - 1s - loss: 1.1233 - acc: 0.5111 - val_loss: 1.2242 - val_acc: 0.5167\n",
      "Epoch 272/400\n",
      "405/405 [==============================] - 1s - loss: 1.1210 - acc: 0.5531 - val_loss: 1.2162 - val_acc: 0.4880\n",
      "Epoch 273/400\n",
      "405/405 [==============================] - 1s - loss: 1.1334 - acc: 0.5086 - val_loss: 1.2163 - val_acc: 0.5072\n",
      "Epoch 274/400\n",
      "405/405 [==============================] - 1s - loss: 1.1434 - acc: 0.5259 - val_loss: 1.3646 - val_acc: 0.4689\n",
      "Epoch 275/400\n",
      "405/405 [==============================] - 1s - loss: 1.1388 - acc: 0.5432 - val_loss: 1.2380 - val_acc: 0.4833\n",
      "Epoch 276/400\n",
      "405/405 [==============================] - 1s - loss: 1.1297 - acc: 0.5432 - val_loss: 1.2108 - val_acc: 0.4976\n",
      "Epoch 277/400\n",
      "405/405 [==============================] - 1s - loss: 1.1227 - acc: 0.5333 - val_loss: 1.2378 - val_acc: 0.4880\n",
      "Epoch 278/400\n",
      "405/405 [==============================] - 1s - loss: 1.1320 - acc: 0.5383 - val_loss: 1.1523 - val_acc: 0.5215\n",
      "Epoch 279/400\n",
      "405/405 [==============================] - 1s - loss: 1.1206 - acc: 0.5309 - val_loss: 1.2039 - val_acc: 0.5072\n",
      "Epoch 280/400\n",
      "405/405 [==============================] - 1s - loss: 1.1276 - acc: 0.5432 - val_loss: 1.1422 - val_acc: 0.5263\n",
      "Epoch 281/400\n",
      "405/405 [==============================] - 1s - loss: 1.1269 - acc: 0.5383 - val_loss: 1.1726 - val_acc: 0.5215\n",
      "Epoch 282/400\n",
      "405/405 [==============================] - 1s - loss: 1.1537 - acc: 0.5333 - val_loss: 1.2485 - val_acc: 0.4833\n",
      "Epoch 283/400\n",
      "405/405 [==============================] - 1s - loss: 1.1470 - acc: 0.5086 - val_loss: 1.2379 - val_acc: 0.4689\n",
      "Epoch 284/400\n",
      "405/405 [==============================] - 1s - loss: 1.0965 - acc: 0.5753 - val_loss: 1.1453 - val_acc: 0.5167\n",
      "Epoch 285/400\n",
      "405/405 [==============================] - 1s - loss: 1.1464 - acc: 0.5481 - val_loss: 1.1858 - val_acc: 0.5024\n",
      "Epoch 286/400\n",
      "405/405 [==============================] - 1s - loss: 1.1413 - acc: 0.5185 - val_loss: 1.1887 - val_acc: 0.5024\n",
      "Epoch 287/400\n",
      "405/405 [==============================] - 1s - loss: 1.1258 - acc: 0.5309 - val_loss: 1.2049 - val_acc: 0.4928\n",
      "Epoch 288/400\n",
      "405/405 [==============================] - 1s - loss: 1.1436 - acc: 0.5383 - val_loss: 1.2554 - val_acc: 0.4593\n",
      "Epoch 289/400\n",
      "405/405 [==============================] - 1s - loss: 1.1405 - acc: 0.5210 - val_loss: 1.1832 - val_acc: 0.4928\n",
      "Epoch 290/400\n",
      "405/405 [==============================] - 1s - loss: 1.1421 - acc: 0.5235 - val_loss: 1.1386 - val_acc: 0.5263\n",
      "Epoch 291/400\n",
      "405/405 [==============================] - 1s - loss: 1.1182 - acc: 0.5556 - val_loss: 1.1494 - val_acc: 0.5215\n",
      "Epoch 292/400\n",
      "405/405 [==============================] - 1s - loss: 1.1235 - acc: 0.5531 - val_loss: 1.1448 - val_acc: 0.5215\n",
      "Epoch 293/400\n",
      "405/405 [==============================] - 1s - loss: 1.1288 - acc: 0.5210 - val_loss: 1.1481 - val_acc: 0.5120\n",
      "Epoch 294/400\n",
      "405/405 [==============================] - 1s - loss: 1.1046 - acc: 0.5531 - val_loss: 1.1431 - val_acc: 0.5407\n",
      "Epoch 295/400\n",
      "405/405 [==============================] - 1s - loss: 1.1297 - acc: 0.5210 - val_loss: 1.1866 - val_acc: 0.5024\n",
      "Epoch 296/400\n",
      "405/405 [==============================] - 1s - loss: 1.1481 - acc: 0.5086 - val_loss: 1.1755 - val_acc: 0.5359\n",
      "Epoch 297/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.1226 - acc: 0.5383 - val_loss: 1.2206 - val_acc: 0.5072\n",
      "Epoch 298/400\n",
      "405/405 [==============================] - 1s - loss: 1.1296 - acc: 0.5432 - val_loss: 1.2618 - val_acc: 0.4928\n",
      "Epoch 299/400\n",
      "405/405 [==============================] - 1s - loss: 1.1044 - acc: 0.5481 - val_loss: 1.1718 - val_acc: 0.5263\n",
      "Epoch 300/400\n",
      "405/405 [==============================] - 1s - loss: 1.1011 - acc: 0.5481 - val_loss: 1.3100 - val_acc: 0.4833\n",
      "Epoch 301/400\n",
      "405/405 [==============================] - 1s - loss: 1.1153 - acc: 0.5333 - val_loss: 1.1532 - val_acc: 0.5024\n",
      "Epoch 302/400\n",
      "405/405 [==============================] - 1s - loss: 1.1186 - acc: 0.5358 - val_loss: 1.1757 - val_acc: 0.4928\n",
      "Epoch 303/400\n",
      "405/405 [==============================] - 1s - loss: 1.1124 - acc: 0.5309 - val_loss: 1.1659 - val_acc: 0.5072\n",
      "Epoch 304/400\n",
      "405/405 [==============================] - 1s - loss: 1.1252 - acc: 0.5284 - val_loss: 1.1772 - val_acc: 0.5167\n",
      "Epoch 305/400\n",
      "405/405 [==============================] - 1s - loss: 1.1114 - acc: 0.5457 - val_loss: 1.1768 - val_acc: 0.5072\n",
      "Epoch 306/400\n",
      "405/405 [==============================] - 2s - loss: 1.1330 - acc: 0.5210 - val_loss: 1.1537 - val_acc: 0.5311\n",
      "Epoch 307/400\n",
      "405/405 [==============================] - 1s - loss: 1.1368 - acc: 0.5333 - val_loss: 1.1411 - val_acc: 0.5455\n",
      "Epoch 308/400\n",
      "405/405 [==============================] - 1s - loss: 1.1403 - acc: 0.5210 - val_loss: 1.1801 - val_acc: 0.5024\n",
      "Epoch 309/400\n",
      "405/405 [==============================] - 1s - loss: 1.1408 - acc: 0.5185 - val_loss: 1.1580 - val_acc: 0.5120\n",
      "Epoch 310/400\n",
      "405/405 [==============================] - 1s - loss: 1.1411 - acc: 0.5358 - val_loss: 1.1579 - val_acc: 0.5215\n",
      "Epoch 311/400\n",
      "405/405 [==============================] - 1s - loss: 1.1460 - acc: 0.5037 - val_loss: 1.1758 - val_acc: 0.5024\n",
      "Epoch 312/400\n",
      "405/405 [==============================] - 1s - loss: 1.1190 - acc: 0.5235 - val_loss: 1.1472 - val_acc: 0.5167\n",
      "Epoch 313/400\n",
      "405/405 [==============================] - 1s - loss: 1.1405 - acc: 0.5235 - val_loss: 1.1706 - val_acc: 0.4641\n",
      "Epoch 314/400\n",
      "405/405 [==============================] - 1s - loss: 1.1468 - acc: 0.5136 - val_loss: 1.1800 - val_acc: 0.5024\n",
      "Epoch 315/400\n",
      "405/405 [==============================] - 1s - loss: 1.1484 - acc: 0.4963 - val_loss: 1.1776 - val_acc: 0.4976\n",
      "Epoch 316/400\n",
      "405/405 [==============================] - 1s - loss: 1.1350 - acc: 0.5235 - val_loss: 1.1859 - val_acc: 0.4737\n",
      "Epoch 317/400\n",
      "405/405 [==============================] - 1s - loss: 1.1053 - acc: 0.5457 - val_loss: 1.1559 - val_acc: 0.5072\n",
      "Epoch 318/400\n",
      "405/405 [==============================] - 1s - loss: 1.1385 - acc: 0.5037 - val_loss: 1.1984 - val_acc: 0.5072\n",
      "Epoch 319/400\n",
      "405/405 [==============================] - 1s - loss: 1.1417 - acc: 0.5136 - val_loss: 1.3464 - val_acc: 0.4593\n",
      "Epoch 320/400\n",
      "405/405 [==============================] - 1s - loss: 1.1048 - acc: 0.5556 - val_loss: 1.2745 - val_acc: 0.4833\n",
      "Epoch 321/400\n",
      "405/405 [==============================] - 2s - loss: 1.1176 - acc: 0.5210 - val_loss: 1.1400 - val_acc: 0.5215\n",
      "Epoch 322/400\n",
      "405/405 [==============================] - 1s - loss: 1.1238 - acc: 0.5383 - val_loss: 1.1427 - val_acc: 0.5263\n",
      "Epoch 323/400\n",
      "405/405 [==============================] - 1s - loss: 1.1203 - acc: 0.5086 - val_loss: 1.1695 - val_acc: 0.4785\n",
      "Epoch 324/400\n",
      "405/405 [==============================] - 1s - loss: 1.1317 - acc: 0.5383 - val_loss: 1.1902 - val_acc: 0.4976\n",
      "Epoch 325/400\n",
      "405/405 [==============================] - 1s - loss: 1.1224 - acc: 0.5086 - val_loss: 1.2109 - val_acc: 0.4880\n",
      "Epoch 326/400\n",
      "405/405 [==============================] - 1s - loss: 1.1298 - acc: 0.5136 - val_loss: 1.1990 - val_acc: 0.4928\n",
      "Epoch 327/400\n",
      "405/405 [==============================] - 2s - loss: 1.1452 - acc: 0.5185 - val_loss: 1.1667 - val_acc: 0.4880\n",
      "Epoch 328/400\n",
      "405/405 [==============================] - 1s - loss: 1.1040 - acc: 0.5531 - val_loss: 1.1571 - val_acc: 0.5167\n",
      "Epoch 329/400\n",
      "405/405 [==============================] - 1s - loss: 1.1171 - acc: 0.5309 - val_loss: 1.1700 - val_acc: 0.4976\n",
      "Epoch 330/400\n",
      "405/405 [==============================] - 1s - loss: 1.1454 - acc: 0.5383 - val_loss: 1.1673 - val_acc: 0.5167\n",
      "Epoch 331/400\n",
      "405/405 [==============================] - 1s - loss: 1.1001 - acc: 0.5333 - val_loss: 1.1757 - val_acc: 0.4689\n",
      "Epoch 332/400\n",
      "405/405 [==============================] - 1s - loss: 1.1257 - acc: 0.5407 - val_loss: 1.1751 - val_acc: 0.4737\n",
      "Epoch 333/400\n",
      "405/405 [==============================] - 1s - loss: 1.1098 - acc: 0.5531 - val_loss: 1.1581 - val_acc: 0.4928\n",
      "Epoch 334/400\n",
      "405/405 [==============================] - 1s - loss: 1.1035 - acc: 0.5506 - val_loss: 1.2174 - val_acc: 0.4737\n",
      "Epoch 335/400\n",
      "405/405 [==============================] - 1s - loss: 1.1332 - acc: 0.5407 - val_loss: 1.1564 - val_acc: 0.5072\n",
      "Epoch 336/400\n",
      "405/405 [==============================] - 1s - loss: 1.1334 - acc: 0.5160 - val_loss: 1.1575 - val_acc: 0.5024\n",
      "Epoch 337/400\n",
      "405/405 [==============================] - 1s - loss: 1.0912 - acc: 0.5333 - val_loss: 1.2033 - val_acc: 0.4928\n",
      "Epoch 338/400\n",
      "405/405 [==============================] - 1s - loss: 1.1392 - acc: 0.5432 - val_loss: 1.2613 - val_acc: 0.4785\n",
      "Epoch 339/400\n",
      "405/405 [==============================] - 1s - loss: 1.1269 - acc: 0.5383 - val_loss: 1.1475 - val_acc: 0.5072\n",
      "Epoch 340/400\n",
      "405/405 [==============================] - 1s - loss: 1.1098 - acc: 0.5259 - val_loss: 1.1256 - val_acc: 0.4928\n",
      "Epoch 341/400\n",
      "405/405 [==============================] - 1s - loss: 1.1129 - acc: 0.5284 - val_loss: 1.1488 - val_acc: 0.5167\n",
      "Epoch 342/400\n",
      "405/405 [==============================] - 1s - loss: 1.1266 - acc: 0.5284 - val_loss: 1.1451 - val_acc: 0.5311\n",
      "Epoch 343/400\n",
      "405/405 [==============================] - 1s - loss: 1.1302 - acc: 0.5185 - val_loss: 1.2227 - val_acc: 0.4928\n",
      "Epoch 344/400\n",
      "405/405 [==============================] - 1s - loss: 1.1102 - acc: 0.5284 - val_loss: 1.1260 - val_acc: 0.5072\n",
      "Epoch 345/400\n",
      "405/405 [==============================] - 1s - loss: 1.0935 - acc: 0.5407 - val_loss: 1.2276 - val_acc: 0.5024\n",
      "Epoch 346/400\n",
      "405/405 [==============================] - 1s - loss: 1.1188 - acc: 0.5580 - val_loss: 1.1536 - val_acc: 0.5072\n",
      "Epoch 347/400\n",
      "405/405 [==============================] - 1s - loss: 1.1349 - acc: 0.5259 - val_loss: 1.1372 - val_acc: 0.5263\n",
      "Epoch 348/400\n",
      "405/405 [==============================] - 1s - loss: 1.1369 - acc: 0.5481 - val_loss: 1.1665 - val_acc: 0.5024\n",
      "Epoch 349/400\n",
      "405/405 [==============================] - 1s - loss: 1.1055 - acc: 0.5481 - val_loss: 1.1308 - val_acc: 0.5215\n",
      "Epoch 350/400\n",
      "405/405 [==============================] - 1s - loss: 1.1107 - acc: 0.5481 - val_loss: 1.3166 - val_acc: 0.4593\n",
      "Epoch 351/400\n",
      "405/405 [==============================] - 1s - loss: 1.1287 - acc: 0.5333 - val_loss: 1.4382 - val_acc: 0.4641\n",
      "Epoch 352/400\n",
      "405/405 [==============================] - 1s - loss: 1.1236 - acc: 0.5160 - val_loss: 1.1550 - val_acc: 0.5167\n",
      "Epoch 353/400\n",
      "405/405 [==============================] - 1s - loss: 1.1326 - acc: 0.5383 - val_loss: 1.1369 - val_acc: 0.5215\n",
      "Epoch 354/400\n",
      "405/405 [==============================] - 1s - loss: 1.1077 - acc: 0.5333 - val_loss: 1.1243 - val_acc: 0.5359\n",
      "Epoch 355/400\n",
      "405/405 [==============================] - 1s - loss: 1.1127 - acc: 0.5235 - val_loss: 1.1339 - val_acc: 0.5215\n",
      "Epoch 356/400\n",
      "405/405 [==============================] - 1s - loss: 1.1275 - acc: 0.5259 - val_loss: 1.1383 - val_acc: 0.5263\n",
      "Epoch 357/400\n",
      "405/405 [==============================] - 1s - loss: 1.1349 - acc: 0.4914 - val_loss: 1.1740 - val_acc: 0.5072\n",
      "Epoch 358/400\n",
      "405/405 [==============================] - 1s - loss: 1.1442 - acc: 0.5136 - val_loss: 1.1459 - val_acc: 0.5311\n",
      "Epoch 359/400\n",
      "405/405 [==============================] - 1s - loss: 1.1374 - acc: 0.5284 - val_loss: 1.1403 - val_acc: 0.5167\n",
      "Epoch 360/400\n",
      "405/405 [==============================] - 1s - loss: 1.1283 - acc: 0.5160 - val_loss: 1.1648 - val_acc: 0.5167\n",
      "Epoch 361/400\n",
      "405/405 [==============================] - 1s - loss: 1.1290 - acc: 0.5160 - val_loss: 1.1237 - val_acc: 0.5167\n",
      "Epoch 362/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 1s - loss: 1.1412 - acc: 0.4790 - val_loss: 1.1197 - val_acc: 0.5215\n",
      "Epoch 363/400\n",
      "405/405 [==============================] - 1s - loss: 1.1379 - acc: 0.5160 - val_loss: 1.1347 - val_acc: 0.5072\n",
      "Epoch 364/400\n",
      "405/405 [==============================] - 1s - loss: 1.1282 - acc: 0.5358 - val_loss: 1.1564 - val_acc: 0.5120\n",
      "Epoch 365/400\n",
      "405/405 [==============================] - 1s - loss: 1.1254 - acc: 0.5235 - val_loss: 1.1495 - val_acc: 0.5167\n",
      "Epoch 366/400\n",
      "405/405 [==============================] - 1s - loss: 1.1163 - acc: 0.5284 - val_loss: 1.1424 - val_acc: 0.5167\n",
      "Epoch 367/400\n",
      "405/405 [==============================] - 1s - loss: 1.1251 - acc: 0.5259 - val_loss: 1.1503 - val_acc: 0.5167\n",
      "Epoch 368/400\n",
      "405/405 [==============================] - 1s - loss: 1.1193 - acc: 0.5235 - val_loss: 1.1312 - val_acc: 0.5215\n",
      "Epoch 369/400\n",
      "405/405 [==============================] - 1s - loss: 1.0886 - acc: 0.5679 - val_loss: 1.1364 - val_acc: 0.5120\n",
      "Epoch 370/400\n",
      "405/405 [==============================] - 1s - loss: 1.1215 - acc: 0.5383 - val_loss: 1.2559 - val_acc: 0.4928\n",
      "Epoch 371/400\n",
      "405/405 [==============================] - 1s - loss: 1.1140 - acc: 0.5556 - val_loss: 1.2122 - val_acc: 0.4976\n",
      "Epoch 372/400\n",
      "405/405 [==============================] - 1s - loss: 1.1088 - acc: 0.5383 - val_loss: 1.1834 - val_acc: 0.4976\n",
      "Epoch 373/400\n",
      "405/405 [==============================] - 1s - loss: 1.1172 - acc: 0.5457 - val_loss: 1.1959 - val_acc: 0.5024\n",
      "Epoch 374/400\n",
      "405/405 [==============================] - 1s - loss: 1.1176 - acc: 0.5383 - val_loss: 1.3079 - val_acc: 0.4880\n",
      "Epoch 375/400\n",
      "405/405 [==============================] - 1s - loss: 1.1166 - acc: 0.5235 - val_loss: 1.2052 - val_acc: 0.5120\n",
      "Epoch 376/400\n",
      "405/405 [==============================] - 1s - loss: 1.0988 - acc: 0.5506 - val_loss: 1.5506 - val_acc: 0.4593\n",
      "Epoch 377/400\n",
      "405/405 [==============================] - 1s - loss: 1.1114 - acc: 0.5383 - val_loss: 1.2273 - val_acc: 0.5167\n",
      "Epoch 378/400\n",
      "405/405 [==============================] - 1s - loss: 1.1084 - acc: 0.5506 - val_loss: 1.2339 - val_acc: 0.5167\n",
      "Epoch 379/400\n",
      "405/405 [==============================] - 1s - loss: 1.1273 - acc: 0.5309 - val_loss: 1.3012 - val_acc: 0.4976\n",
      "Epoch 380/400\n",
      "405/405 [==============================] - 1s - loss: 1.1593 - acc: 0.5136 - val_loss: 1.4191 - val_acc: 0.4737\n",
      "Epoch 381/400\n",
      "405/405 [==============================] - 1s - loss: 1.1085 - acc: 0.5580 - val_loss: 1.2565 - val_acc: 0.5120\n",
      "Epoch 382/400\n",
      "405/405 [==============================] - 1s - loss: 1.1275 - acc: 0.5160 - val_loss: 1.1438 - val_acc: 0.5167\n",
      "Epoch 383/400\n",
      "405/405 [==============================] - 1s - loss: 1.1126 - acc: 0.5432 - val_loss: 1.2042 - val_acc: 0.5215\n",
      "Epoch 384/400\n",
      "405/405 [==============================] - 1s - loss: 1.1463 - acc: 0.5432 - val_loss: 1.1390 - val_acc: 0.5550\n",
      "Epoch 385/400\n",
      "405/405 [==============================] - 1s - loss: 1.1259 - acc: 0.5407 - val_loss: 1.1510 - val_acc: 0.5311\n",
      "Epoch 386/400\n",
      "405/405 [==============================] - 1s - loss: 1.0872 - acc: 0.5753 - val_loss: 1.1453 - val_acc: 0.5120\n",
      "Epoch 387/400\n",
      "405/405 [==============================] - 1s - loss: 1.1150 - acc: 0.5383 - val_loss: 1.1442 - val_acc: 0.5215\n",
      "Epoch 388/400\n",
      "405/405 [==============================] - 1s - loss: 1.1165 - acc: 0.5235 - val_loss: 1.1650 - val_acc: 0.5215\n",
      "Epoch 389/400\n",
      "405/405 [==============================] - 1s - loss: 1.1211 - acc: 0.5185 - val_loss: 1.2252 - val_acc: 0.5215\n",
      "Epoch 390/400\n",
      "405/405 [==============================] - 1s - loss: 1.1452 - acc: 0.5333 - val_loss: 1.2561 - val_acc: 0.4928\n",
      "Epoch 391/400\n",
      "405/405 [==============================] - 1s - loss: 1.1235 - acc: 0.5284 - val_loss: 1.2092 - val_acc: 0.5263\n",
      "Epoch 392/400\n",
      "405/405 [==============================] - 1s - loss: 1.0897 - acc: 0.5259 - val_loss: 1.1297 - val_acc: 0.5359\n",
      "Epoch 393/400\n",
      "405/405 [==============================] - 1s - loss: 1.1399 - acc: 0.5309 - val_loss: 1.4489 - val_acc: 0.4976\n",
      "Epoch 394/400\n",
      "405/405 [==============================] - 1s - loss: 1.1236 - acc: 0.5432 - val_loss: 1.5528 - val_acc: 0.4737\n",
      "Epoch 395/400\n",
      "405/405 [==============================] - 1s - loss: 1.1317 - acc: 0.5432 - val_loss: 1.6118 - val_acc: 0.4593\n",
      "Epoch 396/400\n",
      "405/405 [==============================] - 1s - loss: 1.1382 - acc: 0.5457 - val_loss: 1.1420 - val_acc: 0.5024\n",
      "Epoch 397/400\n",
      "405/405 [==============================] - 1s - loss: 1.1244 - acc: 0.5309 - val_loss: 1.2096 - val_acc: 0.4450\n",
      "Epoch 398/400\n",
      "405/405 [==============================] - 1s - loss: 1.1241 - acc: 0.5086 - val_loss: 1.1579 - val_acc: 0.5167\n",
      "Epoch 399/400\n",
      "405/405 [==============================] - 1s - loss: 1.1220 - acc: 0.5284 - val_loss: 1.1175 - val_acc: 0.5311\n",
      "Epoch 400/400\n",
      "405/405 [==============================] - 1s - loss: 1.1426 - acc: 0.5086 - val_loss: 1.1386 - val_acc: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x198e0ce9b70>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 25\n",
    "num_feat_map = 32\n",
    "feat_dim = 136\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "y2_train1 = label_encoder.fit_transform(y2_train)\n",
    "y2_train2 = one_hot.fit_transform(y2_train1.reshape(len(y2_train1),1))\n",
    "y2_test1 = label_encoder.fit_transform(y2_test)\n",
    "y2_test2 = one_hot.fit_transform(y2_test1.reshape(len(y2_test1),1))\n",
    "\n",
    "model = create_graph_emotions()\n",
    "\n",
    "epochs = 400\n",
    "batch_size=32\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "#filepath=\"weights_best_arousal.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([x_train,aux_inputs_train], y2_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=([x_test,aux_inputs_test], y2_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04768448,  0.0378131 ,  0.60774869,  0.30675372],\n",
       "       [ 0.4703249 ,  0.08609053,  0.33651114,  0.10707346],\n",
       "       [ 0.13342418,  0.06425221,  0.68945199,  0.11287159],\n",
       "       [ 0.09950347,  0.08324497,  0.72535008,  0.09190145],\n",
       "       [ 0.03130779,  0.05089725,  0.72246373,  0.19533123],\n",
       "       [ 0.00725963,  0.08696808,  0.5577212 ,  0.34805104],\n",
       "       [ 0.12774347,  0.09889749,  0.58939576,  0.18396327],\n",
       "       [ 0.01198139,  0.06219753,  0.63255483,  0.29326621],\n",
       "       [ 0.17877708,  0.08132731,  0.4944329 ,  0.24546267],\n",
       "       [ 0.0926079 ,  0.08046555,  0.69305813,  0.13386844],\n",
       "       [ 0.09533754,  0.03669759,  0.76781207,  0.10015278],\n",
       "       [ 0.56097007,  0.1347709 ,  0.19282453,  0.1114345 ],\n",
       "       [ 0.27894655,  0.09811429,  0.46605417,  0.15688492],\n",
       "       [ 0.59160149,  0.17585067,  0.11152893,  0.12101894],\n",
       "       [ 0.55355346,  0.17777236,  0.12956899,  0.13910519],\n",
       "       [ 0.31736138,  0.10727215,  0.43519774,  0.14016868],\n",
       "       [ 0.53038526,  0.18689816,  0.12553397,  0.15718262],\n",
       "       [ 0.56533432,  0.18730022,  0.10730214,  0.14006326],\n",
       "       [ 0.53225553,  0.18249346,  0.12765864,  0.15759236],\n",
       "       [ 0.48519537,  0.16208652,  0.19741409,  0.15530393],\n",
       "       [ 0.48530215,  0.18180922,  0.16978636,  0.16310221],\n",
       "       [ 0.38451892,  0.14517808,  0.30117875,  0.1691242 ],\n",
       "       [ 0.50805455,  0.19789259,  0.13554718,  0.15850562],\n",
       "       [ 0.53908992,  0.192287  ,  0.12318922,  0.1454338 ],\n",
       "       [ 0.44895113,  0.19481939,  0.17252594,  0.18370354],\n",
       "       [ 0.40607303,  0.21606919,  0.16280083,  0.21505691],\n",
       "       [ 0.50126481,  0.19312367,  0.1367712 ,  0.16884033],\n",
       "       [ 0.15224217,  0.09645427,  0.56767553,  0.18362813],\n",
       "       [ 0.55732316,  0.0931313 ,  0.26820627,  0.08133923],\n",
       "       [ 0.54183143,  0.08377247,  0.29904845,  0.07534765],\n",
       "       [ 0.5074057 ,  0.08015192,  0.3323552 ,  0.08008714],\n",
       "       [ 0.03770956,  0.063852  ,  0.69705445,  0.20138402],\n",
       "       [ 0.10490251,  0.05818297,  0.70030993,  0.13660458],\n",
       "       [ 0.37696424,  0.07895515,  0.44659987,  0.09748067],\n",
       "       [ 0.12597129,  0.04538442,  0.71776026,  0.110884  ],\n",
       "       [ 0.12448502,  0.04852609,  0.70939201,  0.1175969 ],\n",
       "       [ 0.21543202,  0.05046699,  0.63804197,  0.09605897],\n",
       "       [ 0.12513873,  0.04483424,  0.699386  ,  0.130641  ],\n",
       "       [ 0.17465033,  0.04710427,  0.67401242,  0.10423304],\n",
       "       [ 0.18244815,  0.04627164,  0.68883967,  0.08244064],\n",
       "       [ 0.09622131,  0.0538513 ,  0.71479923,  0.13512817],\n",
       "       [ 0.1536105 ,  0.04179006,  0.71099228,  0.09360716],\n",
       "       [ 0.42597401,  0.07507314,  0.41369835,  0.0852545 ],\n",
       "       [ 0.40374514,  0.07874543,  0.4224903 ,  0.09501919],\n",
       "       [ 0.07028337,  0.06035663,  0.70224708,  0.16711296],\n",
       "       [ 0.08795767,  0.05376366,  0.71227753,  0.14600118],\n",
       "       [ 0.1697704 ,  0.05702364,  0.66034943,  0.11285649],\n",
       "       [ 0.42757198,  0.07735763,  0.40562546,  0.08944499],\n",
       "       [ 0.20928797,  0.04685283,  0.65509194,  0.08876731],\n",
       "       [ 0.5363611 ,  0.08663281,  0.29008642,  0.08691971],\n",
       "       [ 0.44488758,  0.13100088,  0.30683544,  0.11727614],\n",
       "       [ 0.60799158,  0.11405555,  0.20648885,  0.07146402],\n",
       "       [ 0.34818193,  0.20768155,  0.27211717,  0.17201936],\n",
       "       [ 0.58887833,  0.18651941,  0.12261465,  0.1019877 ],\n",
       "       [ 0.5913226 ,  0.18261687,  0.1226745 ,  0.10338597],\n",
       "       [ 0.51653194,  0.17026582,  0.18998602,  0.12321622],\n",
       "       [ 0.57998919,  0.10975096,  0.22736453,  0.08289531],\n",
       "       [ 0.14554521,  0.0412835 ,  0.72261047,  0.09056076],\n",
       "       [ 0.14995135,  0.0546456 ,  0.67449522,  0.12090779],\n",
       "       [ 0.10137406,  0.04759435,  0.72771358,  0.12331796],\n",
       "       [ 0.51030076,  0.08127023,  0.33353019,  0.07489882],\n",
       "       [ 0.54125226,  0.11334514,  0.25361699,  0.09178551],\n",
       "       [ 0.49314183,  0.08519433,  0.34628087,  0.07538301],\n",
       "       [ 0.03187603,  0.19308345,  0.49047008,  0.28457043],\n",
       "       [ 0.6074208 ,  0.12114864,  0.18318316,  0.08824743],\n",
       "       [ 0.60035431,  0.11280975,  0.20688504,  0.07995085],\n",
       "       [ 0.58546966,  0.11167932,  0.21594909,  0.0869019 ],\n",
       "       [ 0.57257581,  0.10323495,  0.23913191,  0.08505733],\n",
       "       [ 0.49500412,  0.0795873 ,  0.34566692,  0.0797416 ],\n",
       "       [ 0.62119919,  0.10055966,  0.206545  ,  0.07169615],\n",
       "       [ 0.39270249,  0.07035773,  0.4490664 ,  0.08787337],\n",
       "       [ 0.08402822,  0.16854197,  0.53648359,  0.21094626],\n",
       "       [ 0.42722532,  0.05398182,  0.45801896,  0.06077385],\n",
       "       [ 0.16278271,  0.02665044,  0.75101805,  0.05954877],\n",
       "       [ 0.13566104,  0.03344784,  0.74849164,  0.08239949],\n",
       "       [ 0.18363975,  0.0365326 ,  0.70101184,  0.07881579],\n",
       "       [ 0.26188326,  0.03782995,  0.63432693,  0.06595986],\n",
       "       [ 0.11616968,  0.02831221,  0.78043944,  0.07507861],\n",
       "       [ 0.45865092,  0.05803846,  0.41833186,  0.06497867],\n",
       "       [ 0.21338603,  0.03074691,  0.69566554,  0.06020146],\n",
       "       [ 0.12620144,  0.02984259,  0.77389926,  0.07005682],\n",
       "       [ 0.42225373,  0.07837909,  0.41671994,  0.08264733],\n",
       "       [ 0.04756308,  0.14999095,  0.57958311,  0.22286282],\n",
       "       [ 0.57570726,  0.06753134,  0.29590687,  0.06085453],\n",
       "       [ 0.40195239,  0.07353047,  0.40574038,  0.11877676],\n",
       "       [ 0.18396923,  0.08642507,  0.43289885,  0.29670691],\n",
       "       [ 0.10469651,  0.06429095,  0.49757731,  0.33343521],\n",
       "       [ 0.05128166,  0.06513106,  0.43706891,  0.44651833],\n",
       "       [ 0.02084246,  0.04433329,  0.55370045,  0.38112387],\n",
       "       [ 0.10091594,  0.05656562,  0.54281741,  0.29970101],\n",
       "       [ 0.22933017,  0.0497607 ,  0.62063581,  0.10027333],\n",
       "       [ 0.17606428,  0.04791544,  0.62982762,  0.14619267],\n",
       "       [ 0.04593449,  0.04694774,  0.81366313,  0.0934547 ],\n",
       "       [ 0.14220563,  0.05550984,  0.58260477,  0.2196798 ],\n",
       "       [ 0.22426493,  0.06730624,  0.52233982,  0.18608907],\n",
       "       [ 0.06030391,  0.030446  ,  0.73184282,  0.17740728],\n",
       "       [ 0.11625457,  0.03106591,  0.73861116,  0.11406832],\n",
       "       [ 0.11178689,  0.03484994,  0.70534444,  0.14801869],\n",
       "       [ 0.01360184,  0.06282323,  0.47146839,  0.45210651],\n",
       "       [ 0.01032053,  0.08555654,  0.38331464,  0.52080834],\n",
       "       [ 0.05855776,  0.04683145,  0.58575845,  0.30885237],\n",
       "       [ 0.01866253,  0.07716546,  0.53392148,  0.37025052],\n",
       "       [ 0.03700991,  0.03177484,  0.74359185,  0.18762352],\n",
       "       [ 0.0327916 ,  0.04435183,  0.64937133,  0.27348527],\n",
       "       [ 0.06855855,  0.1272753 ,  0.62193549,  0.18223067],\n",
       "       [ 0.64256293,  0.12308232,  0.14696993,  0.0873848 ],\n",
       "       [ 0.05468906,  0.09471538,  0.69153941,  0.15905617],\n",
       "       [ 0.07062097,  0.06663599,  0.7741127 ,  0.08863034],\n",
       "       [ 0.16143006,  0.07133836,  0.55442113,  0.21281041],\n",
       "       [ 0.29226226,  0.08591601,  0.44805023,  0.17377153],\n",
       "       [ 0.04964188,  0.08208412,  0.59807032,  0.27020365],\n",
       "       [ 0.03143406,  0.06941375,  0.52831882,  0.37083334],\n",
       "       [ 0.03657972,  0.09080842,  0.53001028,  0.34260154],\n",
       "       [ 0.20818819,  0.07274127,  0.55213302,  0.16693746],\n",
       "       [ 0.55774665,  0.11288284,  0.21583247,  0.11353807],\n",
       "       [ 0.15577416,  0.05729268,  0.61265808,  0.17427503],\n",
       "       [ 0.14918953,  0.0612474 ,  0.63149613,  0.15806694],\n",
       "       [ 0.10142238,  0.032589  ,  0.77041817,  0.09557052],\n",
       "       [ 0.54503113,  0.1274187 ,  0.23331028,  0.09423991],\n",
       "       [ 0.06954336,  0.09225814,  0.6167528 ,  0.22144572],\n",
       "       [ 0.10920674,  0.04778142,  0.73070484,  0.11230697],\n",
       "       [ 0.12658475,  0.04944213,  0.70596009,  0.118013  ],\n",
       "       [ 0.24920623,  0.06232159,  0.57626647,  0.11220569],\n",
       "       [ 0.4739989 ,  0.08797295,  0.34644479,  0.09158335],\n",
       "       [ 0.18199643,  0.07217401,  0.60021043,  0.1456192 ],\n",
       "       [ 0.05790481,  0.11696513,  0.59689903,  0.22823103],\n",
       "       [ 0.15006439,  0.15293765,  0.55056357,  0.14643441],\n",
       "       [ 0.16355665,  0.11516746,  0.35460597,  0.36666986],\n",
       "       [ 0.54543179,  0.16556506,  0.13998325,  0.14901996],\n",
       "       [ 0.18998361,  0.14205164,  0.28299421,  0.38497061],\n",
       "       [ 0.00443159,  0.07269007,  0.26471269,  0.65816563],\n",
       "       [ 0.07142698,  0.0993188 ,  0.68120849,  0.14804561],\n",
       "       [ 0.0190952 ,  0.05931806,  0.48606518,  0.4355216 ],\n",
       "       [ 0.43828043,  0.17164883,  0.19070923,  0.19936143],\n",
       "       [ 0.03773273,  0.08781127,  0.69845814,  0.17599788],\n",
       "       [ 0.09169029,  0.04188693,  0.71582323,  0.15059951],\n",
       "       [ 0.06590252,  0.04123438,  0.72997612,  0.1628871 ],\n",
       "       [ 0.12628211,  0.04065965,  0.71579492,  0.11726337],\n",
       "       [ 0.10161979,  0.06672896,  0.65376991,  0.17788133],\n",
       "       [ 0.06443091,  0.07405969,  0.77587783,  0.0856315 ],\n",
       "       [ 0.08580359,  0.09516287,  0.68862998,  0.13040356],\n",
       "       [ 0.39578906,  0.08267386,  0.39721331,  0.1243238 ],\n",
       "       [ 0.38228542,  0.04768014,  0.51155823,  0.05847614],\n",
       "       [ 0.12597394,  0.03544385,  0.71790367,  0.12067854],\n",
       "       [ 0.12873772,  0.05618704,  0.56207299,  0.25300229],\n",
       "       [ 0.12480382,  0.0239686 ,  0.7799418 ,  0.07128581],\n",
       "       [ 0.13780135,  0.02556885,  0.75463879,  0.08199104],\n",
       "       [ 0.15424186,  0.04439991,  0.70431519,  0.09704299],\n",
       "       [ 0.17861409,  0.04160888,  0.69619787,  0.0835792 ],\n",
       "       [ 0.5014981 ,  0.07925544,  0.34206048,  0.07718592],\n",
       "       [ 0.07545267,  0.09897554,  0.6845414 ,  0.14103033],\n",
       "       [ 0.13913849,  0.13914272,  0.59214693,  0.12957184],\n",
       "       [ 0.51321018,  0.06383568,  0.36188293,  0.06107128],\n",
       "       [ 0.23141739,  0.03997696,  0.66362894,  0.0649767 ],\n",
       "       [ 0.1129949 ,  0.12029349,  0.62917638,  0.13753523],\n",
       "       [ 0.09634209,  0.08170237,  0.70059609,  0.12135945],\n",
       "       [ 0.08958047,  0.0539932 ,  0.77010632,  0.08632   ],\n",
       "       [ 0.11481245,  0.05826296,  0.7313779 ,  0.09554671],\n",
       "       [ 0.05450981,  0.04475483,  0.79377991,  0.1069554 ],\n",
       "       [ 0.16579987,  0.17178087,  0.49832609,  0.16409311],\n",
       "       [ 0.24892928,  0.19633734,  0.40480196,  0.14993142],\n",
       "       [ 0.04827894,  0.16840416,  0.51973766,  0.26357922],\n",
       "       [ 0.17048685,  0.09609096,  0.59391153,  0.13951062],\n",
       "       [ 0.13218208,  0.10727866,  0.62547612,  0.13506316],\n",
       "       [ 0.18277656,  0.16760659,  0.49231377,  0.15730314],\n",
       "       [ 0.10932749,  0.06915461,  0.72676706,  0.09475078],\n",
       "       [ 0.06976476,  0.05211636,  0.69546574,  0.1826532 ],\n",
       "       [ 0.32721347,  0.04434322,  0.55558586,  0.07285739],\n",
       "       [ 0.32303032,  0.04280369,  0.5631882 ,  0.07097777],\n",
       "       [ 0.09495884,  0.02565036,  0.79409707,  0.08529376],\n",
       "       [ 0.04922046,  0.02218755,  0.80436969,  0.12422232],\n",
       "       [ 0.46318197,  0.05267286,  0.42175207,  0.06239307],\n",
       "       [ 0.1521243 ,  0.03264811,  0.72364396,  0.09158368],\n",
       "       [ 0.12498103,  0.04046549,  0.70726424,  0.12728927],\n",
       "       [ 0.05095688,  0.06239774,  0.68167967,  0.20496574],\n",
       "       [ 0.03742799,  0.02923003,  0.78609037,  0.14725162],\n",
       "       [ 0.04708415,  0.04303389,  0.81997442,  0.08990757],\n",
       "       [ 0.01078858,  0.06982309,  0.73130685,  0.1880815 ],\n",
       "       [ 0.09506025,  0.04652552,  0.69991249,  0.15850177],\n",
       "       [ 0.09256576,  0.03987373,  0.74136555,  0.12619498],\n",
       "       [ 0.06176683,  0.03642676,  0.73970705,  0.16209939],\n",
       "       [ 0.03295131,  0.07075315,  0.68627465,  0.21002094],\n",
       "       [ 0.54819775,  0.0605178 ,  0.33347988,  0.05780458],\n",
       "       [ 0.18328039,  0.04731226,  0.69176871,  0.07763863],\n",
       "       [ 0.11641075,  0.06643267,  0.72808075,  0.08907578],\n",
       "       [ 0.49204284,  0.05951326,  0.38593757,  0.06250637],\n",
       "       [ 0.08902361,  0.0524906 ,  0.77986294,  0.07862288],\n",
       "       [ 0.48761484,  0.05762635,  0.39222273,  0.06253605],\n",
       "       [ 0.24621771,  0.04790352,  0.61689717,  0.08898155],\n",
       "       [ 0.38402817,  0.05559599,  0.48665136,  0.07372443],\n",
       "       [ 0.49358335,  0.06444337,  0.37805438,  0.06391887],\n",
       "       [ 0.43590042,  0.05989215,  0.43325663,  0.07095081],\n",
       "       [ 0.14465503,  0.07860963,  0.68841082,  0.08832455],\n",
       "       [ 0.19074093,  0.03875337,  0.67883354,  0.09167218],\n",
       "       [ 0.07264555,  0.06236311,  0.66260755,  0.20238383],\n",
       "       [ 0.18058936,  0.06513323,  0.6518544 ,  0.10242303],\n",
       "       [ 0.1933192 ,  0.04869782,  0.6217159 ,  0.1362671 ],\n",
       "       [ 0.26057932,  0.05977307,  0.59350777,  0.08613987],\n",
       "       [ 0.26505992,  0.05341666,  0.6052863 ,  0.0762371 ],\n",
       "       [ 0.21299587,  0.07012316,  0.59983808,  0.11704293],\n",
       "       [ 0.08448473,  0.05780746,  0.75663131,  0.10107646],\n",
       "       [ 0.13290596,  0.0675303 ,  0.65729105,  0.1422727 ],\n",
       "       [ 0.09063142,  0.04715624,  0.75627995,  0.10593237],\n",
       "       [ 0.12048025,  0.03338014,  0.76184511,  0.08429448],\n",
       "       [ 0.10911953,  0.05698485,  0.69900799,  0.13488768],\n",
       "       [ 0.12742542,  0.03634606,  0.76354414,  0.07268444],\n",
       "       [ 0.0752986 ,  0.05304535,  0.78941107,  0.08224498],\n",
       "       [ 0.11801247,  0.0624154 ,  0.73254174,  0.0870304 ],\n",
       "       [ 0.08161505,  0.0473842 ,  0.78839159,  0.08260914]], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict([x_test,aux_inputs_test])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 2, 2, 1, 0, 2, 2, 1, 2, 1, 1, 0, 0, 3, 1, 0, 0, 0, 1, 2, 0,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 3, 3, 0, 2, 2, 3, 3, 2, 0, 2, 3, 0, 2, 2, 3,\n",
       "       2, 0, 0, 2, 1, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 0, 3, 2, 3, 0, 0,\n",
       "       0, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 3, 3, 2, 2,\n",
       "       2, 2, 0, 2, 1, 0, 3, 3, 0, 2, 3, 1, 1, 0, 1, 1, 2, 0, 3, 3, 3, 3, 0,\n",
       "       2, 2, 2, 0, 3, 0, 0, 3, 0, 0, 3, 0, 2, 0, 0, 3, 1, 3, 2, 3, 2, 2, 0,\n",
       "       0, 1, 2, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2,\n",
       "       3, 3, 2, 0, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 0, 3, 2, 2, 2, 0, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2], dtype=int64)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3 = np.argmax(y2_test2,axis=1)\n",
    "y_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  0, 22,  1],\n",
       "       [10,  0, 13,  0],\n",
       "       [14,  0, 80,  2],\n",
       "       [ 4,  0, 35,  2]], dtype=int64)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds,axis=1)\n",
    "confusion_matrix(y_test3,np.argmax(preds,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.53      0.50        49\n",
      "          1       0.00      0.00      0.00        23\n",
      "          2       0.53      0.83      0.65        96\n",
      "          3       0.40      0.05      0.09        41\n",
      "\n",
      "avg / total       0.44      0.52      0.43       209\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test3,np.argmax(preds,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Maybe talk about resampling to take care of class imbalances in the optimization section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
